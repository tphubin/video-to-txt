é¡¹ç›® 'video-analyzer-release' çš„ç»“æ„æ ‘:
ğŸ“‚ video-analyzer-release/
    ğŸ“„ MANIFEST.in
    ğŸ“„ app.py
    ğŸ“„ requirements.txt
    ğŸ“„ setup.py
    ğŸ“„ start_windows.bat
    ğŸ“„ test_prompt_loading.py
    ğŸ“„ ui_settings.json
    ğŸ“‚ analysis_results/
    ğŸ“‚ docs/
        ğŸ“„ AI.md
        ğŸ“„ CONTRIBUTING.md
        ğŸ“„ DESIGN.md
        ğŸ“„ USAGES.md
        ğŸ“„ design.excalidraw
        ğŸ“„ sample_analysis.json
    ğŸ“‚ fonts/
    ğŸ“‚ gradio_output/
    ğŸ“‚ prompts/
        ğŸ“‚ frame_analysis/
            ğŸ“„ describe.txt
            ğŸ“„ frame_analysis.txt
            ğŸ“„ video_summary.txt
    ğŸ“‚ video-analyzer-ui/
        ğŸ“„ .gitignore
        ğŸ“„ MANIFEST.in
        ğŸ“„ README.md
        ğŸ“„ pyproject.toml
        ğŸ“„ requirements.txt
        ğŸ“‚ video_analyzer_ui/
            ğŸ“„ __init__.py
            ğŸ“„ server.py
            ğŸ“‚ static/
                ğŸ“‚ css/
                    ğŸ“„ styles.css
                ğŸ“‚ js/
                    ğŸ“„ main.js
            ğŸ“‚ templates/
                ğŸ“„ index.html
    ğŸ“‚ video_analyzer/
        ğŸ“„ __init__.py
        ğŸ“„ analyzer.py
        ğŸ“„ audio_processor.py
        ğŸ“„ cli.py
        ğŸ“„ config.py
        ğŸ“„ frame.py
        ğŸ“„ prompt.py
        ğŸ“‚ clients/
            ğŸ“„ __init__.py
            ğŸ“„ generic_openai_api.py
            ğŸ“„ llm_client.py
            ğŸ“„ ollama.py
        ğŸ“‚ config/
            ğŸ“„ default_config.json
        ğŸ“‚ prompts/
            ğŸ“‚ frame_analysis/
                ğŸ“„ describe.txt
                ğŸ“„ frame_analysis.txt
    ğŸ“‚ video_analyzer.egg-info/
        ğŸ“„ PKG-INFO
        ğŸ“„ SOURCES.txt
        ğŸ“„ dependency_links.txt
        ğŸ“„ entry_points.txt
        ğŸ“„ requires.txt
        ğŸ“„ top_level.txt
================================================================================

--- æ–‡ä»¶è·¯å¾„: MANIFEST.in ---

include requirements.txt
include readme.md
include LICENSE
recursive-include video_analyzer/config *
recursive-include video_analyzer/prompts *
global-exclude __pycache__
global-exclude *.py[cod]
global-exclude *.so
global-exclude .DS_Store
global-exclude .git*


--- æ–‡ä»¶è·¯å¾„: app.py ---

import gradio as gr
import os
import json
import logging
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional, Iterator, Tuple
import requests
import cv2
import time
import psutil
import threading
import numpy as np
from datetime import datetime
import traceback
from dataclasses import dataclass, field
import io
import base64
import inspect
import subprocess  # æ·»åŠ  FFmpeg æ£€æŸ¥ä¾èµ–

# ==============================================================================
# é˜¶æ®µä¸€ï¼šä¾èµ–å¯¼å…¥ä¸å…¨å±€è®¾ç½®
# ==============================================================================

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)
logger = logging.getLogger(__name__)

# --- ä¾èµ–æ£€æŸ¥ä¸åŠ¨æ€å¯¼å…¥ ---
CORE_MODULES_LOADED = True
NVIDIA_GPU_AVAILABLE = False
ADVANCED_FEATURES_AVAILABLE = False
MEDIAINFO_AVAILABLE = False
FFMPEG_AVAILABLE = False

try:
    import pynvml
    pynvml.nvmlInit()
    NVIDIA_GPU_AVAILABLE = True
    logger.info("âœ… pynvml (NVIDIA GPU support) åŠ è½½æˆåŠŸã€‚")
except (ImportError, pynvml.NVMLError) as e:
    NVIDIA_GPU_AVAILABLE = False
    logger.warning(f"âš ï¸ pynvml åŠ è½½å¤±è´¥ï¼ŒGPUç›‘æ§å°†ä¸å¯ç”¨ã€‚é”™è¯¯: {e}")

try:
    from moviepy.editor import VideoFileClip, AudioFileClip, CompositeVideoClip, ImageClip, concatenate_videoclips, TextClip
    import matplotlib
    matplotlib.use('Agg') # é¿å…åœ¨éGUIç¯å¢ƒä¸‹æŠ¥é”™
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    import seaborn as sns
   
    def setup_chinese_font():
        font_dir = Path("fonts")
        font_dir.mkdir(exist_ok=True)
        font_files = list(font_dir.glob('*.ttf')) + list(font_dir.glob('*.otf'))
       
        simhei_path = font_dir / "SimHei.ttf"
        if not simhei_path.exists():
            try:
                logger.info("æ­£åœ¨ä¸‹è½½å¤‡ç”¨ä¸­æ–‡å­—ä½“ SimHei.ttf...")
                # ä½¿ç”¨æ›´å¯é çš„å­—ä½“æº
                font_url = "https://github.com/google/fonts/raw/main/ofl/notosanssc/NotoSansSC-Regular.otf"
                response = requests.get(font_url, timeout=20)
                response.raise_for_status()
                with open(simhei_path, 'wb') as f:
                    f.write(response.content)
                logger.info(f"âœ… å¤‡ç”¨å­—ä½“ä¸‹è½½æˆåŠŸ: {simhei_path}")
                font_files.append(simhei_path)
            except Exception as download_e:
                logger.warning(f"ä¸‹è½½å¤‡ç”¨å­—ä½“å¤±è´¥: {download_e}")
       
        if font_files:
            font_path = font_files[0]
            try:
                fm.fontManager.addfont(str(font_path))
                prop = fm.FontProperties(fname=str(font_path))
                font_name = prop.get_name()
               
                plt.rcParams['font.family'] = 'sans-serif'
                plt.rcParams['font.sans-serif'] = [font_name]
                plt.rcParams['axes.unicode_minus'] = False
               
                fig, ax = plt.subplots()
                ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡å­—ä½“', ha='center', va='center')
                plt.close(fig)
                # æ£€æŸ¥å­—ä½“æ˜¯å¦çœŸçš„è¢«è®¾ç½®
                if plt.rcParams['font.sans-serif'][0] == font_name:
                    logger.info(f"âœ… æˆåŠŸä» '{font_path.name}' åŠ è½½å¹¶è®¾ç½®ä¸­æ–‡å­—ä½“: {font_name}")
                    return True
                else:
                    raise RuntimeError("å­—ä½“è®¾ç½®æœªç”Ÿæ•ˆ")
            except Exception as e:
                logger.warning(f"åŠ è½½æœ¬åœ°å­—ä½“ '{font_path.name}' å¤±è´¥: {e}ã€‚å°†å¼ºåˆ¶é‡å»ºç¼“å­˜å¹¶é‡è¯•ã€‚")
                try:
                    cachedir = matplotlib.get_cachedir()
                    if os.path.exists(cachedir):
                        shutil.rmtree(cachedir)
                        logger.info("Matplotlib å­—ä½“ç¼“å­˜å·²æ¸…é™¤ï¼Œå°†è‡ªåŠ¨é‡å»ºã€‚")
                except (FileNotFoundError, PermissionError) as cache_e:
                    logger.warning(f"æ— æ³•æ¸…é™¤Matplotlibç¼“å­˜: {cache_e}ï¼Œç»§ç»­å°è¯•ã€‚")
               
                try:
                    fm._fontManager = fm.FontManager()
                    fm.fontManager.addfont(str(font_path))
                    prop = fm.FontProperties(fname=str(font_path))
                    font_name = prop.get_name()
                    plt.rcParams['font.family'] = 'sans-serif'
                    plt.rcParams['font.sans-serif'] = [font_name]
                    plt.rcParams['axes.unicode_minus'] = False
                    logger.info(f"âœ… é‡å»ºç¼“å­˜åï¼ŒæˆåŠŸè®¾ç½®ä¸­æ–‡å­—ä½“: {font_name}")
                    return True
                except Exception as final_e:
                    logger.error(f"é‡å»ºç¼“å­˜åä»æ— æ³•è®¾ç½®å­—ä½“ '{font_name}': {final_e}")

        logger.warning("æœ¬åœ° 'fonts' ç›®å½•ä¸ºç©ºæˆ–å­—ä½“åŠ è½½å¤±è´¥ã€‚å°†å°è¯•è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿå­—ä½“ã€‚")
        font_candidates = ['Microsoft YaHei', 'SimHei', 'DengXian', 'PingFang SC', 'Heiti SC', 'Arial Unicode MS']
        for font_name in font_candidates:
            try:
                if fm.findfont(font_name, fallback_to_default=False):
                    plt.rcParams['font.sans-serif'] = [font_name]
                    plt.rcParams['axes.unicode_minus'] = False
                    logger.info(f"âœ… å›é€€æˆåŠŸï¼šæ‰¾åˆ°å¹¶è®¾ç½®ç³»ç»Ÿå­—ä½“: {font_name}")
                    return True
            except Exception:
                continue
       
        logger.error("âŒ å­—ä½“è®¾ç½®å¤±è´¥ï¼šæœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æœ¬åœ°æˆ–ç³»ç»Ÿå­—ä½“ã€‚å›¾è¡¨ä¸­æ–‡å°†æ˜¾ç¤ºä¸ºæ–¹æ¡†ã€‚")
        return False
    FONT_LOADED_SUCCESSFULLY = setup_chinese_font()
    ADVANCED_FEATURES_AVAILABLE = True
    logger.info("âœ… moviepy, matplotlib, seaborn åŠ è½½æˆåŠŸã€‚")
except ImportError as e:
    ADVANCED_FEATURES_AVAILABLE = False
    logger.error(f"\n{'='*60}\nâŒ è­¦å‘Š: moviepy, matplotlibæˆ–seabornåŠ è½½å¤±è´¥ã€‚AIæ‘˜è¦è§†é¢‘å’Œç”»è´¨å›¾åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚\nè¯¦ç»†å¯¼å…¥é”™è¯¯: {e}\n{'='*60}\n")

try:
    result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True, creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0)
    if result.returncode == 0:
        FFMPEG_AVAILABLE = True
        logger.info("âœ… FFmpeg å¯ç”¨ï¼ˆMoviePy å°†æ­£å¸¸å·¥ä½œï¼‰ã€‚")
    else:
        FFMPEG_AVAILABLE = False
        logger.warning("âš ï¸ FFmpeg æœªæ£€æµ‹åˆ°ï¼Œè¯·å®‰è£… FFmpeg å¹¶æ·»åŠ è‡³ PATHã€‚")
except (ImportError, FileNotFoundError):
    FFMPEG_AVAILABLE = False
    logger.warning("âš ï¸ FFmpeg æœªåœ¨ PATH ä¸­æ‰¾åˆ°ã€‚è¯·å®‰è£…å¹¶é…ç½®ã€‚")

try:
    from pymediainfo import MediaInfo
    MEDIAINFO_AVAILABLE = True
    logger.info("âœ… pymediainfo åŠ è½½æˆåŠŸã€‚")
except ImportError:
    MEDIAINFO_AVAILABLE = False
    logger.warning(f"\n{'='*60}\nâš ï¸ è­¦å‘Š: pymediainfoæœªå®‰è£…ã€‚å°†æ— æ³•ç”Ÿæˆä¸“ä¸šçš„è¯¦ç»†å…ƒæ•°æ®JSONã€‚\nè¯·è¿è¡Œ: pip install pymediainfo\n{'='*60}\n")

# --- æ ¸å¿ƒé€»è¾‘ç±»å®šä¹‰ ---
@dataclass
class Frame:
    path: Path
    timestamp: float
    metrics: Dict[str, float] = field(default_factory=dict)

class VideoProcessor:
    def __init__(self, video_path: Path, output_dir: Path):
        self.video_path = video_path
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def extract_keyframes(self, frames_per_minute: int, max_frames: int) -> List[Frame]:
        cap = cv2.VideoCapture(str(self.video_path))
        if not cap.isOpened():
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {self.video_path}")
            return []
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
       
        target_frames = min(max_frames, int(duration / 60 * frames_per_minute))
        if target_frames == 0 and total_frames > 0:
            target_frames = 1
        if target_frames == 0:
            cap.release()
            return []
        frame_indices = np.linspace(0, total_frames - 1, target_frames, dtype=int)
       
        extracted_frames = []
        for i, frame_index in enumerate(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame_data = cap.read()
            if ret:
                timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
                frame_filename = self.output_dir / f"frame_{i:04d}_{timestamp:.2f}s.jpg"
                cv2.imwrite(str(frame_filename), frame_data)
                frame_metrics = get_frame_metrics(frame_data)
                extracted_frames.append(Frame(path=frame_filename, timestamp=timestamp, metrics=frame_metrics))
                logger.info(f"æå–å…³é”®å¸§ {i+1}/{len(frame_indices)}: æ—¶é—´ {timestamp:.2f}s")
       
        cap.release()
        logger.info(f"ä»è§†é¢‘ä¸­æå–äº† {len(extracted_frames)} å¸§ (ç›®æ ‡æ˜¯ {target_frames})")
        return extracted_frames

@dataclass
class AudioTranscript:
    text: str
    segments: List[Dict]
    language: str

class AudioProcessor:
    def __init__(self):
        self.whisper_model = None

    def extract_audio(self, video_path: Path, output_dir: Path) -> Optional[Path]:
        if not ADVANCED_FEATURES_AVAILABLE:
            logger.warning("Moviepy æœªåŠ è½½ï¼Œæ— æ³•æå–éŸ³é¢‘ã€‚")
            return None
        try:
            logger.info(f"æ­£åœ¨ä» {video_path.name} æå–éŸ³é¢‘...")
            video_clip = VideoFileClip(str(video_path))
            if video_clip.audio is None:
                logger.warning(f"è§†é¢‘ {video_path.name} ä¸åŒ…å«éŸ³è½¨ã€‚")
                video_clip.close()
                return None
            audio_path = output_dir / f"{video_path.stem}.mp3"
            video_clip.audio.write_audiofile(str(audio_path), codec='mp3', logger=None)
            video_clip.close()
            logger.info(f"éŸ³é¢‘æå–æˆåŠŸ: {audio_path}")
            return audio_path
        except Exception as e:
            logger.error(f"æå–éŸ³é¢‘å¤±è´¥: {e}")
            return None

    def transcribe(self, audio_path: Path) -> Optional[AudioTranscript]:
        try:
            import whisper
            if self.whisper_model is None:
                logger.info("æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ (base)...")
                self.whisper_model = whisper.load_model("base")
           
            logger.info(f"æ­£åœ¨ä½¿ç”¨ Whisper è½¬å½•éŸ³é¢‘: {audio_path.name}")
            result = self.whisper_model.transcribe(str(audio_path), fp16=NVIDIA_GPU_AVAILABLE)
            logger.info("éŸ³é¢‘è½¬å½•å®Œæˆã€‚")
            return AudioTranscript(
                text=result.get("text", ""),
                segments=result.get("segments", []),
                language=result.get("language", "")
            )
        except ImportError:
            logger.error("Whisper æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡ŒéŸ³é¢‘è½¬å½•ã€‚è¯·è¿è¡Œ: pip install openai-whisper")
            return None
        except Exception as e:
            logger.error(f"ä½¿ç”¨ Whisper è½¬å½•å¤±è´¥: {e}")
            return None

class PromptLoader:
    def __init__(self, prompt_dir: Optional[str], prompts_config: List[Dict[str, str]]):
        self.prompts = {}
        self.prompt_dir = Path(prompt_dir) if prompt_dir else Path("prompts")
       
        for config in prompts_config:
            name = config.get("name")
            path = config.get("path")
            if name and path:
                try:
                    full_path = self.prompt_dir / path
                    with open(full_path, 'r', encoding='utf-8') as f:
                        self.prompts[name] = f.read()
                    logger.info(f"æˆåŠŸåŠ è½½æç¤ºè¯ '{name}' from {full_path}")
                except FileNotFoundError:
                    logger.error(f"æç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°: {full_path}")
                except Exception as e:
                    logger.error(f"åŠ è½½æç¤ºè¯ '{name}' å¤±è´¥: {e}")

    def get_prompt(self, name: str) -> Optional[str]:
        return self.prompts.get(name)

class BaseAPIClient:
    def _encode_image_to_base64(self, image_path: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            logger.error(f"æ— æ³•å°†å›¾ç‰‡ç¼–ç ä¸º Base64: {image_path}, é”™è¯¯: {e}")
            raise
   
    def chat_stream(self, model: str, prompt: str, image_paths: Optional[List[str]] = None, temperature: float = 0.2, timeout: int = 600) -> Iterator[str]:
        raise NotImplementedError

class OllamaClient(BaseAPIClient):
    def __init__(self, url: str = "http://localhost:11434"):
        self.url = url.rstrip('/')
        self.chat_endpoint = f"{self.url}/api/chat"
        logger.info(f"Ollama å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œå°†ä½¿ç”¨åŸç”ŸèŠå¤©æ¥å£: {self.chat_endpoint}")

    def chat_stream(self, model: str, prompt: str, image_paths: Optional[List[str]] = None, temperature: float = 0.2, timeout: int = 600) -> Iterator[str]:
        headers = {"Content-Type": "application/json"}
        messages = [{"role": "user", "content": prompt}]
       
        if image_paths:
            try:
                encoded_images = [self._encode_image_to_base64(p) for p in image_paths]
                messages[0]["images"] = encoded_images
            except Exception as e:
                yield json.dumps({"error": f"å›¾ç‰‡ç¼–ç å¤±è´¥: {e}"}) + "\n"
                return
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            "options": {"temperature": temperature}
        }
        try:
            with requests.post(self.chat_endpoint, headers=headers, data=json.dumps(payload), stream=True, timeout=timeout) as response:
                response.raise_for_status()
                for line in response.iter_lines():
                    if line:
                        yield line.decode('utf-8')
        except requests.exceptions.RequestException as e:
            logger.error(f"è¯·æ±‚ Ollama åŸç”Ÿ API å¤±è´¥: {e}")
            yield json.dumps({"error": f"è¯·æ±‚ Ollama åŸç”Ÿ API å¤±è´¥: {e}"}) + "\n"

class GenericOpenAIAPIClient(BaseAPIClient):
    def __init__(self, api_key: str, api_url: str):
        self.api_key = api_key
        self.api_url = api_url.rstrip('/')
        self.chat_endpoint = f"{self.api_url}/chat/completions"
        logger.info(f"OpenAI å…¼å®¹å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œå°†ä½¿ç”¨æ¥å£: {self.chat_endpoint}")

    def chat_stream(self, model: str, prompt: str, image_paths: Optional[List[str]] = None, temperature: float = 0.2, timeout: int = 600) -> Iterator[str]:
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
       
        content_parts = [{"type": "text", "text": prompt}]
        if image_paths:
            try:
                for image_path in image_paths:
                    base64_image = self._encode_image_to_base64(image_path)
                    content_parts.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
                    })
            except Exception as e:
                yield f'data: {json.dumps({"error": f"å›¾ç‰‡ç¼–ç å¤±è´¥: {e}"})}\n\n'
                return
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": content_parts}],
            "temperature": temperature,
            "stream": True
        }
        try:
            with requests.post(self.chat_endpoint, headers=headers, json=payload, stream=True, timeout=timeout) as response:
                response.raise_for_status()
                for line in response.iter_lines():
                    if line.startswith(b'data: '):
                        yield line.decode('utf-8')
        except requests.exceptions.RequestException as e:
            logger.error(f"è¯·æ±‚ OpenAI å…¼å®¹ API å¤±è´¥: {e}")
            yield f'data: {json.dumps({"error": f"è¯·æ±‚ OpenAI å…¼å®¹ API å¤±è´¥: {e}"})}\n\n'

class VideoAnalyzer:
    def __init__(self, client: BaseAPIClient, model: str, prompt_loader: PromptLoader, temperature: float = 0.2, request_timeout: int = 600):
        self.client = client
        self.model = model
        self.prompt_loader = prompt_loader
        self.temperature = temperature
        self.request_timeout = request_timeout
        self.user_prompt = ""
        self.context_length = 4096

    def _process_stream(self, stream_iterator: Iterator[str]) -> Iterator[str]:
        full_response_text = ""
        for chunk_str in stream_iterator:
            if analysis_state.stop_requested:
                break
            try:
                if chunk_str.startswith('data: '):
                    chunk_str = chunk_str[6:]
                    if chunk_str.strip() == '[DONE]':
                        break
                    chunk = json.loads(chunk_str)
                    delta = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                else:
                    chunk = json.loads(chunk_str)
                    if chunk.get("done"):
                        break
                    delta = chunk.get("message", {}).get("content", "")
               
                if delta:
                    full_response_text += delta
                    yield delta
            except (json.JSONDecodeError, IndexError):
                continue
        yield f"__FULL_RESPONSE_END__{full_response_text}"

    def summarize_all_frames_stream(self, frames: List['Frame'], transcript: 'AudioTranscript') -> Iterator[str]:
        prompt_template = self.prompt_loader.get_prompt("Video Summary")
        if not prompt_template:
            yield "é”™è¯¯: æœªæ‰¾åˆ° 'Video Summary' æç¤ºè¯æ¨¡æ¿ã€‚"
            return
        frame_info = "\n".join([f"- å…³é”®å¸§ at {f.timestamp:.2f}s" for f in frames])
        prompt = prompt_template.format(
            user_prompt=self.user_prompt,
            audio_transcript=transcript.text,
            frame_info=frame_info
        )
       
        if len(prompt) > self.context_length * 2.5:
            prompt = prompt[:int(self.context_length * 2.5)] + "\n...[æç¤ºè¯å› è¿‡é•¿è¢«æˆªæ–­]"
        frame_paths = [str(f.path) for f in frames]
        stream_iterator = self.client.chat_stream(
            model=self.model,
            prompt=prompt,
            image_paths=frame_paths,
            temperature=self.temperature,
            timeout=self.request_timeout
        )
        yield from self._process_stream(stream_iterator)

@dataclass
class AnalysisState:
    is_running: bool = False
    stop_requested: bool = False
    status_message: str = "ç­‰å¾…ä¸­..."
   
analysis_state = AnalysisState()
SETTINGS_FILE = Path("ui_settings.json")

class AppState:
    def __init__(self):
        self.analyzer: Optional[VideoAnalyzer] = None
        self.is_loaded: bool = False
        self.system_stats = {"cpu": 0, "ram": 0, "gpu": 0, "vram": 0}
        self.stop_monitoring = threading.Event()

app_state = AppState()

# å…¨å±€UIç»„ä»¶å¼•ç”¨
status_box, client_type_dd, model_select, api_key_txt, api_url_txt, api_model_txt, load_button, unload_button = [None] * 8
output_report, output_metadata_table, metadata_plot, output_gallery, output_summary_video, output_gif, output_metadata_json = [None] * 7
output_summary_clips_gallery, clip_details_accordion, clip_details_md = None, None, None
run_status_html, analysis_progress, start_button, continue_button, stop_button, refresh_summary_button, clear_outputs_button = [None] * 7
frame_details_accordion, frame_details_md = None, None
analysis_cache_state = None
gif_info_md = None

PRESET_PROMPTS = {
    "å†…å®¹æ€»ç»“ä¸è¯„ä¼°": "è¯·è¯¦ç»†æ€»ç»“è¿™ä¸ªè§†é¢‘çš„æ ¸å¿ƒå†…å®¹ã€å…³é”®ä¿¡æ¯ç‚¹å’Œå™äº‹æµç¨‹ã€‚å¹¶ä»è§‚ä¼—çš„è§’åº¦è¯„ä¼°å…¶æ•´ä½“è´¨é‡ã€è¶£å‘³æ€§å’Œä¿¡æ¯ä»·å€¼ã€‚",
    "æŠ€æœ¯è´¨é‡åˆ†æ": "è¯·ä½œä¸ºä¸€åä¸“ä¸šçš„æ‘„å½±å¸ˆå’Œå‰ªè¾‘å¸ˆï¼Œä¸¥æ ¼è¯„ä¼°è¯¥è§†é¢‘çš„æŠ€æœ¯è´¨é‡ï¼ŒåŒ…æ‹¬æ„å›¾ã€ç¯å…‰ã€è‰²å½©ã€ç„¦ç‚¹ã€ç¨³å®šæ€§ã€å‰ªè¾‘èŠ‚å¥å’ŒéŸ³æ•ˆè®¾è®¡ç­‰æ–¹é¢ã€‚è¯·æä¾›å…·ä½“çš„ä¼˜ç‚¹å’Œå¯ä»¥æ”¹è¿›çš„å»ºè®®ã€‚",
    "æƒ…æ„Ÿä¸é£æ ¼è¯†åˆ«": "è¯·åˆ†æè¿™ä¸ªè§†é¢‘æ‰€ä¼ è¾¾çš„ä¸»è¦æƒ…æ„ŸåŸºè°ƒï¼ˆå¦‚æ¬¢ä¹ã€æ‚²ä¼¤ã€æ‚¬ç–‘ã€åŠ±å¿—ç­‰ï¼‰å’Œè§†è§‰é£æ ¼ï¼ˆå¦‚ç”µå½±æ„Ÿã€çºªå½•ç‰‡ã€Vlogã€å¤å¤ç­‰ï¼‰ã€‚å¹¶æŒ‡å‡ºå“ªäº›è§†å¬å…ƒç´ ï¼ˆå¦‚é…ä¹ã€è‰²è°ƒã€é•œå¤´è¯­è¨€ï¼‰å…±åŒä½œç”¨äºè¿™ç§æ„Ÿå—çš„å½¢æˆã€‚",
    "è‡ªå®šä¹‰": ""
}

# ==============================================================================
# é˜¶æ®µäºŒï¼šå‡½æ•°å®šä¹‰åŒº
# ==============================================================================

def monitor_system_stats():
    while not app_state.stop_monitoring.is_set():
        app_state.system_stats['cpu'] = psutil.cpu_percent()
        app_state.system_stats['ram'] = psutil.virtual_memory().percent
        if NVIDIA_GPU_AVAILABLE:
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                mem = pynvml.nvmlDeviceGetMemoryInfo(handle)
                app_state.system_stats['gpu'], app_state.system_stats['vram'] = util.gpu, (mem.used / mem.total) * 100
            except pynvml.NVMLError:
                app_state.system_stats['gpu'], app_state.system_stats['vram'] = -1, -1
        time.sleep(2)

def update_status_and_sys_info(message: str = "ç­‰å¾…ä»»åŠ¡å¼€å§‹..."):
    stats_html = get_system_stats_html()
    return f"<div style='text-align:center;'>{message}</div>{stats_html}"

def get_system_stats_html() -> str:
    stats = app_state.system_stats
    gpu_html = f"<div class='stat-item'><span class='label'>GPU</span><div class='bar-container'><div class='bar gpu' style='width: {stats.get('gpu', 0):.1f}%;'></div></div><span class='value'>{stats.get('gpu', 0):.1f}%</span></div><div class='stat-item'><span class='label'>VRAM</span><div class='bar-container'><div class='bar vram' style='width: {stats.get('vram', 0):.1f}%;'></div></div><span class='value'>{stats.get('vram', 0):.1f}%</span></div>" if NVIDIA_GPU_AVAILABLE and stats.get('gpu', -1) != -1 else ""
    return f"<div class='stats-container'>{gpu_html}<div class='stat-item'><span class='label'>CPU</span><div class='bar-container'><div class='bar cpu' style='width: {stats.get('cpu', 0):.1f}%;'></div></div><span class='value'>{stats.get('cpu', 0):.1f}%</span></div><div class='stat-item'><span class='label'>RAM</span><div class='bar-container'><div class='bar ram' style='width: {stats.get('ram', 0):.1f}%;'></div></div><span class='value'>{stats.get('ram', 0):.1f}%</span></div></div>"

def get_advanced_video_metrics(video_path: str, num_frames_to_sample=100):
    if not ADVANCED_FEATURES_AVAILABLE: 
        logger.warning("é«˜çº§è§†é¢‘æŒ‡æ ‡ä¸å¯ç”¨ï¼šMoviePy æˆ– Matplotlib æœªåŠ è½½ã€‚")
        return {}, None
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened(): 
        logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘ç”¨äºé«˜çº§æŒ‡æ ‡åˆ†æ: {video_path}")
        return {}, None
   
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    if total_frames == 0 or fps == 0:
        cap.release()
        return {}, None
    sample_indices = np.linspace(0, total_frames - 1, min(num_frames_to_sample, total_frames), dtype=int)
   
    metrics_over_time = {'timestamps': [], 'brightness': [], 'saturation': [], 'sharpness': []}
    frame_durations = []
    last_timestamp = 0
    for idx in sample_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret: 
            logger.warning(f"è¯»å–é‡‡æ ·å¸§ {idx} å¤±è´¥ï¼Œç»§ç»­...")
            continue
       
        timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
        metrics_over_time['timestamps'].append(timestamp)
       
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
       
        metrics_over_time['brightness'].append(np.mean(gray))
        metrics_over_time['saturation'].append(np.mean(hsv[:, :, 1]))
        metrics_over_time['sharpness'].append(cv2.Laplacian(gray, cv2.CV_64F).var())
       
        if last_timestamp > 0:
            duration = timestamp - last_timestamp
            if duration > 0: frame_durations.append(1.0 / duration)
        last_timestamp = timestamp
    cap.release()
   
    avg_metrics = {
        "å¹³å‡äº®åº¦ (0-255)": np.mean(metrics_over_time['brightness']) if metrics_over_time['brightness'] else 0,
        "å¹³å‡é¥±å’Œåº¦ (0-255)": np.mean(metrics_over_time['saturation']) if metrics_over_time['saturation'] else 0,
        "å¹³å‡æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)": np.mean(metrics_over_time['sharpness']) if metrics_over_time['sharpness'] else 0
    }
    logger.info(f"é«˜çº§æŒ‡æ ‡è®¡ç®—å®Œæˆ: äº®åº¦={avg_metrics['å¹³å‡äº®åº¦ (0-255)']:.2f}, é¥±å’Œåº¦={avg_metrics['å¹³å‡é¥±å’Œåº¦ (0-255)']:.2f}, æ¸…æ™°åº¦={avg_metrics['å¹³å‡æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)']:.2f}")
    sns.set_style("darkgrid")
    fig, axes = plt.subplots(2, 2, figsize=(15, 8))
    fig.suptitle('è§†é¢‘ç”»è´¨éšæ—¶é—´å˜åŒ–åˆ†æ', fontsize=16)
    sns.lineplot(x='timestamps', y='brightness', data=metrics_over_time, ax=axes[0, 0], color='skyblue', label=f"å¹³å‡å€¼: {avg_metrics['å¹³å‡äº®åº¦ (0-255)']:.2f}")
    axes[0, 0].set_title('äº®åº¦å˜åŒ–'); axes[0, 0].set_xlabel("æ—¶é—´ (ç§’)"); axes[0, 0].set_ylabel("æ•°å€¼"); axes[0, 0].legend()
    sns.lineplot(x='timestamps', y='saturation', data=metrics_over_time, ax=axes[0, 1], color='salmon', label=f"å¹³å‡å€¼: {avg_metrics['å¹³å‡é¥±å’Œåº¦ (0-255)']:.2f}")
    axes[0, 1].set_title('é¥±å’Œåº¦å˜åŒ–'); axes[0, 1].set_xlabel("æ—¶é—´ (ç§’)"); axes[0, 1].set_ylabel("æ•°å€¼"); axes[0, 1].legend()
    sns.lineplot(x='timestamps', y='sharpness', data=metrics_over_time, ax=axes[1, 0], color='lightgreen', label=f"å¹³å‡å€¼: {avg_metrics['å¹³å‡æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)']:.2f}")
    axes[1, 0].set_title('æ¸…æ™°åº¦ (é”åŒ–) å˜åŒ–'); axes[1, 0].set_xlabel("æ—¶é—´ (ç§’)"); axes[1, 0].set_ylabel("æ•°å€¼"); axes[1, 0].legend()
   
    if frame_durations:
        mean_fps = np.mean(frame_durations)
        sns.histplot(frame_durations, ax=axes[1, 1], color='orchid', bins=20, kde=True)
        axes[1, 1].set_title(f'å¸§ç‡ç¨³å®šæ€§ (å¹³å‡: {mean_fps:.2f} FPS)'); axes[1, 1].set_xlabel("å¸§ç‡ (FPS)")
        axes[1, 1].axvline(mean_fps, color='r', linestyle='--', label=f'å¹³å‡å¸§ç‡: {mean_fps:.2f}'); axes[1, 1].legend()
    else:
        axes[1, 1].text(0.5, 0.5, 'æ’å®šå¸§ç‡æˆ–æ— æ³•è®¡ç®—å¸§ç‡å˜åŒ–', ha='center', va='center'); axes[1, 1].set_title('å¸§ç‡ç¨³å®šæ€§')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    return avg_metrics, fig

def get_frame_metrics(frame: np.ndarray) -> dict:
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    return {
        "äº®åº¦ (0-255)": np.mean(gray),
        "å¯¹æ¯”åº¦ (æ ‡å‡†å·®)": np.std(gray),
        "é¥±å’Œåº¦ (0-255)": np.mean(hsv[:, :, 1]),
        "æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)": cv2.Laplacian(gray, cv2.CV_64F).var()
    }

def create_summary_media_artifacts(
    original_video_path: str,
    video_duration: float,
    frames: List[Frame],
    output_dir: Path,
    video_stem: str,
    num_clips: int,
    clip_duration_around_keyframe: float,
    make_video: bool,
    make_gif: bool,
    gif_resolution: str
) -> Tuple[Optional[List[str]], Optional[List[Frame]], Optional[str], Optional[str]]:
    """
    åˆ›å»ºåŠ¨æ€è§†é¢‘ç‰‡æ®µæ‘˜è¦ã€æ‹¼æ¥è§†é¢‘å’ŒGIFã€‚
    è¿”å›: (ç‰‡æ®µè·¯å¾„åˆ—è¡¨, é€‰ä¸­çš„å¸§åˆ—è¡¨, æ‹¼æ¥è§†é¢‘è·¯å¾„, GIFè·¯å¾„)
    """
    logger.info(f"å¼€å§‹åˆ›å»ºæ‘˜è¦åª’ä½“... å¯ç”¨è§†é¢‘: {make_video}, å¯ç”¨GIF: {make_gif}, ç‰‡æ®µæ•°: {num_clips}, æ—¶é•¿: {clip_duration_around_keyframe}s, åˆ†è¾¨ç‡: {gif_resolution}")
    if not ADVANCED_FEATURES_AVAILABLE or not (make_video or make_gif):
        logger.warning(f"è·³è¿‡æ‘˜è¦åª’ä½“åˆ›å»ºï¼Œå› ä¸ºä¾èµ–é¡¹ä¸å¯ç”¨(ADVANCED_FEATURES_AVAILABLE={ADVANCED_FEATURES_AVAILABLE})æˆ–ç”¨æˆ·æœªå¯ç”¨ã€‚")
        if not FFMPEG_AVAILABLE:
            logger.error("FFMPEG æœªå¯ç”¨ï¼Œè¿™æ˜¯ MoviePy å¤±è´¥çš„ä¸»è¦åŸå› ã€‚è¯·å®‰è£… FFmpegã€‚")
            gr.Warning("ç”Ÿæˆæ‘˜è¦åª’ä½“å¤±è´¥ï¼FFmpeg æœªå®‰è£…æˆ–æœªåœ¨ PATH ä¸­ã€‚è¯·ä¸‹è½½ FFmpeg å¹¶é…ç½®ç¯å¢ƒå˜é‡ã€‚")
        return None, None, None, None
   
    if not frames:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å¸§ï¼Œæ— æ³•åˆ›å»ºæ‘˜è¦åª’ä½“ã€‚")
        return None, None, None, None
    
    logger.info(f"æ­£åœ¨ä» {len(frames)} ä¸ªå€™é€‰å¸§ä¸­é€‰æ‹© {num_clips} ä¸ªæœ€æ¸…æ™°çš„å¸§...")
    # ç¡®ä¿ num_clips ä¸è¶…è¿‡å¯ç”¨å¸§æ•°
    num_clips = min(num_clips, len(frames))
    sorted_frames = sorted(frames, key=lambda x: x.metrics.get('æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)', 0), reverse=True)
    selected_frames = sorted(sorted_frames[:int(num_clips)], key=lambda x: x.timestamp)
    if not selected_frames:
        logger.warning("æ ¹æ®æ¸…æ™°åº¦æ’åºåï¼Œæ²¡æœ‰å¯é€‰çš„å¸§æ¥åˆ›å»ºæ‘˜è¦åª’ä½“ã€‚")
        return None, None, None, None
    logger.info(f"å·²é€‰å®š {len(selected_frames)} ä¸ªå¸§ç”¨äºç”Ÿæˆç‰‡æ®µã€‚æ—¶é—´ç‚¹: {[f.timestamp for f in selected_frames]}")
    
    individual_clip_paths = []
    
    logger.info(f"æ­£åœ¨ä»åŸå§‹è§†é¢‘ä¸­æˆªå– {len(selected_frames)} ä¸ªåŠ¨æ€ç‰‡æ®µ...")
    success_count = 0
    try:
        # ä½¿ç”¨ 'with' è¯­å¥ç¡®ä¿ä¸»è§†é¢‘æ–‡ä»¶åœ¨å¤„ç†å®Œåè¢«å…³é—­
        with VideoFileClip(original_video_path) as video:
            logger.info(f"è§†é¢‘åŠ è½½æˆåŠŸï¼Œæ—¶é•¿: {video.duration}s")
            for i, frame_obj in enumerate(selected_frames):
                try:
                    start_time = max(0, frame_obj.timestamp - clip_duration_around_keyframe / 2)
                    end_time = min(video.duration, frame_obj.timestamp + clip_duration_around_keyframe / 2)
                   
                    logger.info(f"ç‰‡æ®µ {i+1}/{len(selected_frames)}: æˆªå–æ—¶é—´ä» {start_time:.2f}s åˆ° {end_time:.2f}sã€‚")
                    if end_time <= start_time:
                        logger.warning(f"è·³è¿‡ç‰‡æ®µ {i}ï¼Œå› ä¸ºè®¡ç®—å‡ºçš„ç»“æŸæ—¶é—´({end_time})æ—©äºæˆ–ç­‰äºå¼€å§‹æ—¶é—´({start_time})ã€‚")
                        continue
                    
                    # ä»ä¸»è§†é¢‘ä¸­æˆªå–å­å‰ªè¾‘
                    sub_clip = video.subclip(start_time, end_time)
                   
                    clip_path = get_unique_filepath(output_dir, f"{video_stem}_clip_{i:02d}.mp4")
                    
                    # å°†å­å‰ªè¾‘å†™å…¥ç‹¬ç«‹çš„MP4æ–‡ä»¶ï¼ŒåŒ…å«éŸ³é¢‘
                    sub_clip.write_videofile(str(clip_path), codec='libx264', audio_codec='aac', logger=None, threads=4)
                   
                    individual_clip_paths.append(str(clip_path))
                    success_count += 1
                    logger.info(f"âœ… æˆåŠŸåˆ›å»ºè§†é¢‘ç‰‡æ®µ {i+1}/{len(selected_frames)}: {clip_path.name} (å¤§å°: {os.path.getsize(clip_path)/1024:.1f} KB)")
                    
                    # ç«‹å³å…³é—­å­å‰ªè¾‘ä»¥é‡Šæ”¾èµ„æº
                    sub_clip.close()

                except Exception as e:
                    logger.error(f"âŒ åˆ›å»ºè§†é¢‘ç‰‡æ®µ {i} (æ—¶é—´ç‚¹: {frame_obj.timestamp:.2f}s) æ—¶å‡ºé”™: {e}", exc_info=True)
                    gr.Warning(f"åˆ›å»ºç‰‡æ®µ {i+1} å¤±è´¥: {e}ã€‚ç»§ç»­ç”Ÿæˆå…¶ä»–ç‰‡æ®µã€‚")
                    continue
            logger.info(f"ç‰‡æ®µç”Ÿæˆå®Œæˆ: {success_count}/{len(selected_frames)} æˆåŠŸã€‚")
    except OSError as e:
        logger.error(f"âŒ MoviePy ä¸¥é‡é”™è¯¯: æ— æ³•å¤„ç†è§†é¢‘æ–‡ä»¶ã€‚è¿™é€šå¸¸æ„å‘³ç€ FFmpeg æœªå®‰è£…æˆ–æœªåœ¨ç³»ç»Ÿè·¯å¾„ä¸­ã€‚é”™è¯¯: {e}", exc_info=True)
        gr.Warning("ç”Ÿæˆè§†é¢‘æ‘˜è¦å¤±è´¥ï¼è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£… FFmpeg å¹¶å°†å…¶æ·»åŠ è‡³ç³»ç»Ÿç¯å¢ƒå˜é‡(PATH)ã€‚")
        return individual_clip_paths, selected_frames, None, None
   
    if not individual_clip_paths:
        logger.warning("âš ï¸ æœªèƒ½æˆåŠŸåˆ›å»ºä»»ä½•è§†é¢‘ç‰‡æ®µï¼Œæ— æ³•è¿›è¡Œæ‹¼æ¥ã€‚")
        return individual_clip_paths, selected_frames, None, None

    # --- æ‹¼æ¥é˜¶æ®µ ---
    # ã€å…³é”®ä¿®å¤ã€‘: ä»ç£ç›˜é‡æ–°åŠ è½½æ‰€æœ‰ç‰‡æ®µæ–‡ä»¶è¿›è¡Œæ‹¼æ¥ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å†…å­˜ä¸­å¯èƒ½å·²å¤±æ•ˆçš„subclipå¯¹è±¡
    concatenated_video_path, gif_path = None, None
    reloaded_clips = []
    try:
        logger.info(f"æ­£åœ¨ä» {len(individual_clip_paths)} ä¸ªå·²ä¿å­˜çš„ç‰‡æ®µæ–‡ä»¶é‡æ–°åŠ è½½ä»¥è¿›è¡Œæ‹¼æ¥...")
        reloaded_clips = [VideoFileClip(p) for p in individual_clip_paths]
        
        if not reloaded_clips:
            raise ValueError("é‡æ–°åŠ è½½ç‰‡æ®µæ–‡ä»¶ååˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æ‹¼æ¥ã€‚")

        final_clip = concatenate_videoclips(reloaded_clips, method="compose")
        logger.info("ç‰‡æ®µæ‹¼æ¥æˆåŠŸï¼Œå¼€å§‹å¯¼å‡º...")
       
        if make_video:
            concatenated_video_path = get_unique_filepath(output_dir, f"{video_stem}_summary_concatenated.mp4")
            logger.info(f"æ­£åœ¨ç”Ÿæˆæ‹¼æ¥æ‘˜è¦è§†é¢‘: {concatenated_video_path}")
            final_clip.write_videofile(str(concatenated_video_path), fps=24, codec='libx264', audio_codec='aac', logger=None, threads=4)
            logger.info(f"âœ… æ‹¼æ¥æ‘˜è¦è§†é¢‘ç”ŸæˆæˆåŠŸ (å¤§å°: {os.path.getsize(concatenated_video_path)/1024/1024:.1f} MB)")
       
        if make_gif:
            gif_path = get_unique_filepath(output_dir, f"{video_stem}_summary.gif")
            resolution_map = {"ä½": 0.3, "ä¸­": 0.5, "é«˜": 0.8}
            resized_clip = final_clip.resize(resolution_map.get(gif_resolution, 0.5))
            logger.info(f"æ­£åœ¨ç”Ÿæˆæ‘˜è¦GIF (åˆ†è¾¨ç‡: {gif_resolution}): {gif_path}")
            resized_clip.write_gif(str(gif_path), fps=10, logger=None)
            logger.info(f"âœ… æ‘˜è¦GIFç”ŸæˆæˆåŠŸ (å¤§å°: {os.path.getsize(gif_path)/1024:.1f} KB)")
            resized_clip.close()
        
        # å…³é—­æœ€ç»ˆçš„åˆæˆå‰ªè¾‘
        final_clip.close()
           
    except Exception as e:
        logger.error(f"âŒ æ‹¼æ¥è§†é¢‘æˆ–ç”ŸæˆGIFæ—¶å‡ºé”™: {e}", exc_info=True)
        gr.Warning(f"æ‹¼æ¥è§†é¢‘æˆ–ç”ŸæˆGIFæ—¶å‡ºé”™: {e}")
    finally:
        # ã€é‡è¦ã€‘: ç¡®ä¿æ‰€æœ‰é‡æ–°åŠ è½½çš„å‰ªè¾‘éƒ½è¢«å…³é—­ï¼Œä»¥é‡Šæ”¾æ–‡ä»¶å¥æŸ„
        logger.info("æ­£åœ¨å…³é—­æ‰€æœ‰ç”¨äºæ‹¼æ¥çš„è§†é¢‘ç‰‡æ®µèµ„æº...")
        for clip in reloaded_clips:
            try:
                clip.close()
            except Exception as close_err:
                logger.warning(f"å…³é—­ä¸€ä¸ªä¸´æ—¶ç‰‡æ®µæ—¶å‡ºé”™: {close_err}")
        logger.info("æ‰€æœ‰ä¸´æ—¶ç‰‡æ®µèµ„æºå·²å…³é—­ã€‚")
    
    logger.info("æ‘˜è¦åª’ä½“åˆ›å»ºæµç¨‹ç»“æŸã€‚")
    return individual_clip_paths, selected_frames, str(concatenated_video_path) if concatenated_video_path else None, str(gif_path) if gif_path else None

def get_unique_filepath(output_dir: Path, filename: str) -> Path:
    output_dir.mkdir(parents=True, exist_ok=True)
    base, ext = os.path.splitext(filename)
    filepath = output_dir / filename
    if filepath.exists():
        timestamp = datetime.now().strftime("_%Y%m%d%H%M%S")
        filepath = output_dir / f"{base}{timestamp}{ext}"
    return filepath

def get_video_metadata(video_path: str) -> (dict, float):
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened(): 
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘è·å–å…ƒæ•°æ®: {video_path}")
            return {}, 0
        width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps, frame_count = cap.get(cv2.CAP_PROP_FPS), int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps if fps > 0 else 0
        fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))
        codec = fourcc.to_bytes(4, 'little').decode('utf-8', errors='ignore').strip('\x00')
        cap.release()
        file_size_mb = os.path.getsize(video_path) / (1024 * 1024)
        meta = {"æ–‡ä»¶å": os.path.basename(video_path), "æ–‡ä»¶å¤§å° (MB)": f"{file_size_mb:.2f}", "æ—¶é•¿ (ç§’)": f"{duration:.2f}", "åˆ†è¾¨ç‡": f"{width}x{height}", "å¸§ç‡": f"{fps:.2f}", "æ€»å¸§æ•°": frame_count, "ç¼–ç æ ¼å¼": codec or "æœªçŸ¥"}
        logger.info(f"åŸºæœ¬å…ƒæ•°æ®æå–æˆåŠŸ: {meta}")
        return meta, duration
    except Exception as e:
        logger.error(f"æå–å…ƒæ•°æ®å¤±è´¥: {e}")
        return {}, 0

def detect_ollama_models(url: str = "http://localhost:11434") -> List[str]:
    try:
        response = requests.get(f"{url.rstrip('/')}/api/tags", timeout=3)
        response.raise_for_status()
        return sorted([model["name"] for model in response.json().get("models", [])])
    except requests.exceptions.RequestException:
        return []

def refresh_models_action():
    logger.info("UIæ“ä½œï¼šåˆ·æ–°å¯ç”¨Ollamaæ¨¡å‹åˆ—è¡¨")
    models = detect_ollama_models()
    return gr.update(choices=models, value=models[0] if models else None)

def get_ollama_status():
    status_text, running_models_data, running_model_names = "", [], []
    try:
        response = requests.get("http://localhost:11434/", timeout=3)
        response.raise_for_status()
        ps_response = requests.get("http://localhost:11434/api/ps", timeout=3)
        ps_response.raise_for_status()
        models_info = ps_response.json().get("models", [])
        status_text += "âœ… **Ollama æœåŠ¡åœ¨çº¿**\n\n" + ("å½“å‰æ²¡æœ‰æ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­ã€‚\n\n" if not models_info else "")
        for model in models_info:
            running_model_names.append(model['name'])
            running_models_data.append([model['name'], f"{model['size'] / 1e9:.2f} GB"])
        if NVIDIA_GPU_AVAILABLE:
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            total_vram_gb, used_vram_gb = mem_info.total / 1e9, mem_info.used / 1e9
            status_text += f"**GPUçŠ¶æ€**: NVIDIA GPU | æ€»æ˜¾å­˜: {total_vram_gb:.2f} GB | å·²ç”¨: {used_vram_gb:.2f} GB\n\n"
            if total_vram_gb < 20: status_text += "<p style='color:orange;'>âš ï¸ **è­¦å‘Š**: æ‚¨çš„æ€»æ˜¾å­˜ä½äº20GBï¼ŒOllamaå¯èƒ½å·²è¿›å…¥ä½æ˜¾å­˜æ¨¡å¼ï¼Œæ€§èƒ½ä¼šå—å½±å“ã€‚</p>"
    except requests.exceptions.RequestException:
        status_text = "<p style='color:red;'>âŒ **é”™è¯¯**: Ollama æœåŠ¡æœªè¿è¡Œæˆ–æ— æ³•è®¿é—®ã€‚è¯·å…ˆåœ¨ç»ˆç«¯å¯åŠ¨OllamaæœåŠ¡ã€‚</p>"
    except Exception as e:
        status_text = f"<p style='color:red;'>âŒ **é”™è¯¯**: è·å–OllamaçŠ¶æ€æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}</p>"
    return status_text, running_models_data, gr.update(choices=running_model_names, interactive=True)

def unload_ollama_model(model_to_unload):
    logger.info(f"UIæ“ä½œï¼šå°è¯•ä»å†…å­˜å¸è½½æ¨¡å‹ '{model_to_unload}'")
    if not model_to_unload:
        gr.Warning("è¯·ä»ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©ä¸€ä¸ªè¦å¸è½½çš„æ¨¡å‹ã€‚")
        return get_ollama_status()
    try:
        response = requests.post("http://localhost:11434/api/unload", json={"name": model_to_unload}, timeout=10)
        if response.status_code == 404: gr.Warning(f"æ¨¡å‹ '{model_to_unload}' æœªæ‰¾åˆ°ï¼Œå¯èƒ½å·²è¢«å¸è½½ã€‚")
        response.raise_for_status()
        time.sleep(2)
        gr.Info(f"âœ… æ¨¡å‹ '{model_to_unload}' å·²æˆåŠŸä»å†…å­˜ä¸­å¸è½½ã€‚")
    except requests.exceptions.RequestException as e:
        gr.Error(f"å¸è½½æ¨¡å‹æ—¶è¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        gr.Error(f"å¸è½½æ¨¡å‹æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    return get_ollama_status()

def detect_and_set_context(model_name):
    logger.info(f"UIæ“ä½œï¼šæ£€æµ‹æ¨¡å‹ '{model_name}' çš„æ¨èä¸Šä¸‹æ–‡é•¿åº¦")
    if not model_name:
        gr.Warning("è¯·å…ˆåœ¨ä¸Šæ–¹é€‰æ‹©ä¸€ä¸ªOllamaæ¨¡å‹ã€‚")
        return 2048
    try:
        response = requests.post("http://localhost:11434/api/show", json={"name": model_name}, timeout=10)
        response.raise_for_status()
        details = response.json()
        parameters_str = details.get("parameters", "")
        for line in parameters_str.split('\n'):
            if line.startswith("num_ctx"):
                try:
                    context_size = int(line.split()[1])
                    gr.Info(f"âœ… å·²è®¾ç½®ä¸ºæ¨¡å‹æ¨èçš„æœ€å¤§ä¸Šä¸‹æ–‡: {context_size}")
                    return context_size
                except (ValueError, IndexError): continue
        gr.Warning("âš ï¸ æœªèƒ½è‡ªåŠ¨æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå·²è®¾ä¸ºé»˜è®¤å€¼4096ã€‚")
        return 4096
    except Exception as e:
        gr.Error(f"âŒ æ£€æµ‹å¤±è´¥: {e}")
        return 2048

def load_model_action(client_type, ollama_model, api_key, api_url, api_model):
    logger.info(f"UIæ“ä½œï¼šå¼€å§‹åŠ è½½æ¨¡å‹ã€‚ç±»å‹: {client_type}, Ollamaæ¨¡å‹: {ollama_model}, APIæ¨¡å‹: {api_model}")
    try:
        if client_type == "Ollama":
            model_name_to_load = ollama_model
            if not model_name_to_load: raise ValueError("Ollamaæ¨¡å‹åç§°ä¸èƒ½ä¸ºç©ºã€‚")
            client = OllamaClient()
        elif client_type == "OpenAI-compatible API":
            model_name_to_load = api_model
            if not api_url: raise ValueError("API URL ä¸èƒ½ä¸ºç©ºã€‚")
            client = GenericOpenAIAPIClient(api_key=api_key, api_url=api_url)
        else:
            raise ValueError(f"æœªçŸ¥çš„å®¢æˆ·ç«¯ç±»å‹: {client_type}")
        prompts_config = [{"name": "Video Summary", "path": "frame_analysis/video_summary.txt"}]
        prompt_loader = PromptLoader(prompt_dir="prompts", prompts_config=prompts_config)
        app_state.analyzer = VideoAnalyzer(client, model_name_to_load, prompt_loader)
        app_state.is_loaded = True
       
        gr.Info(f"âœ… å®¢æˆ·ç«¯ '{client_type}' åŠ è½½æ¨¡å‹ '{model_name_to_load}' æˆåŠŸï¼")
        return f"åŠ è½½æˆåŠŸ: {model_name_to_load}", gr.update(interactive=False), gr.update(interactive=True)
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½æ—¶å‡ºé”™: {e}", exc_info=True)
        app_state.is_loaded = False
        gr.Error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return f"é”™è¯¯: {e}", gr.update(interactive=True), gr.update(interactive=False)

def unload_model_action():
    logger.info("UIæ“ä½œï¼šå¸è½½å½“å‰åº”ç”¨å†…æ¨¡å‹")
    if app_state.is_loaded and isinstance(app_state.analyzer.client, OllamaClient):
        model_to_unload = app_state.analyzer.model
        logger.info(f"æ£€æµ‹åˆ°Ollamaå®¢æˆ·ç«¯ï¼Œå°è¯•ä»å†…å­˜å¸è½½æ¨¡å‹ '{model_to_unload}'")
        try:
            response = requests.post("http://localhost:11434/api/unload", json={"name": model_to_unload}, timeout=10)
            if response.status_code == 200: gr.Info(f"å·²å‘Ollamaå‘é€å¸è½½ '{model_to_unload}' çš„è¯·æ±‚ã€‚")
            elif response.status_code != 404: gr.Warning(f"å‘Ollamaå‘é€å¸è½½è¯·æ±‚å¤±è´¥: {response.text}")
        except Exception as e:
            gr.Warning(f"è¿æ¥Ollamaå¸è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
    app_state.is_loaded = False
    app_state.analyzer = None
    gr.Info("âœ… åº”ç”¨å†…æ¨¡å‹åŠèµ„æºå·²é‡Šæ”¾ã€‚")
    return "æ¨¡å‹å·²å¸è½½", gr.update(interactive=True), gr.update(interactive=False)

def clear_all_outputs_action():
    gr.Info("å·²æ¸…ç©ºæ‰€æœ‰è¾“å‡ºå†…å®¹ã€‚")
    return (
        update_status_and_sys_info("ç­‰å¾…ä»»åŠ¡å¼€å§‹..."),
        None, # output_report
        gr.update(value=None, visible=False), # output_metadata_table
        gr.update(value=None, visible=False), # metadata_plot
        gr.update(value=None, visible=False), # output_gallery
        gr.update(value=None, visible=False), # output_summary_video
        gr.update(value=None, visible=False), # output_gif
        gr.update(value=None, visible=False), # gif_info_md
        gr.update(value=None, visible=False), # output_metadata_json
        gr.update(value=None, visible=False), # output_summary_clips_gallery
        gr.update(visible=False), # clip_details_accordion
        None, # clip_details_md
        gr.update(visible=False), # frame_details_accordion
        None, # frame_details_md
        None, # analysis_cache_state
        gr.update(visible=True, interactive=True), # start_button
        gr.update(visible=False), # continue_button
        gr.update(visible=False), # stop_button
        gr.update(interactive=False), # refresh_summary_button
    )

# --- æ ¸å¿ƒåˆ†æå‡½æ•° ---

def phase_1_extraction(
    video_file: str, enable_audio: bool, frames_per_min: int, max_frames: int,
    output_save_path: str,
    enable_summary_video: bool, enable_gif: bool, summary_clips: int, summary_duration: float, gif_resolution: str,
    progress=gr.Progress(track_tqdm=True)
):
    if not video_file:
        gr.Error("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼")
        yield {
            run_status_html: update_status_and_sys_info("âŒ é”™è¯¯: è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼"),
            start_button: gr.update(interactive=True),
            stop_button: gr.update(interactive=False)
        }
        return
    
    global analysis_state
    analysis_state = AnalysisState(is_running=True)
   
    yield {
        run_status_html: update_status_and_sys_info("ğŸš€ é˜¶æ®µ 1: å¼€å§‹æ•°æ®æ•´ç†..."),
        start_button: gr.update(interactive=False),
        stop_button: gr.update(interactive=True),
        continue_button: gr.update(visible=False),
        refresh_summary_button: gr.update(visible=False),
        output_report: "", output_metadata_table: gr.update(visible=False),
        metadata_plot: gr.update(visible=False), output_gallery: gr.update(visible=False),
        output_metadata_json: gr.update(visible=False), output_summary_video: gr.update(visible=False),
        output_gif: gr.update(visible=False), gif_info_md: gr.update(visible=False),
        frame_details_accordion: gr.update(visible=False),
        output_summary_clips_gallery: gr.update(visible=False), clip_details_accordion: gr.update(visible=False)
    }
    
    output_dir = Path(output_save_path) if output_save_path else Path("gradio_output")
    video_path = Path(video_file)
    video_output_dir = output_dir / video_path.stem
   
    cache = {"video_path": str(video_path), "output_dir": str(video_output_dir), "frames": [], "transcript": None, "metadata": {}, "plot": None, "media_info_json": {}, "video_duration": 0}
    
    try:
        # 1. æå–å…³é”®å¸§
        progress(0.1, desc="æå–å…³é”®å¸§...")
        yield {run_status_html: update_status_and_sys_info(f"å¤„ç†è§†é¢‘: {video_path.name}<br>é˜¶æ®µ 1: æå–å…³é”®å¸§...")}
        frame_processor = VideoProcessor(video_path, video_output_dir / "frames")
        frames = frame_processor.extract_keyframes(frames_per_minute=frames_per_min, max_frames=max_frames)
        if not frames:
            raise ValueError("æœªèƒ½ä»è§†é¢‘ä¸­æå–ä»»ä½•å…³é”®å¸§ã€‚")
        cache["frames"] = frames
        gallery_items = [(str(f.path), f"æ—¶é—´: {f.timestamp:.2f}s") for f in frames]
        yield {output_gallery: gr.update(value=gallery_items, visible=True)}
        gr.Info("âœ… å…³é”®å¸§ç”»å»Šå·²ç”Ÿæˆï¼")

        # 2. åˆ†æå…ƒæ•°æ®ä¸ç”»è´¨
        progress(0.3, desc="åˆ†æå…ƒæ•°æ®ä¸ç”»è´¨...")
        yield {run_status_html: update_status_and_sys_info(f"å¤„ç†è§†é¢‘: {video_path.name}<br>é˜¶æ®µ 1: åˆ†æå…ƒæ•°æ®ä¸ç”»è´¨...")}
        basic_meta, duration = get_video_metadata(str(video_path))
        cache["video_duration"] = duration
        adv_metrics, plot = get_advanced_video_metrics(str(video_path))
        meta_md = f"### ğŸ“Š è§†é¢‘å…ƒæ•°æ®: {video_path.name}\n\n| å‚æ•° | å€¼ |\n|---|---|\n"
        for k, v in basic_meta.items(): meta_md += f"| {k} | {v} |\n"
        for k, v in adv_metrics.items(): meta_md += f"| {k} | {f'{v:.2f}' if isinstance(v, float) else v} |\n"
        cache["metadata"] = meta_md
        cache["plot"] = plot
        yield {output_metadata_table: gr.update(value=meta_md, visible=True), metadata_plot: gr.update(value=plot, visible=True)}
        if MEDIAINFO_AVAILABLE:
            media_info_json = MediaInfo.parse(str(video_path)).to_data()
            cache["media_info_json"] = media_info_json
            yield {output_metadata_json: gr.update(value={"media_info": media_info_json}, visible=True)}

        # 3. å¤„ç†éŸ³é¢‘
        transcript_obj = AudioTranscript(text="ï¼ˆéŸ³é¢‘åˆ†æå·²ç¦ç”¨ï¼‰", segments=[], language="")
        if enable_audio:
            progress(0.5, desc="æå–å¹¶è½¬å½•éŸ³é¢‘...")
            yield {run_status_html: update_status_and_sys_info(f"å¤„ç†è§†é¢‘: {video_path.name}<br>é˜¶æ®µ 1: å¤„ç†éŸ³é¢‘...")}
            audio_processor = AudioProcessor()
            audio_file_path = audio_processor.extract_audio(video_path, video_output_dir)
            if audio_file_path:
                transcript_obj = audio_processor.transcribe(audio_file_path) or transcript_obj
        cache["transcript"] = transcript_obj

        # 4. ç”Ÿæˆæ‘˜è¦åª’ä½“æ–‡ä»¶
        clip_paths, selected_frames, concat_video_path, gif_path = None, None, None, None
        if enable_summary_video or enable_gif:
            progress(0.7, desc="ç”Ÿæˆæ‘˜è¦åª’ä½“...")
            yield {run_status_html: update_status_and_sys_info("é˜¶æ®µ 1: ç”Ÿæˆæ‘˜è¦åª’ä½“...")}
            
            clip_paths, selected_frames, concat_video_path, gif_path = create_summary_media_artifacts(
                original_video_path=str(video_path),
                video_duration=cache["video_duration"],
                frames=cache["frames"],
                output_dir=video_output_dir,
                video_stem=video_path.stem,
                num_clips=summary_clips,
                clip_duration_around_keyframe=summary_duration,
                make_video=enable_summary_video,
                make_gif=enable_gif,
                gif_resolution=gif_resolution
            )
            cache["selected_summary_frames"] = selected_frames
            
            summary_clip_gallery_items = []
            if clip_paths and selected_frames:
                summary_clip_gallery_items = [
                    (path, f"ç‰‡æ®µä¸­å¿ƒ: {frame.timestamp:.2f}s")
                    for path, frame in zip(clip_paths, selected_frames)
                ]
                gr.Info(f"âœ… ç”Ÿæˆ {len(summary_clip_gallery_items)} ä¸ªæ‘˜è¦ç‰‡æ®µã€‚")

            gif_info_text = ""
            if gif_path and os.path.exists(gif_path):
                gif_size_kb = os.path.getsize(gif_path) / 1024
                gif_size_mb = gif_size_kb / 1024
                gif_info_text = f"**åŠ¨å›¾æ–‡ä»¶å¤§å°:** {gif_size_kb:.2f} KB ({gif_size_mb:.2f} MB)"
            
            yield {
                output_summary_clips_gallery: gr.update(value=summary_clip_gallery_items, visible=bool(summary_clip_gallery_items)),
                output_summary_video: gr.update(value=concat_video_path, visible=bool(concat_video_path)),
                output_gif: gr.update(value=gif_path, visible=bool(gif_path)),
                gif_info_md: gr.update(value=gif_info_text, visible=bool(gif_info_text)),
            }

        # 5. é˜¶æ®µä¸€å®Œæˆ
        analysis_state.status_message = "âœ… æ•°æ®æ•´ç†å®Œæˆï¼Œç‚¹å‡»ç»§ç»­ç”ŸæˆAIæ€»ç»“"
        progress(1.0)
       
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            start_button: gr.update(visible=False),
            stop_button: gr.update(interactive=False),
            continue_button: gr.update(visible=True, interactive=True),
            analysis_cache_state: cache
        }
    except Exception as e:
        logger.error(f"æ•°æ®æå–é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        analysis_state.status_message = f"âŒ é”™è¯¯: {e}"
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            start_button: gr.update(interactive=True),
            stop_button: gr.update(interactive=False),
            analysis_cache_state: None
        }
    finally:
        analysis_state.is_running = False
        analysis_state.stop_requested = False

def phase_2_ai_analysis(
    cache: Dict, prompt_choice: str, custom_prompt: str, temperature: float, context_length: int,
    progress=gr.Progress(track_tqdm=True)
):
    if not app_state.is_loaded or not app_state.analyzer:
        gr.Error("æ¨¡å‹å°šæœªåŠ è½½ï¼")
        return
    if not cache:
        gr.Error("åˆ†æç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆæ‰§è¡Œç¬¬ä¸€é˜¶æ®µçš„æ•°æ®æå–ï¼")
        return
        
    global analysis_state
    analysis_state = AnalysisState(is_running=True)
    
    logger.info("AIæ‘˜è¦ç”Ÿæˆå¼€å§‹")
    
    yield {
        run_status_html: update_status_and_sys_info("ğŸš€ é˜¶æ®µ 2: AIæ‘˜è¦ç”Ÿæˆå¼€å§‹..."),
        continue_button: gr.update(interactive=False),
        stop_button: gr.update(interactive=True),
        refresh_summary_button: gr.update(visible=False),
        output_report: "### ğŸ“œ AI æ‘˜è¦æŠ¥å‘Š\n\n"
    }
    
    app_state.analyzer.user_prompt = custom_prompt if prompt_choice == "è‡ªå®šä¹‰" else PRESET_PROMPTS[prompt_choice]
    app_state.analyzer.temperature = temperature
    app_state.analyzer.context_length = int(context_length)
    
    try:
        progress(0, desc="AIæ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
        current_report = "### ğŸ“œ AI æ‘˜è¦æŠ¥å‘Š\n\n"
        final_summary_text = ""
       
        stream = app_state.analyzer.summarize_all_frames_stream(cache["frames"], cache["transcript"])
        for chunk in stream:
            if analysis_state.stop_requested: raise InterruptedError("ç”¨æˆ·è¯·æ±‚åœæ­¢")
            if "__FULL_RESPONSE_END__" in chunk:
                final_summary_text = chunk.split("__FULL_RESPONSE_END__")[1]
                break
            current_report += chunk
            yield {output_report: current_report}
       
        if analysis_state.stop_requested: raise InterruptedError("ç”¨æˆ·è¯·æ±‚åœæ­¢")
        
        logger.info("AIæ‘˜è¦ç”Ÿæˆç»“æŸ")
        
        if MEDIAINFO_AVAILABLE:
            full_json = {"media_info": cache["media_info_json"], "ai_analysis": {"audio_transcript": cache["transcript"].text, "final_summary": final_summary_text}}
            yield {output_metadata_json: gr.update(value=full_json, visible=True)}
        
        analysis_state.status_message = "âœ… AIåˆ†æä»»åŠ¡å·²å®Œæˆï¼"
        progress(1.0)
        
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            stop_button: gr.update(interactive=False),
            continue_button: gr.update(interactive=True),
            refresh_summary_button: gr.update(visible=True, interactive=True),
            analysis_cache_state: cache
        }
    except InterruptedError:
        analysis_state.status_message = "ğŸ›‘ åˆ†æå·²ç”±ç”¨æˆ·æ‰‹åŠ¨åœæ­¢ã€‚"
        logger.info(analysis_state.status_message)
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            stop_button: gr.update(interactive=False),
            continue_button: gr.update(interactive=True),
            refresh_summary_button: gr.update(visible=True, interactive=True),
        }
    except Exception as e:
        logger.error(f"AIåˆ†æé˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        analysis_state.status_message = f"âŒ ä¸¥é‡é”™è¯¯: {e}"
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            stop_button: gr.update(interactive=False),
            continue_button: gr.update(interactive=True),
        }
    finally:
        analysis_state.is_running = False
        analysis_state.stop_requested = False

def stop_analysis_func():
    if analysis_state.is_running:
        analysis_state.stop_requested = True
        logger.warning("æ”¶åˆ°åœæ­¢è¯·æ±‚ï¼Œå°†åœ¨å½“å‰æ­¥éª¤å®Œæˆåä¸­æ–­åˆ†æã€‚")
    return gr.update(interactive=False)

def save_settings(*args):
    keys = ["client_type", "ollama_model", "api_key", "api_url", "api_model", "prompt_choice", "custom_prompt", "temperature", "enable_audio", "frames_per_min", "max_frames", "context_length", "enable_summary_video", "enable_gif", "summary_clips", "summary_duration", "gif_resolution", "output_path"]
    settings = dict(zip(keys, args))
    with open(SETTINGS_FILE, "w", encoding="utf-8") as f: json.dump(settings, f, indent=2)
    gr.Info("âœ… è®¾ç½®å·²ä¿å­˜ï¼")

def load_settings_and_refresh_models():
    ollama_models = detect_ollama_models()
    default_settings = ["Ollama", gr.update(choices=ollama_models, value=ollama_models[0] if ollama_models else None), "", "http://localhost:1234/v1", "", "å†…å®¹æ€»ç»“ä¸è¯„ä¼°", "", 0.5, True, 30, 25, 4096, True, False, 10, 5.0, "ä¸­", "gradio_output"]
    if not SETTINGS_FILE.exists(): return default_settings
    try:
        with open(SETTINGS_FILE, "r", encoding="utf-8") as f: settings = json.load(f)
        saved_model = settings.get("ollama_model", "")
        selected_model = saved_model if saved_model in ollama_models else (ollama_models[0] if ollama_models else None)
        return [
            settings.get("client_type", "Ollama"),
            gr.update(choices=ollama_models, value=selected_model),
            settings.get("api_key", ""),
            settings.get("api_url", "http://localhost:1234/v1"),
            settings.get("api_model", ""),
            settings.get("prompt_choice", "å†…å®¹æ€»ç»“ä¸è¯„ä¼°"),
            settings.get("custom_prompt", ""),
            settings.get("temperature", 0.5),
            settings.get("enable_audio", True),
            settings.get("frames_per_min", 30),
            settings.get("max_frames", 25),
            settings.get("context_length", 4096),
            settings.get("enable_summary_video", True),
            settings.get("enable_gif", False),
            settings.get("summary_clips", 10),
            settings.get("summary_duration", 5.0),
            settings.get("gif_resolution", "ä¸­"),
            settings.get("output_path", "gradio_output")
        ]
    except (json.JSONDecodeError, KeyError):
        logger.warning("æ— æ³•è§£æè®¾ç½®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®ã€‚")
        return default_settings

def show_frame_details(cache: Dict, evt: gr.SelectData):
    if not cache or not cache.get("frames"):
        return gr.update(visible=False), ""
   
    try:
        selected_frame: Frame = cache["frames"][evt.index]
        metrics = selected_frame.metrics
       
        md_text = f"#### ğŸ–¼ï¸ å¸§è¯¦æƒ… (æ—¶é—´: {selected_frame.timestamp:.2f}s)\n\n"
        md_text += "| å‚æ•° | å€¼ |\n|---|---|\n"
        for key, value in metrics.items():
            md_text += f"| {key} | {value:.2f} |\n"
           
        return gr.update(visible=True), md_text
    except (IndexError, KeyError) as e:
        logger.warning(f"æ— æ³•æ˜¾ç¤ºå¸§è¯¦æƒ…: {e}")
        return gr.update(visible=False), "æ— æ³•åŠ è½½è¯¥å¸§çš„è¯¦ç»†æ•°æ®ã€‚"

def show_clip_details(cache: Dict, evt: gr.SelectData):
    if not cache or not cache.get("selected_summary_frames"):
        return gr.update(visible=False), ""
   
    try:
        selected_frame: Frame = cache["selected_summary_frames"][evt.index]
        metrics = selected_frame.metrics
       
        md_text = f"#### ğŸ¬ ç‰‡æ®µä¸­å¿ƒå¸§è¯¦æƒ… (æ—¶é—´: {selected_frame.timestamp:.2f}s)\n\n"
        md_text += "æ­¤ç‰‡æ®µæ˜¯å›´ç»•è¯¥æ—¶é—´ç‚¹çš„å…³é”®å¸§ç”Ÿæˆçš„ã€‚\n\n"
        md_text += "| å‚æ•° | å€¼ |\n|---|---|\n"
        for key, value in metrics.items():
            md_text += f"| {key} | {value:.2f} |\n"
           
        logger.info(f"æ˜¾ç¤ºç‰‡æ®µè¯¦æƒ…: æ—¶é—´ {selected_frame.timestamp:.2f}s")
        return gr.update(visible=True), md_text
    except (IndexError, KeyError) as e:
        logger.warning(f"æ— æ³•æ˜¾ç¤ºç‰‡æ®µè¯¦æƒ…: {e}")
        return gr.update(visible=False), "æ— æ³•åŠ è½½è¯¥ç‰‡æ®µçš„è¯¦ç»†æ•°æ®ã€‚"

# ==============================================================================
# é˜¶æ®µä¸‰ï¼šUIå®šä¹‰ä¸å¯åŠ¨åŒº
# ==============================================================================
CSS = """.stats-container { display: flex; flex-wrap: wrap; justify-content: space-between; gap: 10px; font-size: 0.9em; } .stat-item { flex: 1; min-width: 120px; background-color: #f0f0f0; border-radius: 5px; padding: 5px; } .label { font-weight: bold; } .value { float: right; } .bar-container { width: 100%; background-color: #e0e0e0; border-radius: 3px; height: 8px; margin-top: 3px; } .bar { height: 100%; border-radius: 3px; } .cpu { background-color: #4CAF50; } .ram { background-color: #2196F3; } .gpu { background-color: #ff9800; } .vram { background-color: #f44336; } footer { display: none !important; }"""

def create_ui():
    global status_box, client_type_dd, model_select, api_key_txt, api_url_txt, api_model_txt, load_button, unload_button
    global output_report, output_metadata_table, metadata_plot, output_gallery, output_summary_video, output_gif, output_metadata_json
    global output_summary_clips_gallery, clip_details_accordion, clip_details_md
    global run_status_html, analysis_progress, start_button, continue_button, stop_button, refresh_summary_button, clear_outputs_button
    global frame_details_accordion, frame_details_md, analysis_cache_state, gif_info_md
   
    with gr.Blocks(css=CSS, title="è§†é¢‘æ·±åº¦åˆ†æå¹³å°", theme=gr.themes.Soft()) as iface:
        analysis_cache_state = gr.State(None)
        gr.Markdown("# ğŸš€ è§†é¢‘æ·±åº¦åˆ†æå¹³å° (V3.5 ç¨³å®šç‰ˆ)")
        if not FONT_LOADED_SUCCESSFULLY:
            gr.Markdown("<div style='background-color: #FFDDDD; color: #D8000C; padding: 10px; border-radius: 5px;'>âš ï¸ **ä¸¥é‡è­¦å‘Š**: æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„ä¸­æ–‡å­—ä½“ã€‚å›¾è¡¨ä¸­çš„ä¸­æ–‡å°†æ˜¾ç¤ºä¸ºæ–¹æ¡†ã€‚</div>")
        if not FFMPEG_AVAILABLE:
            gr.Markdown("<div style='background-color: #FFDDDD; color: #D8000C; padding: 10px; border-radius: 5px;'>âš ï¸ **FFmpeg è­¦å‘Š**: æœªæ£€æµ‹åˆ° FFmpegã€‚AIæ‘˜è¦è§†é¢‘/GIF å°†æ— æ³•ç”Ÿæˆã€‚è¯·å®‰è£… FFmpegã€‚</div>")
       
        with gr.Row():
            with gr.Column(scale=1):
                with gr.Accordion("1. æ¨¡å‹é…ç½®", open=True):
                    status_box = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", value="æœªåŠ è½½", interactive=False)
                    client_type_dd = gr.Dropdown(["Ollama", "OpenAI-compatible API"], label="å®¢æˆ·ç«¯ç±»å‹", value="Ollama")
                    with gr.Group(visible=True) as ollama_group:
                        model_select = gr.Dropdown(label="é€‰æ‹©Ollamaæ¨¡å‹", interactive=True, info="è¯·ç¡®ä¿é€‰æ‹©çš„æ˜¯å¤šæ¨¡æ€(VL)æ¨¡å‹ï¼Œå¦‚qwen-vl, llavaç­‰")
                        refresh_button = gr.Button("ğŸ”„ åˆ·æ–°å¯ç”¨æ¨¡å‹")
                    with gr.Group(visible=False) as api_group:
                        api_url_txt = gr.Textbox(label="API URL (LM Studio / OpenAI)", value="http://localhost:1234/v1", placeholder="ä¾‹å¦‚: http://localhost:1234/v1")
                        api_key_txt = gr.Textbox(label="API Key (å¯é€‰)", type="password", placeholder="æœ¬åœ°æœåŠ¡é€šå¸¸æ— éœ€å¡«å†™")
                        api_model_txt = gr.Textbox(label="æ¨¡å‹åç§°", placeholder="ä¾‹å¦‚: åœ¨LM Studioä¸­åŠ è½½çš„æ¨¡å‹ID")
                    load_button = gr.Button("âœ… åŠ è½½æ¨¡å‹", variant="primary")
                    unload_button = gr.Button("å¸è½½æ¨¡å‹", interactive=False)
               
                with gr.Accordion("Ollama çŠ¶æ€ç›‘æ§ä¸ç®¡ç†", open=False):
                    ollama_status_markdown = gr.Markdown("æ­£åœ¨è·å–çŠ¶æ€...")
                    running_models_df = gr.DataFrame(headers=["è¿è¡Œä¸­çš„æ¨¡å‹", "å ç”¨å†…å­˜"], interactive=False, row_count=(0, "dynamic"))
                    with gr.Row():
                        unload_model_dd = gr.Dropdown(label="é€‰æ‹©è¦å¸è½½çš„æ¨¡å‹", interactive=True)
                        unload_model_button = gr.Button("âš¡ å¸è½½é€‰ä¸­æ¨¡å‹")
                    refresh_status_button = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", elem_id="refresh_ollama_status_button")
               
                with gr.Accordion("2. ä¸Šä¼ ä¸åˆ†æè®¾ç½®", open=True):
                    file_output = gr.File(label="å¾…åˆ†æè§†é¢‘", file_count="single", interactive=True, file_types=["video"])
                    upload_button = gr.UploadButton("ğŸ“ ç‚¹å‡»æˆ–æ‹–æ‹½å•ä¸ªè§†é¢‘ä¸Šä¼ ", file_count="single", file_types=["video"])
                    prompt_choice_dd = gr.Dropdown(label="é€‰æ‹©æç¤ºè¯æ¨¡æ¿", choices=list(PRESET_PROMPTS.keys()))
                    custom_prompt_txt = gr.Textbox(label="è‡ªå®šä¹‰æç¤ºè¯", lines=3, visible=False)
                    temp_slider = gr.Slider(0.0, 1.5, step=0.1, label="æ¸©åº¦ (Temperature)")
                    output_path_txt = gr.Textbox(label="åˆ†æç»“æœè¾“å‡ºè·¯å¾„", value="gradio_output")
               
                with gr.Accordion("3. é«˜çº§å‚æ•°ä¸ç»´æŠ¤", open=False):
                    frames_per_min_slider = gr.Slider(1, 120, step=1, label="æ¯åˆ†é’Ÿå…³é”®å¸§æ•°")
                    max_frames_slider = gr.Slider(5, 100, step=1, label="æœ€å¤§æ€»å¸§æ•°")
                    context_length_slider = gr.Slider(1024, 16384, step=256, label="æ¨¡å‹ä¸Šä¸‹æ–‡ (Context) é•¿åº¦", value=4096)
                    detect_context_button = gr.Button("ğŸ” æ£€æµ‹å¹¶è®¾ç½®æ¨èä¸Šä¸‹æ–‡")
                    enable_audio_checkbox = gr.Checkbox(label="å¯ç”¨éŸ³é¢‘è½¬å½•")
                    with gr.Group():
                        gr.Markdown("#### AIæ‘˜è¦ä¸GIFç”Ÿæˆ")
                        enable_summary_video_cb = gr.Checkbox(label="ç”ŸæˆAIæ‘˜è¦åª’ä½“", value=True)
                        enable_gif_cb = gr.Checkbox(label="ç”ŸæˆGIFåŠ¨å›¾")
                        summary_clips_slider = gr.Slider(3, 30, step=1, label="æ‘˜è¦ç‰‡æ®µæ•°é‡")
                        summary_duration_slider = gr.Slider(1.0, 10.0, step=0.5, label="æ¯ä¸ªç‰‡æ®µæ€»æ—¶é•¿(ç§’)")
                        gif_resolution_dd = gr.Dropdown(["ä½", "ä¸­", "é«˜"], label="GIFåˆ†è¾¨ç‡", value="ä¸­")
                    with gr.Row():
                        save_settings_button = gr.Button("ğŸ’¾ ä¿å­˜æ‰€æœ‰è®¾ç½®")
                        clear_outputs_button = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰è¾“å‡º", variant="stop")
               
                with gr.Blocks():
                    start_button = gr.Button("1. å¼€å§‹æå–æ•°æ®", variant="primary", size='lg')
                    continue_button = gr.Button("2. ç»§ç»­ç”ŸæˆAIæ€»ç»“", variant="primary", size='lg', visible=False)
                    with gr.Row():
                        stop_button = gr.Button("ğŸ›‘ åœæ­¢", variant="stop", interactive=False, scale=1)
                        refresh_summary_button = gr.Button("ğŸ”„ ä»…åˆ·æ–°AIæ€»ç»“", interactive=True, visible=False, scale=1)
            with gr.Column(scale=2):
                run_status_html = gr.HTML(update_status_and_sys_info())
                analysis_progress = gr.Progress()
                with gr.Tabs():
                    with gr.TabItem("ğŸ“ AI æ‘˜è¦æŠ¥å‘Š"):
                        output_report = gr.Markdown()
                    with gr.TabItem("ğŸ¬ æ‘˜è¦åª’ä½“"):
                        gr.Markdown("#### è§†é¢‘ç‰‡æ®µæ‘˜è¦ (å¯ç‚¹å‡»æ’­æ”¾)\nç‚¹å‡»ä¸‹æ–¹çš„è§†é¢‘ç‰‡æ®µä»¥æŸ¥çœ‹å…¶ä¸­å¿ƒå…³é”®å¸§çš„è¯¦ç»†æŠ€æœ¯æŒ‡æ ‡ã€‚")
                        output_summary_clips_gallery = gr.Gallery(label="è§†é¢‘ç‰‡æ®µæ‘˜è¦", columns=4, height="auto", object_fit="contain", visible=False, allow_preview=True)
                        with gr.Accordion("ç‰‡æ®µè¯¦æƒ…", open=False, visible=False) as clip_details_accordion:
                            clip_details_md = gr.Markdown()
                        gr.Markdown("---")
                        with gr.Row():
                            with gr.Column():
                                output_summary_video = gr.Video(label="å®Œæ•´æ‘˜è¦è§†é¢‘ (æ‹¼æ¥ç‰ˆ)", visible=False)
                            with gr.Column():
                                output_gif = gr.Image(label="æ‘˜è¦GIFåŠ¨å›¾", type="filepath", visible=False)
                                gif_info_md = gr.Markdown(visible=False)
                    with gr.TabItem("ğŸ–¼ï¸ å…³é”®å¸§ç”»å»Š"):
                        output_gallery = gr.Gallery(label="å…³é”®å¸§", columns=6, height="auto", object_fit="contain", visible=False)
                        with gr.Accordion("å•å¸§è¯¦æƒ…", open=False, visible=False) as frame_details_accordion:
                            frame_details_md = gr.Markdown()
                    with gr.TabItem("ğŸ“Š å…ƒæ•°æ®ä¸ç”»è´¨"):
                        output_metadata_table = gr.Markdown(visible=False)
                        metadata_plot = gr.Plot(label="ç”»è´¨åˆ†æå›¾", visible=False)
                    with gr.TabItem("ğŸ“„ è¯¦ç»†å…ƒæ•°æ® (JSON)"):
                        output_metadata_json = gr.JSON(label="å¯äº¤äº’çš„å…ƒæ•°æ®æ ‘çŠ¶å›¾", visible=False)
       
        all_settings = [client_type_dd, model_select, api_key_txt, api_url_txt, api_model_txt, prompt_choice_dd, custom_prompt_txt, temp_slider, enable_audio_checkbox, frames_per_min_slider, max_frames_slider, context_length_slider, enable_summary_video_cb, enable_gif_cb, summary_clips_slider, summary_duration_slider, gif_resolution_dd, output_path_txt]
       
        # --- äº‹ä»¶ç»‘å®š ---
        client_type_dd.change(lambda x: (gr.update(visible=x=="Ollama"), gr.update(visible=x!="Ollama")), client_type_dd, [ollama_group, api_group])
        prompt_choice_dd.change(lambda x: gr.update(visible=x=="è‡ªå®šä¹‰"), prompt_choice_dd, custom_prompt_txt)
       
        refresh_button.click(refresh_models_action, outputs=model_select)
        load_button.click(load_model_action, [client_type_dd, model_select, api_key_txt, api_url_txt, api_model_txt], [status_box, load_button, unload_button])
        unload_button.click(unload_model_action, outputs=[status_box, load_button, unload_button])
       
        save_settings_button.click(save_settings, all_settings)
       
        upload_button.upload(lambda file: file.name if file else None, inputs=[upload_button], outputs=[file_output])
       
        clear_outputs_button.click(
            clear_all_outputs_action,
            outputs=[
                run_status_html, output_report, output_metadata_table, metadata_plot,
                output_gallery, output_summary_video, output_gif, gif_info_md, output_metadata_json,
                output_summary_clips_gallery, clip_details_accordion, clip_details_md,
                frame_details_accordion, frame_details_md, analysis_cache_state,
                start_button, continue_button, stop_button, refresh_summary_button
            ]
        )
        
        phase1_inputs = [
            file_output, enable_audio_checkbox, frames_per_min_slider, max_frames_slider, output_path_txt,
            enable_summary_video_cb, enable_gif_cb, summary_clips_slider, summary_duration_slider, gif_resolution_dd
        ]
        phase1_outputs = [
            run_status_html, start_button, stop_button, continue_button, refresh_summary_button,
            output_report, output_metadata_table, metadata_plot, output_gallery,
            output_metadata_json, output_summary_video, output_gif, gif_info_md, frame_details_accordion,
            output_summary_clips_gallery, clip_details_accordion,
            analysis_cache_state
        ]
        start_button.click(phase_1_extraction, phase1_inputs, phase1_outputs)
        
        phase2_inputs = [
            analysis_cache_state, prompt_choice_dd, custom_prompt_txt, temp_slider, context_length_slider
        ]
        phase2_outputs = [
            run_status_html, continue_button, stop_button, refresh_summary_button, output_report,
            output_metadata_json, analysis_cache_state
        ]
        continue_button.click(phase_2_ai_analysis, phase2_inputs, phase2_outputs)
       
        refresh_summary_button.click(phase_2_ai_analysis, phase2_inputs, phase2_outputs)
        stop_button.click(stop_analysis_func, outputs=[stop_button])
       
        output_gallery.select(show_frame_details, [analysis_cache_state], [frame_details_accordion, frame_details_md])
        output_summary_clips_gallery.select(show_clip_details, [analysis_cache_state], [clip_details_accordion, clip_details_md])
       
        ollama_status_outputs = [ollama_status_markdown, running_models_df, unload_model_dd]
        refresh_status_button.click(get_ollama_status, outputs=ollama_status_outputs)
        unload_model_button.click(unload_ollama_model, inputs=[unload_model_dd], outputs=ollama_status_outputs)
        detect_context_button.click(detect_and_set_context, inputs=[model_select], outputs=[context_length_slider])
       
        iface.load(load_settings_and_refresh_models, outputs=all_settings)
        iface.load(get_ollama_status, outputs=ollama_status_outputs)
       
    return iface

if __name__ == "__main__":
    prompt_dir = Path("prompts/frame_analysis")
    prompt_dir.mkdir(parents=True, exist_ok=True)
   
    summary_prompt_path = prompt_dir / "video_summary.txt"
    summary_prompt_content = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚æ¥ä¸‹æ¥æˆ‘ä¼šç»™ä½ ä¸€ä¸ªè§†é¢‘çš„å¤šä¸ªå…³é”®å¸§å›¾åƒï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰ã€ä»¥åŠå¯é€‰çš„éŸ³é¢‘è½¬å½•å†…å®¹ã€‚\n\n"
        "ç”¨æˆ·çš„æ ¸å¿ƒåˆ†æè¦æ±‚æ˜¯ï¼š{user_prompt}\n\n"
        "---éŸ³é¢‘è½¬å½•---\n{audio_transcript}\n\n"
        "---å…³é”®å¸§æ—¶é—´ç‚¹åˆ—è¡¨---\n{frame_info}\n\n"
        "è¯·ç»¼åˆä½ çœ‹åˆ°çš„æ‰€æœ‰å›¾åƒå’Œå¬åˆ°çš„æ‰€æœ‰æ–‡æœ¬ï¼Œç”Ÿæˆä¸€ä»½å…¨é¢ã€æµç•…ã€ç»“æ„åŒ–çš„è§†é¢‘æœ€ç»ˆåˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šéœ€è¦ç›´æ¥å›åº”ç”¨æˆ·çš„æ ¸å¿ƒè¦æ±‚ã€‚\n"
        "é‡è¦æŒ‡ä»¤ï¼šè¯·å°†æ‰€æœ‰å›¾åƒè§†ä¸ºä¸€ä¸ªæ•´ä½“æ•…äº‹çº¿ï¼Œè¿›è¡Œè¿è´¯çš„å™è¿°å’Œåˆ†æï¼Œè€Œä¸æ˜¯å­¤ç«‹åœ°æè¿°æ¯ä¸€å¼ å›¾ã€‚å¦‚æœå¤šä¸ªå…³é”®å¸§å†…å®¹ç›¸ä¼¼ï¼Œè¯·è¿›è¡Œæ¦‚æ‹¬æ€§æè¿°ï¼Œé¿å…é‡å¤ã€‚\n"
        "**è¾“å‡ºè¦æ±‚**ï¼šè¯·ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼çš„æŠ¥å‘Šå…¨æ–‡ã€‚**ä¸¥ç¦**åœ¨æŠ¥å‘Šä¸­åŠ å…¥ä»»ä½•å¯¹è¯æ€§æ–‡å­—ã€æé—®ï¼ˆä¾‹å¦‚ä¸è¦è¯´â€˜ä½ å¯¹è¿™ä¸ªåˆ†ææ»¡æ„å—ï¼Ÿâ€™æˆ–â€˜ä½ çš„åé¦ˆå°†å¸®åŠ©æˆ‘...â€™ï¼‰ã€æˆ–å›¾åƒå ä½ç¬¦ï¼ˆå¦‚`[img-n]`ï¼‰ã€‚ä½ çš„å›ç­”åº”è¯¥**ä»…é™äº**æŠ¥å‘Šæœ¬èº«ï¼Œå†…å®¹ç¿”å®ï¼Œç»“æ„æ¸…æ™°ã€‚"
    )
    with open(summary_prompt_path, "w", encoding="utf-8") as f:
        f.write(summary_prompt_content)
    logger.info(f"å·²æ›´æ–°/åˆ›å»ºä¼˜åŒ–åçš„æ‘˜è¦æç¤ºè¯æ–‡ä»¶: {summary_prompt_path}")
    
    app_state.stop_monitoring.clear()
    monitor_thread = threading.Thread(target=monitor_system_stats, daemon=True)
    monitor_thread.start()
   
    iface = create_ui()
   
    try:
        logger.info("æ­£åœ¨å¯åŠ¨ Gradio Web å¹³å°...")
        logger.info("è„šæœ¬å°†è‡ªåŠ¨æ‰“å¼€æ‚¨çš„æµè§ˆå™¨ã€‚")
        logger.info("æ‚¨ä¹Ÿå¯ä»¥åœ¨æœ¬åœ°é€šè¿‡ http://0.0.0.0:8001 è®¿é—®ã€‚")
        iface.queue().launch(server_name="0.0.0.0", server_port=8001, debug=False, inbrowser=True)
    except (KeyboardInterrupt, OSError):
        logger.info("\næ­£åœ¨å¹³ç¨³å…³é—­ï¼Œè¯·ç¨å€™...")
    finally:
        analysis_state.stop_requested = True
        app_state.stop_monitoring.set()
        if NVIDIA_GPU_AVAILABLE:
            try:
                pynvml.nvmlShutdown()
                logger.info("pynvml å·²æˆåŠŸå…³é—­ã€‚")
            except: pass
        logger.info("åº”ç”¨ç¨‹åºå·²å…³é—­ã€‚")


--- æ–‡ä»¶è·¯å¾„: requirements.txt ---

# ===================================================================
#  Video Analyzer - ç»ˆæç¨³å®šç‰ˆä¾èµ–æ¸…å•
# ===================================================================

# --- æ ¸å¿ƒåŠŸèƒ½ä¸UI (Gradio å·²è§£é™¤ç‰ˆæœ¬é”å®šï¼Œå°†å®‰è£…æœ€æ–°ç‰ˆ) ---
gradio
opencv-python>=4.8.0
numpy>=1.24.0,<2.0.0
requests>=2.31.0
Pillow==10.2.0
pydub>=0.25.1

# --- AIæ¨¡å‹ä¸éŸ³é¢‘å¤„ç† ---
torch>=2.0.0
openai-whisper>=20231117
faster-whisper>=0.6.0

# --- é«˜çº§åŠŸèƒ½ä¸ç³»ç»Ÿç›‘æ§ (é”å®š moviepy åˆ°ç°ä»£ç¨³å®šç‰ˆ) ---
moviepy==1.0.3
matplotlib==3.8.2
seaborn==0.13.2
pymediainfo
psutil
nvidia-ml-py==12.535.108

--- æ–‡ä»¶è·¯å¾„: setup.py ---

from setuptools import setup, find_packages
import os
from pathlib import Path

with open("readme.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = [line.strip() for line in fh if line.strip() and not line.startswith("#")]

setup(
    name="video-analyzer",
    version="0.1.1",
    author="Jesse White",
    description="A tool for analyzing videos using Vision models",
    long_description=long_description,
    long_description_content_type="text/markdown",
    packages=find_packages(),
    package_data={
        'video_analyzer': [
            'config/*.json',
            'prompts/**/*',
        ],
    },
    install_requires=requirements,
    entry_points={
        "console_scripts": [
            "video-analyzer=video_analyzer.cli:main",
        ],
    },
    python_requires=">=3.8",
    include_package_data=True
)


--- æ–‡ä»¶è·¯å¾„: start_windows.bat ---

@echo off
TITLE Video Analysis Platform - Ultimate Launcher (v9.0 FINAL)

REM --- ä½¿ç”¨ %~dp0 ç¡®ä¿æˆ‘ä»¬æ€»æ˜¯åœ¨è„šæœ¬æ‰€åœ¨çš„ç›®å½•ä¸­æ“ä½œ ---
echo Changing directory to the script's location...
cd /d "%~dp0"

REM --- æ£€æŸ¥Pythonæ˜¯å¦å·²å®‰è£… ---
python --version >nul 2>&1
IF %ERRORLEVEL% NEQ 0 (
    echo.
    echo ====================================================================
    echo ERROR: Python is not found in your system's PATH.
    echo Please install Python 3.8+ and ensure it's added to the PATH.
    echo ====================================================================
    echo.
    pause
    exit /b
)

REM --- [ç»ˆææ”¹åŠ¨] è‡ªåŠ¨æ£€æµ‹å¹¶åˆ›å»º/æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ ---
IF EXIST ".\.venv\Scripts\activate.bat" (
    echo Found .venv virtual environment. Activating...
) ELSE (
    echo.
    echo ====================================================================
    echo Virtual environment not found. Creating it now...
    echo This may take a moment.
    echo ====================================================================
    echo.
    python -m venv .venv
    IF %ERRORLEVEL% NEQ 0 (
        echo.
        echo ====================================================================
        echo ERROR: Failed to create the virtual environment.
        echo Please check your Python installation.
        echo ====================================================================
        echo.
        pause
        exit /b
    )
    echo Virtual environment created successfully. Activating...
)
call .\.venv\Scripts\activate.bat

REM --- [æ–°å¢] è‡ªåŠ¨æ›´æ–°pipåˆ°æœ€æ–°ç‰ˆæœ¬ ---
echo.
echo ====================================================================
echo  Updating pip to the latest version...
echo ====================================================================
python.exe -m pip install --upgrade pip

REM --- [ç»ˆææ”¹åŠ¨] è‡ªåŠ¨è¯»å– requirements.txt å¹¶å®‰è£…æ‰€æœ‰ä¾èµ– ---
echo.
echo ====================================================================
echo  Checking and installing all required dependencies from requirements.txt...
echo  This might take some time on the first run.
echo ====================================================================
echo.
pip install -r requirements.txt
echo.
echo ====================================================================
echo  Dependency check complete.
echo ====================================================================
echo.


REM --- å¯åŠ¨Gradio Webåº”ç”¨ ---
echo.
echo Starting the Gradio Web Platform...
echo The Python script will now automatically open your browser to http://127.0.0.1:8001 once the server is ready.
echo.

python app.py

echo.
echo The application has been closed.
pause


--- æ–‡ä»¶è·¯å¾„: test_prompt_loading.py ---

#!/usr/bin/env python3
"""Test script for the prompt loading system."""
import tempfile
from pathlib import Path
from video_analyzer.prompt import PromptLoader

def test_prompt_loading():
    """Test prompt loading with package and custom prompts."""
    prompts = [
        {
            "name": "Frame Analysis",
            "path": "frame_analysis/frame_analysis.txt"
        },
        {
            "name": "Video Reconstruction", 
            "path": "frame_analysis/describe.txt"
        }
    ]
    
    # Test default package prompts
    loader = PromptLoader("", prompts)
    assert loader.get_by_name("Frame Analysis"), "Failed to load package frame analysis prompt"
    assert loader.get_by_name("Video Reconstruction"), "Failed to load package describe prompt"
    
    # Test custom prompts in temporary directory
    with tempfile.TemporaryDirectory() as temp_dir:
        test_dir = Path(temp_dir) / 'test_prompts'
        frame_dir = test_dir / 'frame_analysis'
        frame_dir.mkdir(parents=True)
        
        (frame_dir / 'frame_analysis.txt').write_text('Test frame analysis')
        (frame_dir / 'describe.txt').write_text('Test description')
        
        loader = PromptLoader(str(test_dir), prompts)
        assert loader.get_by_name("Frame Analysis") == 'Test frame analysis'
        assert loader.get_by_name("Video Reconstruction") == 'Test description'

if __name__ == "__main__":
    test_prompt_loading()
    print("All tests passed!")


--- æ–‡ä»¶è·¯å¾„: ui_settings.json ---

{
  "client_type": "Ollama",
  "ollama_model": "Qwen2.5-VL-3B-Abliterated-Caption-it.IQ4_XS.gguf:latest",
  "api_key": "",
  "api_url": "",
  "api_model": "",
  "prompt_choice": "\u5185\u5bb9\u603b\u7ed3\u4e0e\u8bc4\u4f30",
  "custom_prompt": "",
  "temperature": 0.5,
  "enable_audio": false,
  "frames_per_min": 13,
  "max_frames": 5,
  "context_length": 4096,
  "enable_summary_video": true,
  "enable_gif": true,
  "summary_clips": 3,
  "summary_duration": 5,
  "gif_resolution": "\u4f4e",
  "output_path": "analysis_results"
}

--- æ–‡ä»¶è·¯å¾„: docs\AI.md ---

# Rules and Guidelines for AI Assistants and Agents

### Code Structure and Organization

**Modularity**:
* Keep files small and modular.
* No file must exceed 400 lines of code**.
* If a file becomes too large, refactor by breaking down classes or functions into smaller modules.

**Component Management**:

* Split large components into smaller, manageable parts to enhance readability and maintainability.

**Dependencies**:

* Limit the use of dependencies when possible.

**Separation of Concerns**:

* Move constants, configurations, and lengthy strings to separate files or configuration modules.

**Naming Conventions**:

* Use descriptive and meaningful names for files, functions, and variables to improve code clarity.

**Documentation**:

* Document all file dependencies.
* Maintain a clean and organized project structure with clear directories and file hierarchies.
* Ensure the readme.md, docs/DESIGN.md and docs/USAGES.md stay up to date

--- æ–‡ä»¶è·¯å¾„: docs\CONTRIBUTING.md ---

# Contributing to Video Analyzer

Thank you for your interest in contributing to the Video Analyzer project! This guide outlines the process for making contributions through pull requests.

## Before You Start

1. Read [docs/DESIGN.md](DESIGN.md) thoroughly to understand:
   - The project's architecture
   - Core components and their interactions
   - Design decisions and rationale
   - Implementation details

2. Familiarize yourself with the codebase:
   - Review the project structure
   - Understand the different modules
   - Check existing features and implementations

## Proposing Changes

1. Before creating a PR, start a discussion in the [GitHub Discussions](https://github.com/byjlw/video-analyzer/discussions) section:
   - Outline your proposed changes
   - Explain the motivation behind the changes
   - Describe your planned implementation approach
   - Wait for community feedback and maintainer input

2. Use the appropriate discussion category:
   - "Ideas" for new features
   - "Q&A" for questions about implementation
   - "Show and Tell" for sharing prototypes

## Making Changes

Once your proposal has been discussed and approved:

1. Fork the repository and create a new branch:
   ```bash
   git checkout -b feature/your-feature-name
   ```

2. Follow the project's coding standards:
   - Maintain consistent code style
   - Add appropriate documentation
   - Include type hints
   - Write clear commit messages

3. Add tests for new functionality:
   - Unit tests for individual components
   - Integration tests for feature workflows
   - Ensure all tests pass

4. Update documentation:
   - Add/update docstrings
   - Update relevant .md files
   - Add examples if applicable

## Submitting Pull Requests

1. Before submitting:
   - Ensure all tests pass
   - Update documentation
   - Add your changes to CHANGELOG.md
   - Rebase on latest main branch

2. Create a pull request:
   - Reference the discussion thread
   - Provide a clear description of changes
   - List any breaking changes
   - Include testing steps

3. PR description should include:
   - Link to the discussion thread
   - Summary of changes
   - Testing performed
   - Screenshots/videos if UI changes
   - Breaking changes (if any)

## Review Process

1. Maintainers will review your PR:
   - Code quality
   - Test coverage
   - Documentation
   - Design consistency

2. Address review feedback:
   - Make requested changes
   - Respond to comments
   - Update tests if needed

3. Once approved:
   - Squash commits if requested
   - Ensure branch is up to date
   - Wait for merge by maintainers

## Additional Guidelines

### Code Style
- Follow PEP 8 guidelines
- Use meaningful variable names
- Keep functions focused and concise
- Add type hints to function parameters
- Document complex logic

### Testing
- Write tests for new features
- Update existing tests if needed
- Ensure test coverage
- Test edge cases

### Documentation
- Keep documentation up to date
- Use clear, concise language
- Include code examples
- Update README.md if needed

### Commit Messages
- Use clear, descriptive messages
- Reference issues/discussions
- Follow conventional commits format:
  ```
  feat: add new feature X
  fix: resolve issue with Y
  docs: update contributing guidelines
  test: add tests for feature Z
  ```

## Getting Help

If you need help:
1. Check existing [discussions](https://github.com/byjlw/video-analyzer/discussions)
2. Start a new discussion if needed
3. Tag maintainers for urgent issues

## License

By contributing, you agree that your contributions will be licensed under the project's apache License.


--- æ–‡ä»¶è·¯å¾„: docs\DESIGN.md ---

# Video Analyzer Design
![Design](design.png)
## Core Workflow

1. Frame Extraction
   - Uses OpenCV to extract frames from video
   - Calculates frame differences to identify key moments
   - Saves frames as JPEGs for LLM analysis
   - Adaptive sampling based on video length and target frames per minute

   ### Frame Selection Algorithm
   1. Target Frame Calculation
      - Calculates target frames based on video duration and frames_per_minute
      - Respects optional max_frames limit
      - Ensures at least 1 frame and no more than total video frames

   2. Adaptive Sampling
      - Uses sampling interval = total_frames / (target_frames * 2)
      - Reduces processing load while maintaining coverage
      - Samples more frequently than target to ensure enough candidates

   3. Frame Difference Analysis
      - Converts frames to grayscale for efficient comparison
      - Uses OpenCV's absdiff to calculate absolute difference
      - Compares against FRAME_DIFFERENCE_THRESHOLD (default 10.0)
      - Stores frame number, image data, and difference score

   4. Final Selection Process
      - Selects frames with highest difference scores
      - Takes top N frames based on target frame count
      - If max_frames specified, samples evenly across selected frames
      - Ensures most significant changes are captured

   ### Limitations
   - Frames between sampling intervals may be missed
   - Rapid sequences may only have one frame selected
   - High-scoring frames may be excluded if outranked by others
   - Even sampling with max_frames may skip some significant changes

2. Audio Processing
   - Extracts audio using FFmpeg
   - Uses Whisper for transcription
   - Handles poor quality audio by checking confidence scores
   - Segments audio for better context in final analysis

3. Frame Analysis
   - Each frame is analyzed independently using vision LLM
   - Uses frame_analysis.txt prompt to guide LLM analysis
   - Captures timestamp, visual elements, and actions
   - Maintains chronological order for narrative flow

4. Video Reconstruction
   - Combines frame analyses chronologically
   - Integrates audio transcript if available
   - Uses video_reconstruction.txt prompt to create technical description
   - Uses narrate_storyteller.txt to transform into engaging narrative

## LLM Integration

### Base Client (llm_client.py)
```python
class LLMClient:
    def encode_image(self, image_path: str) -> str:
        # Common base64 encoding for all clients
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    @abstractmethod
    def generate(self,
        prompt: str,
        image_path: Optional[str] = None,
        stream: bool = False,
        model: str = "llama3.2-vision",
        temperature: float = 0.2,
        num_predict: int = 256) -> Dict[Any, Any]:
        pass
```

### Client Implementations

1. Ollama (ollama.py)
   - Uses local Ollama API
   - Sends images as base64 in "images" array
   - Returns raw response from Ollama

2. Generic OpenAI API (generic_openai_api.py)
   - Compatible with OpenAI-style APIs (OpenAI, OpenRouter, etc.)
   - Configurable API URL (e.g. OpenRouter: https://openrouter.ai/api/v1, OpenAI: https://api.openai.com/v1)
   - Sends images as content array with type "image_url"
   - Requires API key and service URL
   - Returns standardized response format

## Configuration System

Uses cascade priority:
1. Command line args
2. User config (config.json)
3. Default config (default_config.json)

Key configuration groups:
```json
{
    "clients": {
        "default": "ollama",
        "ollama": {
            "url": "http://localhost:11434",
            "model": "llama3.2-vision"
        },
        "openai_api": {
            "api_key": "",
            "api_url": "https://openrouter.ai/api/v1",
            "model": "meta-llama/llama-3.2-11b-vision-instruct:free"
        }
    },
    "frames": {
        "per_minute": 60,
        "analysis_threshold": 10.0,
        "min_difference": 5.0,
        "max_count": 30
    }
}
```

## Prompt System

### Prompt Files

Two key prompts:

1. frame_analysis.txt
   - Analyzes single frame
   - Includes timestamp context
   - Focuses on visual elements and actions
   - Supports user questions through {prompt} token

2. describe.txt
   - Combines frame analyses
   - Uses 1 frame
   - Integrates transcript
   - Creates a description of the video based on all the past frames
   - Supports user questions through {prompt} token

Both prompts support user questions via the --prompt flag. When a question is provided, it is prefixed with "I want to know" and injected into the prompts using the {prompt} token. This allows users to ask specific questions about the video that guide both the frame analysis and final description.

### Prompt Loading System

The prompt loading system supports flexible prompt file locations and custom prompts:

1. Path Resolution:
   - User-specified directory (via config `prompt_dir`):
     * Absolute paths: `/path/to/prompts`
     * User home paths: `~/prompts`
     * Relative paths: Checked against:
       1. Current working directory
       2. Package root directory
   - Package resources (fallback)

2. Development Workflow:
   - Install in dev mode: `pip install -e .`
   - Modify prompt files directly
   - Changes reflect immediately without reinstall
   - Works from any directory

3. Configuration:
```json
{
    "prompt_dir": "/absolute/path/to/prompts",  // Absolute path
    // or "~/prompts"                          // User home directory
    // or "prompts"                            // Relative path
    // or ""                                   // Use package prompts only
    "prompts": [
        {
            "name": "Frame Analysis",
            "path": "frame_analysis/frame_analysis.txt"
        },
        {
            "name": "Video Reconstruction",
            "path": "frame_analysis/describe.txt"
        }
    ]
}
```

The system prioritizes user-specified prompts over package prompts, enabling customization while maintaining reliable fallbacks.

## Sample output
[Sample Output](sample_analysis.json)

## Common Issues & Solutions

1. Frame Analysis Failures
   - Ollama: Check if service is running and model is loaded
   - OpenRouter: Verify API key and check response format
   - Both: Ensure image encoding is correct for each API

2. Memory Usage
   - Adjust frames_per_minute based on video length
   - Clean up frames after analysis
   - Use appropriate Whisper model size

3. Poor Analysis Quality
   - Check frame extraction threshold
   - Verify prompt templates
   - Ensure correct model is being used

## Adding New Features

1. New LLM Provider
   - Inherit from LLMClient
   - Implement correct image format for API
   - Add client config to default_config.json
   - Update create_client() in video_analyzer.py

2. Custom Analysis
   - Add new prompt template
   - Update VideoAnalyzer methods
   - Modify output format in results


--- æ–‡ä»¶è·¯å¾„: docs\USAGES.md ---

# Video Analyzer Usage Guide

This guide covers all configuration options and command line arguments for the video analyzer tool, along with practical examples for different use cases.

## Table of Contents
- [Basic Usage](#basic-usage)
- [Command Line Arguments](#command-line-arguments)
- [Configuration System](#configuration-system)
- [Common Use Cases](#common-use-cases)
- [Advanced Examples](#advanced-examples)

## Basic Usage

### Local Analysis with Ollama (Default)
```bash
video-analyzer path/to/video.mp4
```

### Using OpenAI-Compatible API (OpenRouter/OpenAI)
```bash
video-analyzer path/to/video.mp4 --client openai_api --api-key your-key --api-url https://openrouter.ai/api/v1
```

## Command Line Arguments

| Argument | Description | Default | Example |
|----------|-------------|---------|---------|
| `video_path` | Path to the input video file | (Required) | `video.mp4` |
| `--config` | Path to configuration directory | config/ | `--config /path/to/config/` |
| `--output` | Output directory for analysis results | output/ | `--output ./results/` |
| `--client` | Client to use (ollama or openai_api) | ollama | `--client openai_api` |
| `--ollama-url` | URL for the Ollama service | http://localhost:11434 | `--ollama-url http://localhost:11434` |
| `--api-key` | API key for OpenAI-compatible service | None | `--api-key sk-xxx...` |
| `--api-url` | API URL for OpenAI-compatible API | None | `--api-url https://openrouter.ai/api/v1` |
| `--model` | Name of the vision model to use | llama3.2-vision | `--model gpt-4-vision-preview` |
| `--duration` | Duration in seconds to process | None (full video) | `--duration 60` |
| `--keep-frames` | Keep extracted frames after analysis | False | `--keep-frames` |
| `--whisper-model` | Whisper model size or model path | medium | `--whisper-model large` |
| `--start-stage` | Stage to start processing from (1-3) | 1 | `--start-stage 2` |
| `--max-frames` | Maximum number of frames to process. When specified, frames are sampled evenly across the video duration rather than just taking the first N frames. | sys.maxsize | `--max-frames 100` |
| `--log-level` | Set logging level | INFO | `--log-level DEBUG` |
| `--prompt` | Question to ask about the video | "" | `--prompt "What activities are shown?"` |
| `--language` | Set language for transcription | None (auto-detect) | `--language en` |
| `--device` | Select device for Whisper model | cpu | `--device cuda` |
| `--temperature` | Temperature for LLM generation | 0.2 | `--temperature 0.2` |

### Processing Stages
The `--start-stage` argument allows you to begin processing from a specific stage:
1. Frame and Audio Processing
2. Frame Analysis
3. Video Reconstruction

## Configuration System

The tool uses a cascading configuration system with the following priority:
1. Command line arguments (highest priority)
2. User config (config/config.json)
3. Default config (config/default_config.json)

### Configuration File Structure

```json
{
  "clients": {
    "default": "ollama",
    "temperature": 0.2,
    "ollama": {
      "url": "http://localhost:11434",
      "model": "llama3.2-vision"
    },
    "openai_api": {
      "api_key": "",
      "api_url": "https://openrouter.ai/api/v1",
      "model": "meta-llama/llama-3.2-11b-vision-instruct:free"
    }
  },
  "prompt_dir": "",
  "output_dir": "output",
  "frames": {
    "per_minute": 10,
    "analysis_threshold": 10.0,
    "min_difference": 5.0,
    "max_count": 30
  },
  "response_length": {
    "frame": 256,
    "reconstruction": 512,
    "narrative": 1024
  },
  "audio": {
    "sample_rate": 16000,
    "channels": 1,
    "quality_threshold": 0.5,
    "chunk_length": 30,
    "language_confidence_threshold": 0.5,
    "language": null
  },
  "keep_frames": false,
  "prompt": ""
}
```

### Configuration Options Explained

#### Client Settings
- `clients.default`: Default LLM client (ollama/openai_api)
- `clients.temperature`: Temperature for LLM generation (0.0-1.0, higher values = more creative)
- `clients.ollama.url`: Ollama service URL
- `clients.ollama.model`: Vision model for Ollama
- `clients.openai_api.api_key`: API key for OpenAI-compatible services
- `clients.openai_api.api_url`: API endpoint URL
- `clients.openai_api.model`: Vision model for API service

#### Frame Analysis Settings
- `frames.per_minute`: Target frames to extract per minute
- `frames.analysis_threshold`: Threshold for key frame detection
- `frames.min_difference`: Minimum difference between frames
- `frames.max_count`: Maximum frames to extract

#### Response Length Settings
- `response_length.frame`: Max length for frame analysis
- `response_length.reconstruction`: Max length for video reconstruction
- `response_length.narrative`: Max length for enhanced narrative

#### Audio Processing Settings
- `audio.sample_rate`: Audio sample rate in Hz
- `audio.channels`: Number of audio channels
- `audio.quality_threshold`: Minimum quality for transcription
- `audio.chunk_length`: Audio chunk processing length
- `audio.language_confidence_threshold`: Language detection confidence
- `audio.language`: Force specific language (null for auto-detect)

#### General Settings
- `prompt_dir`: Custom prompt directory path
- `output_dir`: Analysis output directory
- `keep_frames`: Retain extracted frames
- `prompt`: Custom analysis prompt

## Common Use Cases

### Quick Local Analysis
```bash
video-analyzer video.mp4
```

### High-Quality Cloud Analysis with Custom Prompt
```bash
video-analyzer video.mp4 \
    --client openai_api \
    --api-key your-key \
    --api-url https://openrouter.ai/api/v1 \
    --model meta-llama/llama-3.2-11b-vision-instruct:free \
    --whisper-model large \
    --prompt "What activities are happening in this video?"
```

### Resume from Frame Analysis Stage
```bash
video-analyzer video.mp4 \
    --start-stage 2 \
    --max-frames 50 \
    --keep-frames
```

### Analyze Video with Evenly Sampled Frames
```bash
video-analyzer video.mp4 \
    --max-frames 5 \
    --keep-frames
```
This will extract frames evenly spaced across the video duration. For example, in a 5-minute video, it would sample approximately one frame per minute rather than taking the first 5 frames.

### Specific Language Processing
```bash
video-analyzer video.mp4 \
    --language es \
    --whisper-model large
```

### GPU-Accelerated Processing
```bash
video-analyzer video.mp4 \
    --device cuda \
    --whisper-model large
```

## Advanced Examples

### Full Configuration with OpenRouter
```bash
video-analyzer video.mp4 \
    --config custom_config.json \
    --output ./analysis_results \
    --client openai_api \
    --api-key your-key \
    --api-url https://openrouter.ai/api/v1 \
    --model meta-llama/llama-3.2-11b-vision-instruct:free \
    --duration 120 \
    --whisper-model large \
    --keep-frames \
    --log-level DEBUG \
    --prompt "Focus on the interactions between people"
```

### Local Processing with Frame Limits
```bash
video-analyzer video.mp4 \
    --client ollama \
    --ollama-url http://localhost:11434 \
    --model llama3.2-vision \
    --max-frames 30 \
    --whisper-model medium \
    --device cuda \
    --language en
```

### Resume Analysis from Specific Stage
```bash
video-analyzer video.mp4 \
    --start-stage 2 \
    --output ./custom_output \
    --keep-frames \
    --max-frames 50 \
    --prompt "Describe the main events"
```

### Using Local Whisper Model
```bash
video-analyzer video.mp4 \
    --whisper-model /path/to/whisper/model \
    --device cuda \
    --start-stage 1


--- æ–‡ä»¶è·¯å¾„: docs\design.excalidraw ---

{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "id": "imaCwtIZblsqQxQkGv0AT",
      "type": "rectangle",
      "x": 660,
      "y": 220,
      "width": 443,
      "height": 55,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffc9c9",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "a0",
      "roundness": {
        "type": 3
      },
      "seed": 900501615,
      "version": 328,
      "versionNonce": 1641790005,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "ofbN4XMrINdKa21FhXdy5",
          "type": "arrow"
        },
        {
          "id": "cHrz7AdSi8dRYyDiKGI1Q",
          "type": "arrow"
        }
      ],
      "updated": 1734561506821,
      "link": null,
      "locked": false
    },
    {
      "id": "wfbTCWjRTzDQu_JxANdWr",
      "type": "text",
      "x": 858,
      "y": 236,
      "width": 106.70833333333333,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "a2",
      "roundness": null,
      "seed": 1223596079,
      "version": 148,
      "versionNonce": 1392706907,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561656793,
      "link": null,
      "locked": false,
      "text": "LLM Server",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "LLM Server",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "seuHK_GQic0IXdRuf1nSr",
      "type": "rectangle",
      "x": 419,
      "y": 471,
      "width": 108,
      "height": 44,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "a3",
      "roundness": {
        "type": 3
      },
      "seed": 1164680001,
      "version": 171,
      "versionNonce": 2037534933,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "CczVwPkrSniLfbG_24sSJ",
          "type": "text"
        },
        {
          "id": "qH0KtOnvaWHebHzwk_xaw",
          "type": "arrow"
        },
        {
          "id": "b8Dzs_eM0OvcVTvcLlQAT",
          "type": "arrow"
        },
        {
          "id": "rfNS3tMT7ofX5Qdk2_kaU",
          "type": "arrow"
        }
      ],
      "updated": 1734561481650,
      "link": null,
      "locked": false
    },
    {
      "id": "CczVwPkrSniLfbG_24sSJ",
      "type": "text",
      "x": 432.40625,
      "y": 483,
      "width": 81.1875,
      "height": 20,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "a4",
      "roundness": null,
      "seed": 783846177,
      "version": 60,
      "versionNonce": 627009051,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561303795,
      "link": null,
      "locked": false,
      "text": "Transcribe",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "seuHK_GQic0IXdRuf1nSr",
      "originalText": "Transcribe",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "MZeTAMveLiS2gtnPH1Mc8",
      "type": "rectangle",
      "x": 555,
      "y": 470,
      "width": 151,
      "height": 44,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "a9",
      "roundness": {
        "type": 3
      },
      "seed": 509219073,
      "version": 227,
      "versionNonce": 444603125,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "ASKGWh94E1s_nqM0Oa8HH",
          "type": "text"
        },
        {
          "id": "b8Dzs_eM0OvcVTvcLlQAT",
          "type": "arrow"
        },
        {
          "id": "OE8KDGyqU2OeAygYPcbfg",
          "type": "arrow"
        },
        {
          "id": "aWQpJCW8SJyepzmVyisiy",
          "type": "arrow"
        }
      ],
      "updated": 1734561491824,
      "link": null,
      "locked": false
    },
    {
      "id": "ASKGWh94E1s_nqM0Oa8HH",
      "type": "text",
      "x": 570.2604179382324,
      "y": 482,
      "width": 120.47916412353516,
      "height": 20,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aA",
      "roundness": null,
      "seed": 1439844577,
      "version": 127,
      "versionNonce": 1678523841,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561089847,
      "link": null,
      "locked": false,
      "text": "Frame Selection",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "MZeTAMveLiS2gtnPH1Mc8",
      "originalText": "Frame Selection",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "FT9Rc5Z0yKRlk16lpLGBb",
      "type": "rectangle",
      "x": 760,
      "y": 462,
      "width": 179.00000000000003,
      "height": 42,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aB",
      "roundness": {
        "type": 3
      },
      "seed": 1357102913,
      "version": 387,
      "versionNonce": 2113215381,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "PBxEckNziUXI-zMA5ZIbs",
          "type": "text"
        },
        {
          "id": "mg0v2y92CjyMqSebd2Hy5",
          "type": "arrow"
        },
        {
          "id": "Gk2lLsZgj_7fnAQdqPgcc",
          "type": "arrow"
        },
        {
          "id": "cHrz7AdSi8dRYyDiKGI1Q",
          "type": "arrow"
        },
        {
          "id": "CRTLSYgcOM5UyFGmdOnMa",
          "type": "arrow"
        },
        {
          "id": "gB76Djr-LLFq4PcO1YX4q",
          "type": "arrow"
        }
      ],
      "updated": 1734561512564,
      "link": null,
      "locked": false
    },
    {
      "id": "PBxEckNziUXI-zMA5ZIbs",
      "type": "text",
      "x": 786.4791666666667,
      "y": 473,
      "width": 126.04166666666666,
      "height": 20,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aC",
      "roundness": null,
      "seed": 1603022625,
      "version": 298,
      "versionNonce": 743891189,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561512564,
      "link": null,
      "locked": false,
      "text": "Describe Frames",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "FT9Rc5Z0yKRlk16lpLGBb",
      "originalText": "Describe Frames",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "oPVzikRJWJPYSywFBwl_f",
      "type": "rectangle",
      "x": 978,
      "y": 461,
      "width": 151,
      "height": 44,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aD",
      "roundness": {
        "type": 3
      },
      "seed": 1072026479,
      "version": 389,
      "versionNonce": 162063669,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "kqc7i8vXwf0DFK_F8vJ9f",
          "type": "text"
        },
        {
          "id": "CRTLSYgcOM5UyFGmdOnMa",
          "type": "arrow"
        },
        {
          "id": "ofbN4XMrINdKa21FhXdy5",
          "type": "arrow"
        },
        {
          "id": "V3q6_b2vfzPf-IVtfjCYw",
          "type": "arrow"
        }
      ],
      "updated": 1734561512564,
      "link": null,
      "locked": false
    },
    {
      "id": "kqc7i8vXwf0DFK_F8vJ9f",
      "type": "text",
      "x": 996.6041666666665,
      "y": 473,
      "width": 113.79166666666666,
      "height": 20,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aE",
      "roundness": null,
      "seed": 2024185601,
      "version": 294,
      "versionNonce": 1310750357,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561512564,
      "link": null,
      "locked": false,
      "text": "Describe Video",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "oPVzikRJWJPYSywFBwl_f",
      "originalText": "Describe Video",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "yw3-BGfeFF7Ov-AIAvO6n",
      "type": "rectangle",
      "x": 402,
      "y": 611,
      "width": 696,
      "height": 56,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffec99",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aL",
      "roundness": {
        "type": 3
      },
      "seed": 1745480449,
      "version": 236,
      "versionNonce": 565186549,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "ff8BdseWVgdo9N2J5bX6d"
        },
        {
          "id": "qH0KtOnvaWHebHzwk_xaw",
          "type": "arrow"
        },
        {
          "id": "mg0v2y92CjyMqSebd2Hy5",
          "type": "arrow"
        },
        {
          "id": "Gk2lLsZgj_7fnAQdqPgcc",
          "type": "arrow"
        },
        {
          "id": "V3q6_b2vfzPf-IVtfjCYw",
          "type": "arrow"
        }
      ],
      "updated": 1734561312026,
      "link": null,
      "locked": false
    },
    {
      "id": "ff8BdseWVgdo9N2J5bX6d",
      "type": "text",
      "x": 701.9270820617676,
      "y": 629,
      "width": 96.14583587646484,
      "height": 20,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aLV",
      "roundness": null,
      "seed": 1352296321,
      "version": 87,
      "versionNonce": 1788648367,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734560799033,
      "link": null,
      "locked": false,
      "text": "analysis.json",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "yw3-BGfeFF7Ov-AIAvO6n",
      "originalText": "analysis.json",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "qH0KtOnvaWHebHzwk_xaw",
      "type": "arrow",
      "x": 471.79401208977714,
      "y": 522,
      "width": 26.20598791022286,
      "height": 86,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aO",
      "roundness": {
        "type": 2
      },
      "seed": 1349810849,
      "version": 141,
      "versionNonce": 1986128571,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "tlXonZYMHlC05Pd8IBgCg"
        }
      ],
      "updated": 1734561303795,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          26.20598791022286,
          86
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "seuHK_GQic0IXdRuf1nSr",
        "focus": 0.16544073548035823,
        "gap": 7,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "yw3-BGfeFF7Ov-AIAvO6n",
        "focus": -0.6815749487288532,
        "gap": 3,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "tlXonZYMHlC05Pd8IBgCg",
      "type": "text",
      "x": 406.4706400488845,
      "y": 545,
      "width": 157.625,
      "height": 40,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aP",
      "roundness": null,
      "seed": 1545324481,
      "version": 32,
      "versionNonce": 819085729,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561110070,
      "link": null,
      "locked": false,
      "text": "transcription + meta\ndata",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "qH0KtOnvaWHebHzwk_xaw",
      "originalText": "transcription + meta data",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "mg0v2y92CjyMqSebd2Hy5",
      "type": "arrow",
      "x": 783.2266103814313,
      "y": 509,
      "width": 16,
      "height": 101,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aT",
      "roundness": {
        "type": 2
      },
      "seed": 2000619777,
      "version": 327,
      "versionNonce": 863927355,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "ZHO8CulgUlznWL5OEyfeS"
        }
      ],
      "updated": 1734561518887,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          -8.226610381431328,
          51
        ],
        [
          7.773389618568672,
          101
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "FT9Rc5Z0yKRlk16lpLGBb",
        "focus": 0.6683297498108252,
        "gap": 5,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "yw3-BGfeFF7Ov-AIAvO6n",
        "focus": 0.1408561183325863,
        "gap": 1,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "ZHO8CulgUlznWL5OEyfeS",
      "type": "text",
      "x": 776.8854166666666,
      "y": 548,
      "width": 102.22916666666666,
      "height": 40,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aU",
      "roundness": null,
      "seed": 886787439,
      "version": 35,
      "versionNonce": 988417249,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561067694,
      "link": null,
      "locked": false,
      "text": "frame details\n+ description",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "mg0v2y92CjyMqSebd2Hy5",
      "originalText": "frame details\n+ description",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "Gk2lLsZgj_7fnAQdqPgcc",
      "type": "arrow",
      "x": 941,
      "y": 605,
      "width": 30.049959707178914,
      "height": 98,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aV",
      "roundness": {
        "type": 2
      },
      "seed": 1551376993,
      "version": 406,
      "versionNonce": 2051864891,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "Ck5JqxMaHPuDWPuJQrIOf"
        }
      ],
      "updated": 1734561560259,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          -16,
          -53
        ],
        [
          -30.049959707178914,
          -98
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "yw3-BGfeFF7Ov-AIAvO6n",
        "focus": 0.5646305314418801,
        "gap": 6,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "FT9Rc5Z0yKRlk16lpLGBb",
        "focus": -0.5617177951676074,
        "gap": 3,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "Ck5JqxMaHPuDWPuJQrIOf",
      "type": "text",
      "x": 874.25,
      "y": 532,
      "width": 117.5,
      "height": 40,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aW",
      "roundness": null,
      "seed": 645071599,
      "version": 29,
      "versionNonce": 1584432795,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561526016,
      "link": null,
      "locked": false,
      "text": "previous frame \ndetails",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "Gk2lLsZgj_7fnAQdqPgcc",
      "originalText": "previous frame \ndetails",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "cHrz7AdSi8dRYyDiKGI1Q",
      "type": "arrow",
      "x": 853.2686232093424,
      "y": 455,
      "width": 10.561373316589652,
      "height": 166,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aX",
      "roundness": {
        "type": 2
      },
      "seed": 1171016385,
      "version": 642,
      "versionNonce": 1137270107,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "yvfQIYSNp37_BGIMnMdON"
        }
      ],
      "updated": 1734561639935,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          -10.561373316589652,
          -166
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "FT9Rc5Z0yKRlk16lpLGBb",
        "focus": 0.061099728593111316,
        "gap": 7,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "imaCwtIZblsqQxQkGv0AT",
        "focus": 0.1855908757664243,
        "gap": 14,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "yvfQIYSNp37_BGIMnMdON",
      "type": "text",
      "x": 792.8629365510476,
      "y": 332,
      "width": 110.25,
      "height": 80,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aY",
      "roundness": null,
      "seed": 1566808719,
      "version": 142,
      "versionNonce": 1702361877,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561638454,
      "link": null,
      "locked": false,
      "text": "current frame \n+ \npast frame\ndescriptions",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "cHrz7AdSi8dRYyDiKGI1Q",
      "originalText": "current frame \n+ \npast frame descriptions",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "CRTLSYgcOM5UyFGmdOnMa",
      "type": "arrow",
      "x": 940,
      "y": 482.74903291482866,
      "width": 36,
      "height": 3.664269991403728,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aZ",
      "roundness": {
        "type": 2
      },
      "seed": 1554580225,
      "version": 363,
      "versionNonce": 2049398267,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561513055,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          36,
          -3.664269991403728
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "FT9Rc5Z0yKRlk16lpLGBb",
        "focus": 0.2975976521849843,
        "gap": 1,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "oPVzikRJWJPYSywFBwl_f",
        "focus": 0.39763113367174246,
        "gap": 2,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "ofbN4XMrINdKa21FhXdy5",
      "type": "arrow",
      "x": 1040.3808711556576,
      "y": 454,
      "width": 56.02415845569453,
      "height": 168,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aa",
      "roundness": {
        "type": 2
      },
      "seed": 1361498159,
      "version": 260,
      "versionNonce": 1894800821,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "oOKP2a-_7pXsF8Swh9auC"
        }
      ],
      "updated": 1734561512575,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          -56.02415845569453,
          -168
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "oPVzikRJWJPYSywFBwl_f",
        "focus": -0.04162768942937324,
        "gap": 7,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "imaCwtIZblsqQxQkGv0AT",
        "focus": -0.33129896907216494,
        "gap": 11,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "oOKP2a-_7pXsF8Swh9auC",
      "type": "text",
      "x": 1005.4791666666666,
      "y": 358,
      "width": 167.04166666666666,
      "height": 40,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ab",
      "roundness": null,
      "seed": 1414233857,
      "version": 45,
      "versionNonce": 590285153,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561039885,
      "link": null,
      "locked": false,
      "text": "Frame 0 and all frame\ndescriptions",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "ofbN4XMrINdKa21FhXdy5",
      "originalText": "Frame 0 and all frame descriptions",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "b8Dzs_eM0OvcVTvcLlQAT",
      "type": "arrow",
      "x": 528,
      "y": 493,
      "width": 26,
      "height": 0,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ac",
      "roundness": {
        "type": 2
      },
      "seed": 36470273,
      "version": 146,
      "versionNonce": 1508563803,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561303796,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          26,
          0
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "seuHK_GQic0IXdRuf1nSr",
        "focus": 0,
        "gap": 1,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "MZeTAMveLiS2gtnPH1Mc8",
        "focus": -0.045454545454545456,
        "gap": 1,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "V3q6_b2vfzPf-IVtfjCYw",
      "type": "arrow",
      "x": 1083.2590934265233,
      "y": 516,
      "width": 26.25909342652335,
      "height": 91,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ad",
      "roundness": {
        "type": 2
      },
      "seed": 45639329,
      "version": 98,
      "versionNonce": 1055186709,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "IHFWDVoRymlDZ3Cbdh7UR"
        }
      ],
      "updated": 1734561512575,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          -26.25909342652335,
          91
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "oPVzikRJWJPYSywFBwl_f",
        "focus": -0.4799317362998546,
        "gap": 11,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "yw3-BGfeFF7Ov-AIAvO6n",
        "focus": 0.7740432716358181,
        "gap": 4,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "IHFWDVoRymlDZ3Cbdh7UR",
      "type": "text",
      "x": 1045.0625,
      "y": 553.5,
      "width": 84.875,
      "height": 20,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ae",
      "roundness": null,
      "seed": 1169161487,
      "version": 16,
      "versionNonce": 1124892431,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1734561136932,
      "link": null,
      "locked": false,
      "text": "description",
      "fontSize": 16,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "V3q6_b2vfzPf-IVtfjCYw",
      "originalText": "description",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "CBHG4qwdAisaJcXoiQ0Qv",
      "type": "rectangle",
      "x": 599.4000244140625,
      "y": 317.79998779296875,
      "width": 122,
      "height": 35,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#a5d8ff",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "af",
      "roundness": {
        "type": 3
      },
      "seed": 1973956277,
      "version": 61,
      "versionNonce": 1961143349,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "ZsaPFOWdvaz493l8Cfgz1",
          "type": "text"
        }
      ],
      "updated": 1734561414295,
      "link": null,
      "locked": false
    },
    {
      "id": "ZsaPFOWdvaz493l8Cfgz1",
      "type": "text",
      "x": 621.6708564758301,
      "y": 322.79998779296875,
      "width": 77.45833587646484,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#a5d8ff",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ag",
      "roundness": null,
      "seed": 1585090581,
      "version": 12,
      "versionNonce": 1763013883,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1734561414295,
      "link": null,
      "locked": false,
      "text": "Frame 0",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "CBHG4qwdAisaJcXoiQ0Qv",
      "originalText": "Frame 0",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "Tg1MHNjdwwzeA-iGYnmBX",
      "type": "rectangle",
      "x": 599.4000244140625,
      "y": 332.79998779296875,
      "width": 122,
      "height": 35,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#a5d8ff",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ah",
      "roundness": {
        "type": 3
      },
      "seed": 1262408021,
      "version": 119,
      "versionNonce": 1833238229,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "B7DabXLtryiatw04Iiacl",
          "type": "text"
        }
      ],
      "updated": 1734561428206,
      "link": null,
      "locked": false
    },
    {
      "id": "B7DabXLtryiatw04Iiacl",
      "type": "text",
      "x": 624.0354423522949,
      "y": 337.79998779296875,
      "width": 72.72916412353516,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#a5d8ff",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ai",
      "roundness": null,
      "seed": 442752693,
      "version": 71,
      "versionNonce": 1087015003,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1734561428206,
      "link": null,
      "locked": false,
      "text": "Frame 1",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "Tg1MHNjdwwzeA-iGYnmBX",
      "originalText": "Frame 1",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "RXS9jkVyKiAfA8eoJi_sR",
      "type": "rectangle",
      "x": 599.4000244140625,
      "y": 347.79998779296875,
      "width": 122,
      "height": 35,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#a5d8ff",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aj",
      "roundness": {
        "type": 3
      },
      "seed": 64295195,
      "version": 141,
      "versionNonce": 408038299,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "fACzMAYfz9Xw5hnuYvm-c",
          "type": "text"
        }
      ],
      "updated": 1734561431956,
      "link": null,
      "locked": false
    },
    {
      "id": "fACzMAYfz9Xw5hnuYvm-c",
      "type": "text",
      "x": 624.0354423522949,
      "y": 352.79998779296875,
      "width": 72.72916412353516,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#a5d8ff",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ak",
      "roundness": null,
      "seed": 1860842939,
      "version": 93,
      "versionNonce": 515358261,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1734561431956,
      "link": null,
      "locked": false,
      "text": "Frame 1",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "RXS9jkVyKiAfA8eoJi_sR",
      "originalText": "Frame 1",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "P86-Gg9gD_ATe72clJ3e6",
      "type": "rectangle",
      "x": 599.4000244140625,
      "y": 361.79998779296875,
      "width": 122,
      "height": 35,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#a5d8ff",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "al",
      "roundness": {
        "type": 3
      },
      "seed": 964147221,
      "version": 155,
      "versionNonce": 1225582965,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "Z-p77x5tsRSqxUkwzm2LA"
        },
        {
          "id": "aWQpJCW8SJyepzmVyisiy",
          "type": "arrow"
        },
        {
          "id": "gB76Djr-LLFq4PcO1YX4q",
          "type": "arrow"
        }
      ],
      "updated": 1734561496133,
      "link": null,
      "locked": false
    },
    {
      "id": "Z-p77x5tsRSqxUkwzm2LA",
      "type": "text",
      "x": 621.8062744140625,
      "y": 366.79998779296875,
      "width": 77.1875,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#a5d8ff",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "am",
      "roundness": null,
      "seed": 499204789,
      "version": 107,
      "versionNonce": 358603291,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1734561440009,
      "link": null,
      "locked": false,
      "text": "Frame N",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "P86-Gg9gD_ATe72clJ3e6",
      "originalText": "Frame N",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "7-XIDR9qX2go2HV9r2vTH",
      "type": "rectangle",
      "x": 387.4000244140625,
      "y": 315.79998779296875,
      "width": 157,
      "height": 77,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffec99",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "an",
      "roundness": {
        "type": 3
      },
      "seed": 1935560539,
      "version": 61,
      "versionNonce": 926552597,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "Mse6NL4eJH181iBnw8gfw"
        },
        {
          "id": "OE8KDGyqU2OeAygYPcbfg",
          "type": "arrow"
        },
        {
          "id": "rfNS3tMT7ofX5Qdk2_kaU",
          "type": "arrow"
        }
      ],
      "updated": 1734561481650,
      "link": null,
      "locked": false
    },
    {
      "id": "Mse6NL4eJH181iBnw8gfw",
      "type": "text",
      "x": 440.1187744140625,
      "y": 341.79998779296875,
      "width": 51.5625,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffec99",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ao",
      "roundness": null,
      "seed": 1725637467,
      "version": 7,
      "versionNonce": 1486915515,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1734561463270,
      "link": null,
      "locked": false,
      "text": "Video",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "7-XIDR9qX2go2HV9r2vTH",
      "originalText": "Video",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "OE8KDGyqU2OeAygYPcbfg",
      "type": "arrow",
      "x": 535.4000244140625,
      "y": 391.79998779296875,
      "width": 56,
      "height": 75,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffec99",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ap",
      "roundness": {
        "type": 2
      },
      "seed": 1531450619,
      "version": 39,
      "versionNonce": 718504475,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1734561478285,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          56,
          75
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "7-XIDR9qX2go2HV9r2vTH",
        "focus": -0.38695841362590905,
        "gap": 1,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "MZeTAMveLiS2gtnPH1Mc8",
        "focus": -0.2206537798754895,
        "gap": 3.20001220703125,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "rfNS3tMT7ofX5Qdk2_kaU",
      "type": "arrow",
      "x": 445.4000244140625,
      "y": 397.79998779296875,
      "width": 16,
      "height": 66,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffec99",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aq",
      "roundness": {
        "type": 2
      },
      "seed": 270139349,
      "version": 43,
      "versionNonce": 247007093,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1734561481650,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          16,
          66
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "7-XIDR9qX2go2HV9r2vTH",
        "focus": 0.3534586855269968,
        "gap": 5,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "seuHK_GQic0IXdRuf1nSr",
        "focus": -0.07619974294161772,
        "gap": 7.20001220703125,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "aWQpJCW8SJyepzmVyisiy",
      "type": "arrow",
      "x": 646.4000244140625,
      "y": 460.79998779296875,
      "width": 16,
      "height": 57,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffec99",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ar",
      "roundness": {
        "type": 2
      },
      "seed": 761363355,
      "version": 24,
      "versionNonce": 243334517,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1734561489521,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          16,
          -57
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "MZeTAMveLiS2gtnPH1Mc8",
        "focus": 0.08744521453959027,
        "gap": 9.20001220703125,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "P86-Gg9gD_ATe72clJ3e6",
        "focus": -0.1346819270694703,
        "gap": 7,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "gB76Djr-LLFq4PcO1YX4q",
      "type": "arrow",
      "x": 725.4000244140625,
      "y": 379.79998779296875,
      "width": 37.86115517093049,
      "height": 81.20001220703125,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffec99",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "as",
      "roundness": {
        "type": 2
      },
      "seed": 639123483,
      "version": 105,
      "versionNonce": 142333685,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1734561512575,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          37.86115517093049,
          81.20001220703125
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "P86-Gg9gD_ATe72clJ3e6",
        "focus": -0.8596585562575614,
        "gap": 4,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "FT9Rc5Z0yKRlk16lpLGBb",
        "focus": -0.7652290314923686,
        "gap": 1,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}

--- æ–‡ä»¶è·¯å¾„: docs\sample_analysis.json ---

{
  "metadata": {
    "client": "ollama",
    "model": "llama3.2-vision",
    "whisper_model": "medium",
    "frames_per_minute": 60,
    "duration_processed": null,
    "frames_extracted": 5,
    "frames_processed": 5,
    "start_stage": 1,
    "audio_language": "en",
    "transcription_successful": true
  },
  "transcript": {
    "text": " I'm scared!",
    "segments": [
      {
        "text": " I'm scared!",
        "start": 1.78,
        "end": 2.24,
        "words": [
          {
            "word": " I'm",
            "start": 1.78,
            "end": 2.04,
            "probability": 0.4382356107234955
          },
          {
            "word": " scared!",
            "start": 2.04,
            "end": 2.24,
            "probability": 0.9464112520217896
          }
        ]
      }
    ]
  },
  "frame_analyses": [
    {
      "model": "llama3.2-vision",
      "created_at": "2024-12-18T23:14:35.871404545Z",
      "response": "Frame 0\n\nSetting/Scene: A person with long blonde hair, wearing a pink t-shirt and yellow shorts, stands in front of a black plastic tub or container on wheels. The ground appears to be covered in wood chips.\n\nAction/Movement: The person is facing away from the camera, looking down at something inside the tub. Their left hand is resting on their hip, while their right arm hangs loosely by their side.\n\nNew Information: There are no new objects or people visible in this frame. However, there appears to be some greenery and possibly fruit scattered around the ground behind the person.\n\nContinuity Points:\n\n* The person's pink t-shirt matches the color of the shirt worn by the person in the background of Frame 1.\n* The black plastic tub on wheels is also present in Frame 1.\n* The wood chips covering the ground are consistent with those seen in Frame 1.\n\nKey Continuation Point: Watch for the person to pick up an object from the tub and examine it more closely.",
      "done": true,
      "done_reason": "stop",
      "total_duration": 7952576674,
      "load_duration": 2623794964,
      "prompt_eval_count": 349,
      "prompt_eval_duration": 1787000000,
      "eval_count": 207,
      "eval_duration": 3317000000
    },
    {
      "model": "llama3.2-vision",
      "created_at": "2024-12-18T23:14:41.664468523Z",
      "response": "Frame 1\n\nSetting/Scene: A person with long blonde hair, wearing a pink t-shirt and yellow shorts, stands in front of a black plastic tub or container on wheels. The ground appears to be covered in wood chips.\n\nAction/Movement: The person is facing away from the camera, looking down at something inside the tub. Their left hand is resting on their hip, while their right arm hangs loosely by their side.\n\nNew Information: There are no new objects or people visible in this frame. However, there appears to be some greenery and possibly fruit scattered around the ground behind the person.\n\nContinuity Points:\n\n* The person's pink t-shirt matches the color of the shirt worn by the person in Frame 0.\n* The black plastic tub on wheels is also present in Frame 0.\n* The wood chips covering the ground are consistent with those seen in Frame 0.\n\nKey Continuation Point: Watch for the person to pick up an object from the tub and examine it more closely.",
      "done": true,
      "done_reason": "stop",
      "total_duration": 5451905730,
      "load_duration": 15567833,
      "prompt_eval_count": 559,
      "prompt_eval_duration": 1810000000,
      "eval_count": 204,
      "eval_duration": 3408000000
    },
    {
      "model": "llama3.2-vision",
      "created_at": "2024-12-18T23:14:47.516490633Z",
      "response": "**Frame 2**\n\n*   **Setting/Scene**: The person with long blonde hair, wearing a pink t-shirt and yellow shorts, stands in front of a black plastic tub or container on wheels. The ground appears to be covered in wood chips.\n*   **Action/Movement**: The person is facing away from the camera, looking down at something inside the tub. Their left hand is resting on their hip, while their right arm hangs loosely by their side.\n*   **New Information**: There are no new objects or people visible in this frame. However, there appears to be some greenery and possibly fruit scattered around the ground behind the person.\n*   **Continuity Points**:\n    *   The person's pink t-shirt matches the color of the shirt worn by the person in Frame 0.\n    *   The black plastic tub on wheels is also present in Frame 0.\n    *   The wood chips covering the ground are consistent with those seen in Frame 0.\n*   **Key Continuation Point**: Watch for the person to pick up an object from the tub and examine it more closely.",
      "done": true,
      "done_reason": "stop",
      "total_duration": 5848181203,
      "load_duration": 14482916,
      "prompt_eval_count": 766,
      "prompt_eval_duration": 1798000000,
      "eval_count": 227,
      "eval_duration": 3823000000
    },
    {
      "model": "llama3.2-vision",
      "created_at": "2024-12-18T23:14:54.869592761Z",
      "response": "**Frame Description**\n\n* **Setting/Scene**: The person with long blonde hair, wearing a pink t-shirt and yellow shorts, stands in front of a black plastic tub or container on wheels. The ground appears to be covered in wood chips.\n* **Action/Movement**: The person is facing away from the camera, looking down at something inside the tub. Their left hand is resting on their hip, while their right arm hangs loosely by their side.\n* **New Information**: There are no new objects or people visible in this frame. However, there appears to be some greenery and possibly fruit scattered around the ground behind the person.\n* **Continuity Points**:\n\t+ The person's pink t-shirt matches the color of the shirt worn by the person in Frame 0.\n\t+ The black plastic tub on wheels is also present in Frame 0.\n\t+ The wood chips covering the ground are consistent with those seen in Frame 0.\n* **Key Continuation Point**: Watch for the person to pick up an object from the tub and examine it more closely.\n\n**Writing Guidelines**\n\n* Use present tense\n* Be specific and concise\n* Avoid interpretation - stick to what you can see\n* Use clear transitional phrases to connect to previous descriptions\n* Include timestamp if available\n\n**Format Your Notes As**\n\n```\nFrame 3 (5.10 seconds)\n[Your observations following the structure above]\nKey continuation points:\n- [List 2-3 elements that the next viewer",
      "done": true,
      "done_reason": "length",
      "total_duration": 7348923526,
      "load_duration": 14771659,
      "prompt_eval_count": 996,
      "prompt_eval_duration": 1915000000,
      "eval_count": 301,
      "eval_duration": 5193000000
    },
    {
      "model": "llama3.2-vision",
      "created_at": "2024-12-18T23:15:02.285939575Z",
      "response": "**Frame Description**\n\n* **Setting/Scene**: The person with long blonde hair, wearing a pink t-shirt and yellow shorts, stands in front of a black plastic tub or container on wheels. The ground appears to be covered in wood chips.\n* **Action/Movement**: The person is facing away from the camera, looking down at something inside the tub. Their left hand is resting on their hip, while their right arm hangs loosely by their side.\n* **New Information**: There are no new objects or people visible in this frame. However, there appears to be some greenery and possibly fruit scattered around the ground behind the person.\n* **Continuity Points**:\n\t+ The person's pink t-shirt matches the color of the shirt worn by the person in Frame 0.\n\t+ The black plastic tub on wheels is also present in Frame 0.\n\t+ The wood chips covering the ground are consistent with those seen in Frame 0.\n* **Key Continuation Point**: Watch for the person to pick up an object from the tub and examine it more closely.\n\n**Writing Guidelines**\n\n* Use present tense\n* Be specific and concise\n* Avoid interpretation - stick to what you can see\n* Use clear transitional phrases to connect to previous descriptions\n* Include timestamp if available\n\n**Format Your Notes As**\n\n```\nFrame 4 (5.67 seconds)\n[Your observations following the structure above]\nKey continuation points:\n- [List 2-3 elements that the next viewer",
      "done": true,
      "done_reason": "length",
      "total_duration": 7412131422,
      "load_duration": 14780998,
      "prompt_eval_count": 1302,
      "prompt_eval_duration": 2047000000,
      "eval_count": 301,
      "eval_duration": 5137000000
    }
  ],
  "video_description": {
    "model": "llama3.2-vision",
    "created_at": "2024-12-18T23:15:06.166299111Z",
    "response": "**Video Summary**\n\nDuration: 5 minutes and 67 seconds\n\nThe video begins with a person with long blonde hair, wearing a pink t-shirt and yellow shorts, standing in front of a black plastic tub or container on wheels. The ground appears to be covered in wood chips.\n\nAs the video progresses, the person remains facing away from the camera, looking down at something inside the tub. Their left hand is resting on their hip, while their right arm hangs loosely by their side. There are no new objects or people visible in this frame, but there appears to be some greenery and possibly fruit scattered around the ground behind the person.\n\nThe black plastic tub on wheels is present throughout the video, and the wood chips covering the ground remain consistent with those seen in Frame 0. The person's pink t-shirt matches the color of the shirt worn by the person in Frame 0.\n\nAs the video continues, the person remains stationary, looking down at something inside the tub. There are no significant changes or developments in this frame.\n\nThe key continuation point is to watch for the person to pick up an object from the tub and examine it more closely.\n\n**Key Continuation Points:**\n\n*   The person's pink t-shirt matches the color of the shirt worn by the person in Frame 0.\n*   The black plastic tub on wheels is also present in Frame 0.\n*   The wood chips covering the ground are consistent with those seen in Frame 0.",
    "done": true,
    "done_reason": "stop",
    "total_duration": 3877694027,
    "load_duration": 10604604,
    "prompt_eval_count": 1705,
    "prompt_eval_duration": 558000000,
    "eval_count": 297,
    "eval_duration": 3308000000
  }
}

--- æ–‡ä»¶è·¯å¾„: prompts\frame_analysis\describe.txt ---

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯è§†é¢‘çš„éŸ³é¢‘è½¬å½•å’Œä¸€ç³»åˆ—æŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„å…³é”®å¸§åˆ†æã€‚

ç”¨æˆ·çš„æ ¸å¿ƒåˆ†æè¦æ±‚æ˜¯ï¼š{user_prompt}

---éŸ³é¢‘è½¬å½•---
{audio_transcript}

---å…³é”®å¸§åˆ†æ---
{frame_descriptions}

è¯·ç»¼åˆä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½å…¨é¢ã€æµç•…ã€ç»“æ„åŒ–çš„è§†é¢‘æœ€ç»ˆåˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šéœ€è¦ç›´æ¥å›åº”ç”¨æˆ·çš„æ ¸å¿ƒè¦æ±‚ã€‚é‡è¦æç¤ºï¼šå¦‚æœå¤šä¸ªå…³é”®å¸§çš„å†…å®¹éå¸¸ç›¸ä¼¼ï¼Œè¯·åœ¨æœ€ç»ˆæŠ¥å‘Šä¸­è¿›è¡Œæ¦‚æ‹¬æ€§æè¿°ï¼Œé¿å…é€å­—é€å¥åœ°é‡å¤ç›¸åŒçš„å†…å®¹ï¼Œä»¥æé«˜æŠ¥å‘Šçš„å¯è¯»æ€§ã€‚

--- æ–‡ä»¶è·¯å¾„: prompts\frame_analysis\frame_analysis.txt ---

è¿™æ˜¯è§†é¢‘ä¸­çš„ä¸€å¸§ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç”¨æˆ·è¦æ±‚è¿›è¡Œåˆ†æï¼š

ç”¨æˆ·è¦æ±‚ï¼š{user_prompt}

è¯·å¯¹è¿™ä¸€å¸§å›¾åƒè¿›è¡Œè¯¦ç»†æè¿°å’Œåˆ†æã€‚

--- æ–‡ä»¶è·¯å¾„: prompts\frame_analysis\video_summary.txt ---

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚æ¥ä¸‹æ¥æˆ‘ä¼šç»™ä½ ä¸€ä¸ªè§†é¢‘çš„å¤šä¸ªå…³é”®å¸§å›¾åƒï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰ã€ä»¥åŠå¯é€‰çš„éŸ³é¢‘è½¬å½•å†…å®¹ã€‚

ç”¨æˆ·çš„æ ¸å¿ƒåˆ†æè¦æ±‚æ˜¯ï¼š{user_prompt}

---éŸ³é¢‘è½¬å½•---
{audio_transcript}

---å…³é”®å¸§æ—¶é—´ç‚¹åˆ—è¡¨---
{frame_info}

è¯·ç»¼åˆä½ çœ‹åˆ°çš„æ‰€æœ‰å›¾åƒå’Œå¬åˆ°çš„æ‰€æœ‰æ–‡æœ¬ï¼Œç”Ÿæˆä¸€ä»½å…¨é¢ã€æµç•…ã€ç»“æ„åŒ–çš„è§†é¢‘æœ€ç»ˆåˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šéœ€è¦ç›´æ¥å›åº”ç”¨æˆ·çš„æ ¸å¿ƒè¦æ±‚ã€‚
é‡è¦æŒ‡ä»¤ï¼šè¯·å°†æ‰€æœ‰å›¾åƒè§†ä¸ºä¸€ä¸ªæ•´ä½“æ•…äº‹çº¿ï¼Œè¿›è¡Œè¿è´¯çš„å™è¿°å’Œåˆ†æï¼Œè€Œä¸æ˜¯å­¤ç«‹åœ°æè¿°æ¯ä¸€å¼ å›¾ã€‚å¦‚æœå¤šä¸ªå…³é”®å¸§å†…å®¹ç›¸ä¼¼ï¼Œè¯·è¿›è¡Œæ¦‚æ‹¬æ€§æè¿°ï¼Œé¿å…é‡å¤ã€‚
**è¾“å‡ºè¦æ±‚**ï¼šè¯·ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼çš„æŠ¥å‘Šå…¨æ–‡ã€‚**ä¸¥ç¦**åœ¨æŠ¥å‘Šä¸­åŠ å…¥ä»»ä½•å¯¹è¯æ€§æ–‡å­—ã€æé—®ï¼ˆä¾‹å¦‚ä¸è¦è¯´â€˜ä½ å¯¹è¿™ä¸ªåˆ†ææ»¡æ„å—ï¼Ÿâ€™æˆ–â€˜ä½ çš„åé¦ˆå°†å¸®åŠ©æˆ‘...â€™ï¼‰ã€æˆ–å›¾åƒå ä½ç¬¦ï¼ˆå¦‚`[img-n]`ï¼‰ã€‚ä½ çš„å›ç­”åº”è¯¥**ä»…é™äº**æŠ¥å‘Šæœ¬èº«ï¼Œå†…å®¹ç¿”å®ï¼Œç»“æ„æ¸…æ™°ã€‚

--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\.gitignore ---

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
.env/
.venv/
ENV/

# IDE
.idea/
.vscode/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Project specific
output/
tmp/


--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\MANIFEST.in ---

include README.md
include requirements.txt
recursive-include video_analyzer_ui/templates *
recursive-include video_analyzer_ui/static *


--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\README.md ---

# Video Analyzer UI

A lightweight web interface for the video-analyzer tool.

## Features
- Simple, intuitive interface for video analysis
- Real-time command output streaming
- Drag-and-drop video upload
- Results visualization and download
- Session management and cleanup

## Prerequisites
- Python 3.8 or higher
- video-analyzer package installed
- FFmpeg (required by video-analyzer)

## Installation

```bash
pip install video-analyzer-ui
```

## Quick Start

1. Start the server:
   ```bash
   video-analyzer-ui
   ```

2. Open in browser:
   http://localhost:5000

## Usage

### Development Mode
```bash
video-analyzer-ui --dev
```
- Auto-reload on code changes
- Debug logging
- Development error pages

### Production Mode
```bash
video-analyzer-ui --host 0.0.0.0 --port 5000
```
- Optimized for performance
- Error logging to file
- Production-ready security

### Command Line Options
- `--dev`: Enable development mode
- `--host`: Bind address (default: localhost)
- `--port`: Port number (default: 5000)
- `--log-file`: Log file path
- `--config`: Custom config file path

## Development Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/username/video-analyzer-ui.git
   cd video-analyzer-ui
   ```

2. Create virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # or venv\Scripts\activate on Windows
   ```

3. Install dependencies:
   ```bash
   pip install -e .
   ```

4. Run development server:
   ```bash
   video-analyzer-ui --dev
   ```

## How It Works

1. Upload Video:
   - Drag and drop or select a video file
   - File is temporarily stored in a session-specific directory

2. Configure Analysis:
   - Required fields are marked with *
   - Optional parameters can be left empty
   - Real-time command preview shows what will be executed

3. Run Analysis:
   - Progress is shown in real-time
   - Output is streamed as it becomes available
   - Results are stored in session directory

4. View Results:
   - Download analysis.json
   - View extracted frames (if kept)
   - Access transcripts and other outputs

## License

Apache License


--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\pyproject.toml ---

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "video-analyzer-ui"
version = "0.1.0"
authors = [
    { name="Jesse White", email="your.email@example.com" },
]
description = "Web interface for the video-analyzer tool"
readme = "README.md"
requires-python = ">=3.8"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: OS Independent",
    "Environment :: Web Environment",
    "Framework :: Flask",
]
dependencies = [
    "flask>=3.0.0",
    "video-analyzer>=0.1.0",
    "werkzeug>=3.0.0",
]

[project.urls]
"Homepage" = "https://github.com/username/video-analyzer-ui"
"Bug Tracker" = "https://github.com/username/video-analyzer-ui/issues"

[project.scripts]
video-analyzer-ui = "video_analyzer_ui.server:main"


--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\requirements.txt ---

flask>=3.0.0
video-analyzer>=0.1.0
werkzeug>=3.0.0


--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\video_analyzer_ui\__init__.py ---

"""
Video Analyzer UI - A lightweight web interface for the video-analyzer tool.
"""

__version__ = "0.1.0"


--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\video_analyzer_ui\server.py ---

#!/usr/bin/env python3
import argparse
import logging
import os
import subprocess
import sys
import tempfile
import uuid
from pathlib import Path
from flask import Flask, render_template, request, jsonify, send_file, Response
from werkzeug.utils import secure_filename

# Initialize logger
logger = logging.getLogger(__name__)

class VideoAnalyzerUI:
    def __init__(self, host='localhost', port=5000, dev_mode=False):
        self.app = Flask(__name__)
        self.host = host
        self.port = port
        self.dev_mode = dev_mode
        self.sessions = {}
        
        # Ensure tmp directories exist
        self.tmp_root = Path(tempfile.gettempdir()) / 'video-analyzer-ui'
        self.uploads_dir = self.tmp_root / 'uploads'
        self.results_dir = self.tmp_root / 'results'
        self.uploads_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.setup_routes()
        
    def setup_routes(self):
        @self.app.route('/')
        def index():
            return render_template('index.html')
            
        @self.app.route('/upload', methods=['POST'])
        def upload_file():
            if 'video' not in request.files:
                return jsonify({'error': 'No video file provided'}), 400
                
            file = request.files['video']
            if file.filename == '':
                return jsonify({'error': 'No selected file'}), 400
                
            if not file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
                return jsonify({'error': 'Invalid file type'}), 400
                
            try:
                # Create session
                session_id = str(uuid.uuid4())
                session_upload_dir = self.uploads_dir / session_id
                session_results_dir = self.results_dir / session_id
                session_upload_dir.mkdir(parents=True)
                session_results_dir.mkdir(parents=True)
                
                # Save file
                filename = secure_filename(file.filename)
                filepath = session_upload_dir / filename
                file.save(filepath)
                
                self.sessions[session_id] = {
                    'video_path': str(filepath),
                    'results_dir': str(session_results_dir),
                    'filename': filename
                }
                
                return jsonify({
                    'session_id': session_id,
                    'message': 'File uploaded successfully'
                })
                
            except Exception as e:
                logger.error(f"Upload error: {e}")
                return jsonify({'error': str(e)}), 500
                
        @self.app.route('/analyze/<session_id>', methods=['POST'])
        def analyze(session_id):
            if session_id not in self.sessions:
                return jsonify({'error': 'Invalid session'}), 404
                
            session = self.sessions[session_id]
            
            # Build command
            cmd = ['video-analyzer', session['video_path']]
            
            # Add optional parameters
            for param, value in request.form.items():
                if value:  # Only add parameters with values
                    if param in ['keep-frames', 'dev']:  # Flags without values
                        cmd.append(f'--{param}')
                    else:
                        cmd.extend([f'--{param}', value])
                        
            # Create output directory if it doesn't exist
            results_dir = Path(session['results_dir'])
            results_dir.mkdir(parents=True, exist_ok=True)
            
            # Add output directory
            cmd.extend(['--output', str(results_dir)])
            
            # Store output directory in session for later use
            session['output_dir'] = str(results_dir)
            logger.debug(f"Set output directory to: {results_dir}")
            
            # Store command in session for streaming
            session['cmd'] = cmd
            
            return jsonify({'message': 'Analysis started'})
            
        @self.app.route('/analyze/<session_id>/stream')
        def stream_output(session_id):
            if session_id not in self.sessions:
                return jsonify({'error': 'Invalid session'}), 404
                
            session = self.sessions[session_id]
            if 'cmd' not in session:
                return jsonify({'error': 'Analysis not started'}), 400
                
            def generate_output():
                logger.debug(f"Starting analysis with command: {' '.join(session['cmd'])}")
                try:
                    process = subprocess.Popen(
                        session['cmd'],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.STDOUT,
                        universal_newlines=True,
                        bufsize=1
                    )
                    
                    for line in process.stdout:
                        line = line.strip()
                        if line:  # Only send non-empty lines
                            logger.debug(f"Output: {line}")
                            yield f"data: {line}\n\n"
                    
                    process.wait()
                    if process.returncode == 0:
                        logger.info("Analysis completed successfully")
                        yield f"data: Analysis completed successfully\n\n"
                    else:
                        logger.error(f"Analysis failed with code {process.returncode}")
                        yield f"data: Analysis failed with code {process.returncode}\n\n"
                except Exception as e:
                    logger.error(f"Error during analysis: {e}")
                    yield f"data: Error during analysis: {str(e)}\n\n"
                    yield f"data: Analysis failed\n\n"
                    
            return Response(
                generate_output(),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'Connection': 'keep-alive'
                }
            )
            
        @self.app.route('/results/<session_id>')
        def get_results(session_id):
            if session_id not in self.sessions:
                return jsonify({'error': 'Invalid session'}), 404
                
            session = self.sessions[session_id]
            results_dir = Path(session['results_dir'])
            logger.debug(f"Looking for results in: {results_dir}")
            
            if not results_dir.exists():
                logger.error(f"Results directory not found: {results_dir}")
                return jsonify({'error': 'Results directory not found'}), 404
            
            # List all files in results directory for debugging
            logger.debug("Files in results directory:")
            for file in results_dir.glob('**/*'):
                logger.debug(f"- {file}")
            
            # Check both the results directory and the default 'output' directory
            analysis_file = results_dir / 'analysis.json'
            default_output = Path('output/analysis.json')
            
            if default_output.exists():
                logger.debug(f"Found analysis file in default output directory: {default_output}")
                try:
                    # Move the file to our results directory
                    default_output.rename(analysis_file)
                    logger.debug(f"Moved analysis file to: {analysis_file}")
                except Exception as e:
                    logger.error(f"Error moving analysis file: {e}")
                    # If move fails, try to copy the content
                    try:
                        analysis_file.write_text(default_output.read_text())
                        logger.debug("Copied analysis file content")
                        default_output.unlink()
                    except Exception as copy_error:
                        logger.error(f"Error copying analysis file: {copy_error}")
                        return jsonify({'error': 'Error accessing analysis file'}), 500
            if not analysis_file.exists():
                logger.error(f"Analysis file not found: {analysis_file}")
                return jsonify({'error': 'Analysis file not found'}), 404
                
            try:
                return send_file(
                    analysis_file,
                    mimetype='application/json',
                    as_attachment=True,
                    download_name=f"analysis_{session['filename']}.json"
                )
            except Exception as e:
                logger.error(f"Error sending file: {e}")
                return jsonify({'error': f'Error sending file: {str(e)}'}), 500
            
        @self.app.route('/cleanup/<session_id>', methods=['POST'])
        def cleanup_session(session_id):
            if session_id not in self.sessions:
                return jsonify({'error': 'Invalid session'}), 404
                
            try:
                session = self.sessions[session_id]
                # Clean up upload directory
                upload_dir = Path(session['video_path']).parent
                if upload_dir.exists():
                    for file in upload_dir.glob('*'):
                        file.unlink()
                    upload_dir.rmdir()
                
                # Clean up results directory
                results_dir = Path(session['results_dir'])
                if results_dir.exists():
                    for file in results_dir.glob('**/*'):
                        if file.is_file():
                            file.unlink()
                    for dir_path in sorted(results_dir.glob('**/*'), reverse=True):
                        if dir_path.is_dir():
                            dir_path.rmdir()
                    results_dir.rmdir()
                
                # Clean up default output directory if it exists
                default_output_dir = Path('output')
                if default_output_dir.exists():
                    for file in default_output_dir.glob('**/*'):
                        if file.is_file():
                            file.unlink()
                    for dir_path in sorted(default_output_dir.glob('**/*'), reverse=True):
                        if dir_path.is_dir():
                            dir_path.rmdir()
                    default_output_dir.rmdir()
                
                del self.sessions[session_id]
                return jsonify({'message': 'Session cleaned up successfully'})
                
            except Exception as e:
                logger.error(f"Cleanup error: {e}")
                return jsonify({'error': str(e)}), 500
    
    def run(self):
        self.app.run(
            host=self.host,
            port=self.port,
            debug=self.dev_mode
        )

def main():
    parser = argparse.ArgumentParser(description="Video Analyzer UI Server")
    parser.add_argument('--host', default='localhost', help='Host to bind to')
    parser.add_argument('--port', type=int, default=5000, help='Port to listen on')
    parser.add_argument('--dev', action='store_true', help='Enable development mode')
    parser.add_argument('--log-file', help='Log file path')
    
    args = parser.parse_args()
    
    # Configure logging
    log_config = {
        'level': logging.DEBUG if args.dev else logging.INFO,
        'format': '%(asctime)s - %(levelname)s - %(message)s',
    }
    if args.log_file:
        log_config['filename'] = args.log_file
    logging.basicConfig(**log_config)
    
    try:
        # Check if video-analyzer is installed
        subprocess.run(['video-analyzer', '--help'], capture_output=True, check=True)
    except subprocess.CalledProcessError:
        logger.error("video-analyzer command not found. Please install video-analyzer package.")
        sys.exit(1)
    except FileNotFoundError:
        logger.error("video-analyzer command not found. Please install video-analyzer package.")
        sys.exit(1)
    
    # Start server
    server = VideoAnalyzerUI(
        host=args.host,
        port=args.port,
        dev_mode=args.dev
    )
    server.run()

if __name__ == '__main__':
    main()


--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\video_analyzer_ui\static\css\styles.css ---

:root {
    --primary-color: #007bff;
    --primary-hover: #0056b3;
    --secondary-color: #6c757d;
    --success-color: #28a745;
    --danger-color: #dc3545;
    --background-color: #f8f9fa;
    --border-color: #dee2e6;
    --text-color: #212529;
    --text-muted: #6c757d;
    --shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    line-height: 1.6;
    color: var(--text-color);
    background-color: var(--background-color);
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 2rem;
}

header {
    text-align: center;
    margin-bottom: 2rem;
}

h1 {
    font-size: 2.5rem;
    color: var(--primary-color);
    margin-bottom: 1rem;
}

section {
    background: white;
    border-radius: 8px;
    padding: 2rem;
    margin-bottom: 2rem;
    box-shadow: var(--shadow);
}

/* Upload Area */
.upload-area {
    border: 2px dashed var(--border-color);
    border-radius: 8px;
    padding: 3rem;
    text-align: center;
    cursor: pointer;
    transition: border-color 0.3s ease;
}

.upload-area:hover {
    border-color: var(--primary-color);
}

.upload-area.drag-over {
    border-color: var(--primary-color);
    background-color: rgba(0, 123, 255, 0.05);
}

.upload-icon {
    width: 48px;
    height: 48px;
    fill: var(--primary-color);
    margin-bottom: 1rem;
}

.upload-formats {
    color: var(--text-muted);
    font-size: 0.875rem;
    margin-top: 0.5rem;
}

/* Form Styling */
.form-group {
    margin-bottom: 1.5rem;
}

label {
    display: block;
    margin-bottom: 0.5rem;
    font-weight: 500;
}

label::after {
    content: " *";
    color: var(--danger-color);
    display: none;
}

label.required::after {
    display: inline;
}

input[type="text"],
input[type="password"],
input[type="number"],
select,
textarea {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid var(--border-color);
    border-radius: 4px;
    font-size: 1rem;
    transition: border-color 0.3s ease;
}

input:focus,
select:focus,
textarea:focus {
    outline: none;
    border-color: var(--primary-color);
    box-shadow: 0 0 0 3px rgba(0, 123, 255, 0.1);
}

textarea {
    min-height: 100px;
    resize: vertical;
}

.checkbox {
    display: flex;
    align-items: center;
    gap: 0.5rem;
}

.checkbox input {
    width: auto;
}

.checkbox label {
    margin: 0;
}

/* Command Preview */
.command-preview {
    background: #f8f9fa;
    border-radius: 4px;
    padding: 1rem;
    margin: 1.5rem 0;
}

.command-preview pre {
    white-space: pre-wrap;
    word-break: break-all;
    font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
    font-size: 0.875rem;
}

/* Buttons */
button {
    padding: 0.75rem 1.5rem;
    border: none;
    border-radius: 4px;
    font-size: 1rem;
    font-weight: 500;
    cursor: pointer;
    transition: background-color 0.3s ease;
}

.primary-button {
    background-color: var(--primary-color);
    color: white;
}

.primary-button:hover {
    background-color: var(--primary-hover);
}

.secondary-button {
    background-color: var(--secondary-color);
    color: white;
}

.secondary-button:hover {
    background-color: #5a6268;
}

/* Output Section */
.output-container {
    background: #f8f9fa;
    border-radius: 4px;
    padding: 1rem;
    max-height: 400px;
    overflow-y: auto;
    position: relative;
}

.loading {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    text-align: center;
}

.loading::after {
    content: '';
    display: block;
    width: 40px;
    height: 40px;
    margin: 10px auto;
    border: 4px solid var(--primary-color);
    border-right-color: transparent;
    border-radius: 50%;
    animation: spin 1s linear infinite;
}

@keyframes spin {
    from { transform: rotate(0deg); }
    to { transform: rotate(360deg); }
}

.loading-text {
    color: var(--primary-color);
    font-weight: 500;
    margin-top: 1rem;
}

.output-container pre {
    font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
    font-size: 0.875rem;
    white-space: pre-wrap;
    word-break: break-word;
}

.output-actions {
    display: flex;
    gap: 1rem;
    margin-top: 1rem;
}

/* Responsive Design */
@media (max-width: 768px) {
    .container {
        padding: 1rem;
    }

    section {
        padding: 1.5rem;
    }

    .upload-area {
        padding: 2rem;
    }

    .form-actions {
        flex-direction: column;
    }

    button {
        width: 100%;
        margin: 0.5rem 0;
    }
}

/* Client-specific settings */
#ollamaSettings,
#openaiSettings {
    border-left: 3px solid var(--primary-color);
    padding-left: 1rem;
    margin: 1rem 0;
}


--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\video_analyzer_ui\static\js\main.js ---

// Global state
let currentSession = null;
let outputEventSource = null;

// DOM Elements
const dropZone = document.getElementById('dropZone');
const fileInput = document.getElementById('fileInput');
const configSection = document.getElementById('configSection');
const outputSection = document.getElementById('outputSection');
const analysisForm = document.getElementById('analysisForm');
const outputText = document.getElementById('outputText');
const commandPreview = document.getElementById('commandPreview');
const downloadResults = document.getElementById('downloadResults');
const newAnalysis = document.getElementById('newAnalysis');
const clientSelect = document.getElementById('client');
const ollamaSettings = document.getElementById('ollamaSettings');
const openaiSettings = document.getElementById('openaiSettings');

// Event Listeners
dropZone.addEventListener('click', () => fileInput.click());
dropZone.addEventListener('dragover', handleDragOver);
dropZone.addEventListener('dragleave', handleDragLeave);
dropZone.addEventListener('drop', handleDrop);
fileInput.addEventListener('change', handleFileSelect);
analysisForm.addEventListener('submit', handleAnalysis);
analysisForm.addEventListener('input', updateCommandPreview);
clientSelect.addEventListener('change', toggleClientSettings);
downloadResults.addEventListener('click', downloadAnalysisResults);
newAnalysis.addEventListener('click', resetUI);

// File Upload Handlers
function handleDragOver(e) {
    e.preventDefault();
    dropZone.classList.add('drag-over');
}

function handleDragLeave(e) {
    e.preventDefault();
    dropZone.classList.remove('drag-over');
}

function handleDrop(e) {
    e.preventDefault();
    dropZone.classList.remove('drag-over');
    
    const file = e.dataTransfer.files[0];
    if (isValidVideoFile(file)) {
        handleFile(file);
    } else {
        alert('Please upload a valid video file (MP4, AVI, MOV, or MKV)');
    }
}

function handleFileSelect(e) {
    const file = e.target.files[0];
    if (file && isValidVideoFile(file)) {
        handleFile(file);
    }
}

function isValidVideoFile(file) {
    const validTypes = ['.mp4', '.avi', '.mov', '.mkv'];
    return validTypes.some(type => file.name.toLowerCase().endsWith(type));
}

async function handleFile(file) {
    const formData = new FormData();
    formData.append('video', file);
    
    try {
        const response = await fetch('/upload', {
            method: 'POST',
            body: formData
        });
        
        const data = await response.json();
        if (response.ok) {
            currentSession = data.session_id;
            showConfigSection();
        } else {
            throw new Error(data.error || 'Upload failed');
        }
    } catch (error) {
        alert(`Error uploading file: ${error.message}`);
    }
}

// Analysis Handlers
async function handleAnalysis(e) {
    e.preventDefault();
    if (!currentSession) return;
    
    const formData = new FormData(analysisForm);
    showOutputSection();
    
    // Close any existing event source
    if (outputEventSource) {
        outputEventSource.close();
    }
    
    // Clear previous output and show loading
    outputText.textContent = '';
    const loadingDiv = document.createElement('div');
    loadingDiv.className = 'loading';
    loadingDiv.innerHTML = '<div class="loading-text">Analyzing video...</div>';
    outputText.parentElement.appendChild(loadingDiv);
    
    try {
        // Make POST request to start analysis
        const response = await fetch(`/analyze/${currentSession}`, {
            method: 'POST',
            body: formData
        });
        
        if (!response.ok) {
            const data = await response.json();
            throw new Error(data.error || 'Failed to start analysis');
        }
    } catch (error) {
        outputText.textContent = `Error starting analysis: ${error.message}\n`;
        document.querySelector('.output-actions').style.display = 'flex';
        downloadResults.style.display = 'none';
        loadingDiv.remove();
        return;
    }

    // Start SSE connection for output
    outputEventSource = new EventSource(`/analyze/${currentSession}/stream`);
    
    outputEventSource.onmessage = (event) => {
        // Remove loading indicator on first message
        if (loadingDiv.parentElement) {
            loadingDiv.remove();
        }
        
        outputText.textContent += event.data + '\n';
        outputText.scrollTop = outputText.scrollHeight;
        
        if (event.data.includes('Analysis completed successfully')) {
            outputEventSource.close();
            document.querySelector('.output-actions').style.display = 'flex';
            downloadResults.style.display = 'inline-block';
        } else if (event.data.includes('Analysis failed')) {
            outputEventSource.close();
            outputText.textContent += '\nAnalysis failed. Please check the output above for errors.\n';
            // Show new analysis button but not download button
            document.querySelector('.output-actions').style.display = 'flex';
            downloadResults.style.display = 'none';
        }
    };
    
    outputEventSource.onerror = (error) => {
        console.error('SSE Error:', error);
        outputEventSource.close();
        
        // Remove loading indicator if it exists
        if (loadingDiv.parentElement) {
            loadingDiv.remove();
        }
        
        outputText.textContent += '\nError: Connection to server lost. Please try again.\n';
        // Show new analysis button but not download button
        document.querySelector('.output-actions').style.display = 'flex';
        downloadResults.style.display = 'none';
    };
}

// UI Updates
function showConfigSection() {
    dropZone.style.display = 'none';
    configSection.style.display = 'block';
    updateCommandPreview();
}

function showOutputSection() {
    configSection.style.display = 'none';
    outputSection.style.display = 'block';
    document.querySelector('.output-actions').style.display = 'none';
}

function toggleClientSettings() {
    const client = clientSelect.value;
    if (client === 'ollama') {
        ollamaSettings.style.display = 'block';
        openaiSettings.style.display = 'none';
    } else {
        ollamaSettings.style.display = 'none';
        openaiSettings.style.display = 'block';
    }
    updateCommandPreview();
}

function updateCommandPreview() {
    const formData = new FormData(analysisForm);
    let command = 'video-analyzer <video_path>';
    
    for (const [key, value] of formData.entries()) {
        if (value) {
            if (key === 'keep-frames') {
                command += ` --${key}`;
            } else {
                command += ` --${key} ${value}`;
            }
        }
    }
    
    commandPreview.textContent = command;
}

// Results Handling
async function downloadAnalysisResults() {
    if (!currentSession) return;
    
    try {
        const response = await fetch(`/results/${currentSession}`);
        if (!response.ok) {
            const data = await response.json();
            throw new Error(data.error || 'Failed to fetch results');
        }
        
        const blob = await response.blob();
        const url = window.URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = 'analysis.json';
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        window.URL.revokeObjectURL(url);
    } catch (error) {
        alert(`Error downloading results: ${error.message}`);
        console.error('Download error:', error);
    }
}

function resetUI() {
    // Clean up current session
    if (currentSession) {
        fetch(`/cleanup/${currentSession}`, { method: 'POST' })
            .catch(error => console.error('Cleanup error:', error));
    }
    
    // Reset state
    currentSession = null;
    if (outputEventSource) {
        outputEventSource.close();
    }
    
    // Reset form
    analysisForm.reset();
    
    // Reset UI
    dropZone.style.display = 'block';
    configSection.style.display = 'none';
    outputSection.style.display = 'none';
    outputText.textContent = '';
    fileInput.value = '';
    
    // Reset client settings
    toggleClientSettings();
}

// Initialize UI
toggleClientSettings();


--- æ–‡ä»¶è·¯å¾„: video-analyzer-ui\video_analyzer_ui\templates\index.html ---

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Video Analyzer</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
</head>
<body>
    <div class="container">
        <header>
            <h1>Video Analyzer</h1>
        </header>

        <main>
            <!-- Upload Section -->
            <section class="upload-section">
                <div class="upload-area" id="dropZone">
                    <input type="file" id="fileInput" accept=".mp4,.avi,.mov,.mkv" hidden>
                    <div class="upload-content">
                        <svg class="upload-icon" viewBox="0 0 24 24">
                            <path d="M19 13h-6v6h-2v-6H5v-2h6V5h2v6h6v2z"/>
                        </svg>
                        <p>Drag & drop video or click to upload</p>
                        <p class="upload-formats">Supported formats: MP4, AVI, MOV, MKV</p>
                    </div>
                </div>
            </section>

            <!-- Configuration Section -->
            <section class="config-section" id="configSection" style="display: none;">
                <h2>Analysis Configuration</h2>
                <form id="analysisForm">
                    <!-- Client Configuration -->
                    <div class="form-group">
                        <label for="client">Client:</label>
                        <select id="client" name="client">
                            <option value="ollama">Ollama (Local)</option>
                            <option value="openai_api">OpenAI API</option>
                        </select>
                    </div>

                    <!-- Client-specific settings -->
                    <div id="ollamaSettings">
                        <div class="form-group">
                            <label for="ollama-url">Ollama URL:</label>
                            <input type="text" id="ollama-url" name="ollama-url" value="http://localhost:11434">
                        </div>
                    </div>

                    <div id="openaiSettings" style="display: none;">
                        <div class="form-group">
                            <label for="api-key">API Key: *</label>
                            <input type="password" id="api-key" name="api-key">
                        </div>
                        <div class="form-group">
                            <label for="api-url">API URL: *</label>
                            <input type="text" id="api-url" name="api-url" placeholder="https://openrouter.ai/api/v1">
                        </div>
                    </div>

                    <!-- Model Configuration -->
                    <div class="form-group">
                        <label for="model">Vision Model:</label>
                        <input type="text" id="model" name="model" placeholder="llama3.2-vision">
                    </div>

                    <!-- Processing Options -->
                    <div class="form-group">
                        <label for="duration">Duration (seconds):</label>
                        <input type="number" id="duration" name="duration" min="1">
                    </div>

                    <div class="form-group">
                        <label for="max-frames">Max Frames:</label>
                        <input type="number" id="max-frames" name="max-frames" min="1">
                    </div>

                    <div class="form-group">
                        <label for="whisper-model">Whisper Model:</label>
                        <select id="whisper-model" name="whisper-model">
                            <option value="tiny">Tiny</option>
                            <option value="base">Base</option>
                            <option value="small">Small</option>
                            <option value="medium" selected>Medium</option>
                            <option value="large">Large</option>
                        </select>
                    </div>

                    <div class="form-group">
                        <label for="language">Language:</label>
                        <input type="text" id="language" name="language" placeholder="Auto-detect">
                    </div>

                    <div class="form-group">
                        <label for="device">Device:</label>
                        <select id="device" name="device">
                            <option value="cpu">CPU</option>
                            <option value="cuda">CUDA (GPU)</option>
                            <option value="mps">MPS (Apple Silicon)</option>
                        </select>
                    </div>

                    <div class="form-group">
                        <label for="prompt">Analysis Prompt:</label>
                        <textarea id="prompt" name="prompt" placeholder="What would you like to know about the video?"></textarea>
                    </div>

                    <div class="form-group checkbox">
                        <input type="checkbox" id="keep-frames" name="keep-frames">
                        <label for="keep-frames">Keep Extracted Frames</label>
                    </div>

                    <!-- Command Preview -->
                    <div class="command-preview">
                        <h3>Command Preview</h3>
                        <pre id="commandPreview"></pre>
                    </div>

                    <div class="form-actions">
                        <button type="submit" class="primary-button">Start Analysis</button>
                    </div>
                </form>
            </section>

            <!-- Output Section -->
            <section class="output-section" id="outputSection" style="display: none;">
                <h2>Analysis Output</h2>
                <div class="output-container">
                    <pre id="outputText"></pre>
                </div>
                <div class="output-actions" style="display: none;">
                    <button id="downloadResults" class="secondary-button">Download Results</button>
                    <button id="newAnalysis" class="primary-button">New Analysis</button>
                </div>
            </section>
        </main>
    </div>

    <script src="{{ url_for('static', filename='js/main.js') }}"></script>
</body>
</html>


--- æ–‡ä»¶è·¯å¾„: video_analyzer\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: video_analyzer\analyzer.py ---

from typing import List, Dict, Any, Optional, Iterator
import logging
from .clients.llm_client import LLMClient
from .prompt import PromptLoader
from .frame import Frame
from .audio_processor import AudioTranscript

logger = logging.getLogger(__name__)

class VideoAnalyzer:
    def __init__(self, client: LLMClient, model: str, prompt_loader: PromptLoader, temperature: float, user_prompt: str = ""):
        self.client = client
        self.model = model
        self.prompt_loader = prompt_loader
        self.temperature = temperature
        self.user_prompt = user_prompt
        self.context_length: int = 4096
        self._load_prompts()
        self.previous_analyses = []
        
    def _format_user_prompt(self) -> str:
        if self.user_prompt:
            return f"I want to know {self.user_prompt}"
        return ""
        
    def _load_prompts(self):
        self.frame_prompt = self.prompt_loader.get_by_index(0)
        self.video_prompt = self.prompt_loader.get_by_index(1)

    def _format_previous_analyses(self) -> str:
        if not self.previous_analyses:
            return ""
        formatted_analyses = [f"Frame {i}\n{analysis.get('response', 'No analysis available')}\n" for i, analysis in enumerate(self.previous_analyses)]
        return "\n".join(formatted_analyses)

    def analyze_frame(self, frame: Frame, stream: bool = False) -> Iterator[str] | Dict[str, Any]:
        """Analyze a single frame. Can return a stream of text chunks or a final dictionary."""
        prompt = self.frame_prompt.replace("{PREVIOUS_FRAMES}", self._format_previous_analyses())
        prompt = prompt.replace("{prompt}", self._format_user_prompt())
        prompt = f"{prompt}\nThis is frame {frame.number} captured at {frame.timestamp:.2f} seconds."
        
        try:
            response_generator = self.client.generate(
                prompt=prompt,
                image_path=str(frame.path),
                model=self.model,
                temperature=self.temperature,
                num_predict=300,
                context_length=self.context_length,
                stream=stream
            )
            
            if stream:
                return response_generator
            else:
                # Consume the generator to get the full response
                full_response = "".join(list(response_generator))
                analysis_result = {"response": full_response}
                self.previous_analyses.append(analysis_result)
                return analysis_result

        except Exception as e:
            logger.error(f"Error analyzing frame {frame.number}: {e}")
            error_result = {"response": f"Error analyzing frame {frame.number}: {str(e)}"}
            if not stream:
                self.previous_analyses.append(error_result)
            
            if stream:
                yield f"Error: {e}"
            else:
                return error_result

    def reconstruct_video(self, frame_analyses: List[Dict[str, Any]], frames: List[Frame], 
                         transcript: Optional[AudioTranscript] = None) -> Dict[str, Any]:
        frame_notes = []
        for i, (frame, analysis) in enumerate(zip(frames, frame_analyses)):
            frame_note = (
                f"Frame {i} ({frame.timestamp:.2f}s):\n"
                f"{analysis.get('response', 'No analysis available')}"
            )
            frame_notes.append(frame_note)
        
        analysis_text = "\n\n".join(frame_notes)
        
        first_frame_text = frame_analyses[0].get('response', '') if frame_analyses else ''
        
        transcript_text = transcript.text if transcript and transcript.text.strip() else ""
        
        prompt = self.video_prompt.replace("{prompt}", self._format_user_prompt())
        prompt = prompt.replace("{FRAME_NOTES}", analysis_text)
        prompt = prompt.replace("{FIRST_FRAME}", first_frame_text)
        prompt = prompt.replace("{TRANSCRIPT}", transcript_text)
        
        try:
            # Final reconstruction is not streamed
            response_generator = self.client.generate(
                prompt=prompt,
                model=self.model,
                temperature=self.temperature,
                num_predict=1000,
                context_length=self.context_length,
                stream=False
            )
            full_response = "".join(list(response_generator))
            logger.info("Successfully reconstructed video description")
            return {"response": full_response}
        except Exception as e:
            logger.error(f"Error reconstructing video: {e}")
            return {"response": f"Error reconstructing video: {str(e)}"}

--- æ–‡ä»¶è·¯å¾„: video_analyzer\audio_processor.py ---

import logging
from pathlib import Path
from typing import Optional, Dict, List, Any
from dataclasses import dataclass
import subprocess
import torch
from pydub import AudioSegment

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class AudioTranscript:
    text: str
    segments: List[Dict[str, Any]]
    language: str

class AudioProcessor:
    def __init__(self, 
                 language: str | None = None,
                 model_size_or_path: str = "medium",
                 device: str = "cpu"):
        """Initialize audio processor with specified Whisper model size or model path. By default, the medium model is used."""
        try:
            from faster_whisper import WhisperModel
            
            # Log cache directory
            cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
            logger.debug(f"Using HuggingFace cache directory: {cache_dir}")
            
            # Force CPU usage for now faster whisper having issues with cudas
            self.device = device
            compute_type = "float32"
            logger.debug(f"Using device: {self.device}")

            self.language = language if language else None

            self.model = WhisperModel(
                model_size_or_path,
                device=device,
                compute_type=compute_type
            )
            logger.info(f"Initiation Input: Model size or path: {model_size_or_path}, Device: {device}, Compute type: {compute_type}, Language: {self.language if self.language else 'auto detected'}")
            logger.debug(f"Successfully loaded Whisper model: {model_size_or_path}")
            
            # Check for ffmpeg installation
            try:
                subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
                self.has_ffmpeg = True
            except (subprocess.CalledProcessError, FileNotFoundError):
                self.has_ffmpeg = False
                logger.warning("FFmpeg not found. Please install ffmpeg for better audio extraction.")
                
        except Exception as e:
            logger.error(f"Error loading Whisper model: {e}")
            raise

    def extract_audio(self, video_path: Path, output_dir: Path) -> Optional[Path]:
        """Extract audio from video file and convert to format suitable for Whisper.
        Returns None if video has no audio streams."""
        audio_path = output_dir / "audio.wav"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # Extract audio using ffmpeg
            subprocess.run([
                "ffmpeg", "-i", str(video_path),
                "-vn",  # No video
                "-acodec", "pcm_s16le",  # PCM 16-bit little-endian
                "-ar", "16000",  # 16kHz sampling rate
                "-ac", "1",  # Mono
                "-y",  # Overwrite output
                str(audio_path)
            ], check=True, capture_output=True)
            
            logger.debug("Successfully extracted audio using ffmpeg")
            return audio_path
        except subprocess.CalledProcessError as e:
            error_output = e.stderr.decode()
            logger.error(f"FFmpeg error: {error_output}")
            
            # Check if error indicates no audio streams
            if "Output file does not contain any stream" in error_output:
                logger.debug("No audio streams found in video - skipping audio extraction")
                return None
                
            # If error is not about missing audio, try pydub as fallback
            logger.info("Falling back to pydub for audio extraction...")
            try:
                video = AudioSegment.from_file(str(video_path))
                audio = video.set_channels(1).set_frame_rate(16000)
                audio.export(str(audio_path), format="wav")
                logger.debug("Successfully extracted audio using pydub")
                return audio_path
            except Exception as e2:
                logger.error(f"Error extracting audio using pydub: {e2}")
                # If both methods fail, raise error
                raise RuntimeError(
                    "Failed to extract audio. Please install ffmpeg using:\n"
                    "Ubuntu/Debian: sudo apt-get update && sudo apt-get install -y ffmpeg\n"
                    "MacOS: brew install ffmpeg\n"
                    "Windows: choco install ffmpeg"
                )

    def transcribe(self, audio_path: Path) -> Optional[AudioTranscript]:
        """Transcribe audio file using Whisper with quality checks."""
        accepted_languages = {
                "af", "am", "ar", "as", "az", "ba", "be", "bg", "bn", "bo", "br", "bs", "ca", "cs", "cy", "da", "de", "el", "en", "es", "et", "eu", "fa", "fi", "fo", "fr", "gl", "gu", "ha", "haw", "he", "hi", "hr", "ht", "hu", "hy", "id", "is", "it", "ja", "jw", "ka", "kk", "km", "kn", "ko", "la", "lb", "ln", "lo", "lt", "lv", "mg", "mi", "mk", "ml", "mn", "mr", "ms", "mt", "my", "ne", "nl", "nn", "no", "oc", "pa", "pl", "ps", "pt", "ro", "ru", "sa", "sd", "si", "sk", "sl", "sn", "so", "sq", "sr", "su", "sv", "sw", "ta", "te", "tg", "th", "tk", "tl", "tr", "tt", "uk", "ur", "uz", "vi", "yi", "yo", "zh", "yue"
        }
        if self.language and self.language not in accepted_languages:
            logger.warning(f"Invalid language code: {self.language}, will detect language automatically")
        try:
            # Initial transcription with VAD filtering
            segments, info = self.model.transcribe(
                str(audio_path),
                beam_size=5,
                word_timestamps=True,
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=500),
                language = self.language if self.language in accepted_languages else None
            )
            
            segments_list = list(segments)
            if not segments_list:
                logger.warning("No speech detected in audio")
                return None
            
            # Convert segments to the expected format
            segment_data = [
                {
                    "text": segment.text,
                    "start": segment.start,
                    "end": segment.end,
                    "words": [
                        {
                            "word": word.word,
                            "start": word.start,
                            "end": word.end,
                            "probability": word.probability
                        }
                        for word in (segment.words or [])
                    ]
                }
                for segment in segments_list
            ]
            
            return AudioTranscript(
                text=" ".join(segment.text for segment in segments_list),
                segments=segment_data,
                language=info.language
            )
            
        except Exception as e:
            logger.error(f"Error transcribing audio: {e}")
            logger.exception(e)
            return None


--- æ–‡ä»¶è·¯å¾„: video_analyzer\cli.py ---

import argparse
from pathlib import Path
import json
import logging
import shutil
import sys
from typing import Optional
import torch
import torch.backends.mps

from .config import Config, get_client, get_model
from .frame import VideoProcessor
from .prompt import PromptLoader
from .analyzer import VideoAnalyzer
from .audio_processor import AudioProcessor, AudioTranscript
from .clients.ollama import OllamaClient
from .clients.generic_openai_api import GenericOpenAIAPIClient

# Initialize logger at module level
logger = logging.getLogger(__name__)

def get_log_level(level_str: str) -> int:
    """Convert string log level to logging constant."""
    levels = {
        'DEBUG': logging.DEBUG,
        'INFO': logging.INFO,
        'WARNING': logging.WARNING,
        'ERROR': logging.ERROR,
        'CRITICAL': logging.CRITICAL
    }
    return levels.get(level_str.upper(), logging.INFO)

def cleanup_files(output_dir: Path):
    """Clean up temporary files and directories."""
    try:
        frames_dir = output_dir / "frames"
        if frames_dir.exists():
            shutil.rmtree(frames_dir)
            logger.debug(f"Cleaned up frames directory: {frames_dir}")
            
        audio_file = output_dir / "audio.wav"
        if audio_file.exists():
            audio_file.unlink()
            logger.debug(f"Cleaned up audio file: {audio_file}")
    except Exception as e:
        logger.error(f"Error during cleanup: {e}")

def create_client(config: Config):
    """Create the appropriate client based on configuration."""
    client_type = config.get("clients", {}).get("default", "ollama")
    client_config = get_client(config)
    
    if client_type == "ollama":
        return OllamaClient(client_config["url"])
    elif client_type == "openai_api":
        return GenericOpenAIAPIClient(client_config["api_key"], client_config["api_url"])
    else:
        raise ValueError(f"Unknown client type: {client_type}")

def main():
    parser = argparse.ArgumentParser(description="Analyze video using Vision models")
    parser.add_argument("video_path", type=str, help="Path to the video file")
    parser.add_argument("--config", type=str, default="config",
                        help="Path to configuration directory")
    parser.add_argument("--output", type=str, help="Output directory for analysis results")
    parser.add_argument("--client", type=str, help="Client to use (ollama or openrouter)")
    parser.add_argument("--ollama-url", type=str, help="URL for the Ollama service")
    parser.add_argument("--api-key", type=str, help="API key for OpenAI-compatible service")
    parser.add_argument("--api-url", type=str, help="API URL for OpenAI-compatible API")
    parser.add_argument("--model", type=str, help="Name of the vision model to use")
    parser.add_argument("--duration", type=float, help="Duration in seconds to process")
    parser.add_argument("--keep-frames", action="store_true", help="Keep extracted frames after analysis")
    parser.add_argument("--whisper-model", type=str, help="Whisper model size (tiny, base, small, medium, large), or path to local Whisper model snapshot")
    parser.add_argument("--start-stage", type=int, default=1, help="Stage to start processing from (1-3)")
    parser.add_argument("--max-frames", type=int, default=sys.maxsize, help="Maximum number of frames to process")
    parser.add_argument("--log-level", type=str, default="INFO", 
                        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        help="Set the logging level (default: INFO)")
    parser.add_argument("--prompt", type=str, default="",
                        help="Question to ask about the video")
    parser.add_argument("--language", type=str, default=None)
    parser.add_argument("--device", type=str, default="cpu")
    parser.add_argument("--temperature", type=float, help="Temperature for LLM generation")
    args = parser.parse_args()

    # Set up logging with specified level
    log_level = get_log_level(args.log_level)
    # Configure the root logger
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        force=True  # Force reconfiguration of the root logger
    )
    # Ensure our module logger has the correct level
    logger.setLevel(log_level)

    # Load and update configuration
    config = Config(args.config)
    config.update_from_args(args)

    # Initialize components
    video_path = Path(args.video_path)
    output_dir = Path(config.get("output_dir"))
    client = create_client(config)
    model = get_model(config)
    prompt_loader = PromptLoader(config.get("prompt_dir"), config.get("prompts", []))
    
    try:
        transcript = None
        frames = []
        frame_analyses = []
        video_description = None
        
        # Stage 1: Frame and Audio Processing
        if args.start_stage <= 1:
            # Initialize audio processor and extract transcript, the AudioProcessor accept following parameters that can be set in config.json:
            # language (str): Language code for audio transcription (default: None)
            # whisper_model (str): Whisper model size or path (default: "medium")
            # device (str): Device to use for audio processing (default: "cpu")
            logger.debug("Initializing audio processing...")
            audio_processor = AudioProcessor(language=config.get("audio", {}).get("language", ""), 
                                             model_size_or_path=config.get("audio", {}).get("whisper_model", "medium"),
                                             device=config.get("audio", {}).get("device", "cpu"))
            
            logger.info("Extracting audio from video...")
            audio_path = audio_processor.extract_audio(video_path, output_dir)
            
            if audio_path is None:
                logger.debug("No audio found in video - skipping transcription")
                transcript = None
            else:
                logger.info("Transcribing audio...")
                transcript = audio_processor.transcribe(audio_path)
                if transcript is None:
                    logger.warning("Could not generate reliable transcript. Proceeding with video analysis only.")
            
            logger.info(f"Extracting frames from video using model {model}...")
            processor = VideoProcessor(
                video_path, 
                output_dir / "frames", 
                model
            )
            frames = processor.extract_keyframes(
                frames_per_minute=config.get("frames", {}).get("per_minute", 60),
                duration=config.get("duration"),
                max_frames=args.max_frames
            )
            
        # Stage 2: Frame Analysis
        if args.start_stage <= 2:
            logger.info("Analyzing frames...")
            analyzer = VideoAnalyzer(
                client, 
                model, 
                prompt_loader,
                config.get("clients", {}).get("temperature", 0.2),
                config.get("prompt", "")
            )
            frame_analyses = []
            for frame in frames:
                analysis = analyzer.analyze_frame(frame)
                frame_analyses.append(analysis)
                
        # Stage 3: Video Reconstruction
        if args.start_stage <= 3:
            logger.info("Reconstructing video description...")
            video_description = analyzer.reconstruct_video(
                frame_analyses, frames, transcript
            )
        
        output_dir.mkdir(parents=True, exist_ok=True)
        results = {
            "metadata": {
                "client": config.get("clients", {}).get("default"),
                "model": model,
                "whisper_model": config.get("audio", {}).get("whisper_model"),
                "frames_per_minute": config.get("frames", {}).get("per_minute"),
                "duration_processed": config.get("duration"),
                "frames_extracted": len(frames),
                "frames_processed": min(len(frames), args.max_frames),
                "start_stage": args.start_stage,
                "audio_language": transcript.language if transcript else None,
                "transcription_successful": transcript is not None
            },
            "transcript": {
                "text": transcript.text if transcript else None,
                "segments": transcript.segments if transcript else None
            } if transcript else None,
            "frame_analyses": frame_analyses,
            "video_description": video_description
        }
        
        with open(output_dir / "analysis.json", "w") as f:
            json.dump(results, f, indent=2)
            
        logger.info("\nTranscript:")
        if transcript:
            logger.info(transcript.text)
        else:
            logger.info("No reliable transcript available")
            
        if video_description:
            logger.info("\nVideo Description:")
            logger.info(video_description.get("response", "No description generated"))
        
        if not config.get("keep_frames"):
            cleanup_files(output_dir)
        
        logger.info(f"Analysis complete. Results saved to {output_dir / 'analysis.json'}")
            
    except Exception as e:
        logger.error(f"Error during video analysis: {e}")
        if not config.get("keep_frames"):
            cleanup_files(output_dir)
        raise

if __name__ == "__main__":
    main()


--- æ–‡ä»¶è·¯å¾„: video_analyzer\config.py ---

import argparse
from pathlib import Path
import json
from typing import Any
import logging
import pkg_resources

logger = logging.getLogger(__name__)

class Config:
    def __init__(self, config_dir: str = "config"):
        # Handle user-provided config directory
        self.config_dir = Path(config_dir)
        self.user_config = self.config_dir / "config.json"
        
        # First try to find default_config.json in the user-provided directory
        self.default_config = self.config_dir / "default_config.json"
        
        # If not found, fallback to package's default config
        if not self.default_config.exists():
            try:
                default_config_path = pkg_resources.resource_filename('video_analyzer', 'config/default_config.json')
                self.default_config = Path(default_config_path)
                logger.debug(f"Using packaged default config from {self.default_config}")
            except Exception as e:
                logger.error(f"Error finding default config: {e}")
                raise
            
        self.load_config()

    def load_config(self):
        """Load configuration from JSON file with cascade:
        1. Try user config (config.json)
        2. Fall back to default config (default_config.json)
        """
        try:
            if self.user_config.exists():
                logger.debug(f"Loading user config from {self.user_config}")
                with open(self.user_config) as f:
                    self.config = json.load(f)
            else:
                logger.debug(f"No user config found, loading default config from {self.default_config}")
                with open(self.default_config) as f:
                    self.config = json.load(f)
                    
            # Ensure prompts is a list
            if not isinstance(self.config.get("prompts", []), list):
                logger.warning("Prompts in config is not a list, setting to empty list")
                self.config["prompts"] = []
                
        except Exception as e:
            logger.error(f"Error loading config: {e}")
            raise

    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value with optional default."""
        return self.config.get(key, default)

    def update_from_args(self, args: argparse.Namespace):
        """Update configuration with command line arguments."""
        for key, value in vars(args).items():
            if value is not None:  # Only update if argument was provided
                if key == "client":
                    self.config["clients"]["default"] = value
                elif key == "ollama_url":
                    self.config["clients"]["ollama"]["url"] = value
                elif key == "api_key":
                    self.config["clients"]["openai_api"]["api_key"] = value
                    # If key is provided but no client specified, use OpenAI API
                    if not args.client:
                        self.config["clients"]["default"] = "openai_api"
                elif key == "api_url":
                    self.config["clients"]["openai_api"]["api_url"] = value
                elif key == "model":
                    client = self.config["clients"]["default"]
                    self.config["clients"][client]["model"] = value
                elif key == "prompt":
                    self.config["prompt"] = value
                #overide audio config
                elif key == "whisper_model":
                    self.config["audio"]["whisper_model"] = value  # default is 'medium'
                elif key == "language":
                    if value is not None:
                        self.config["audio"]["language"] = value
                elif key == "device":
                    self.config["audio"]["device"] = value
                elif key == "temperature":
                    self.config["clients"]["temperature"] = value
                elif key not in ["start_stage", "max_frames"]:  # Ignore these as they're command-line only
                    self.config[key] = value

    def save_user_config(self):
        """Save current configuration to user config file."""
        try:
            self.config_dir.mkdir(parents=True, exist_ok=True)
            with open(self.user_config, 'w') as f:
                json.dump(self.config, f, indent=2)
            logger.debug(f"Saved user config to {self.user_config}")
        except Exception as e:
            logger.error(f"Error saving user config: {e}")
            raise

def get_client(config: Config) -> dict:
    """Get the appropriate client configuration based on configuration."""
    client_type = config.get("clients", {}).get("default", "ollama")
    client_config = config.get("clients", {}).get(client_type, {})
    
    if client_type == "ollama":
        return {"url": client_config.get("url", "http://localhost:11434")}
    elif client_type == "openai_api":
        api_key = client_config.get("api_key")
        api_url = client_config.get("api_url")
        if not api_key:
            raise ValueError("API key is required when using OpenAI API client")
        if not api_url:
            raise ValueError("API URL is required when using OpenAI API client")
        return {
            "api_key": api_key,
            "api_url": api_url
        }
    else:
        raise ValueError(f"Unknown client type: {client_type}")

def get_model(config: Config) -> str:
    """Get the appropriate model based on client type and configuration."""
    client_type = config.get("clients", {}).get("default", "ollama")
    client_config = config.get("clients", {}).get(client_type, {})
    return client_config.get("model", "llama3.2-vision")


--- æ–‡ä»¶è·¯å¾„: video_analyzer\frame.py ---

from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional
import cv2
import numpy as np
import logging

logger = logging.getLogger(__name__)

@dataclass
class Frame:
    number: int
    path: Path
    timestamp: float
    score: float

class VideoProcessor:
    # Class constants
    FRAME_DIFFERENCE_THRESHOLD = 10.0
    
    def __init__(self, video_path: Path, output_dir: Path, model: str):
        self.video_path = video_path
        self.output_dir = output_dir
        self.model = model
        self.frames: List[Frame] = []
        
    def _calculate_frame_difference(self, frame1: np.ndarray, frame2: np.ndarray) -> float:
        """Calculate the difference between two frames using absolute difference."""
        if frame1 is None or frame2 is None:
            return 0.0
        
        # Convert to grayscale for simpler comparison
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
        
        # Calculate absolute difference and mean
        diff = cv2.absdiff(gray1, gray2)
        score = np.mean(diff)
        
        return float(score)

    def _is_keyframe(self, current_frame: np.ndarray, prev_frame: np.ndarray, threshold: float = FRAME_DIFFERENCE_THRESHOLD) -> bool:
        """Determine if frame is significantly different from previous frame."""
        if prev_frame is None:
            return True
            
        score = self._calculate_frame_difference(current_frame, prev_frame)
        return score > threshold

    def extract_keyframes(self, frames_per_minute: int = 10, duration: Optional[float] = None, max_frames: Optional[int] = None) -> List[Frame]:
        """Extract keyframes from video targeting a specific number of frames per minute."""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        cap = cv2.VideoCapture(str(self.video_path))
        if not cap.isOpened():
            raise ValueError(f"Could not open video file: {self.video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        video_duration = total_frames / fps
        
        if duration:
            video_duration = min(duration, video_duration)
            total_frames = int(min(total_frames, duration * fps))
        
        # Calculate target number of frames
        target_frames = max(1, min(
            int((video_duration / 60) * frames_per_minute),
            total_frames,
            max_frames if max_frames is not None else float('inf')
        ))
        
        # Calculate adaptive sampling interval
        sample_interval = max(1, total_frames // (target_frames * 2))
        
        frame_candidates = []
        prev_frame = None
        frame_count = 0
        
        while frame_count < total_frames:
            ret, frame = cap.read()
            if not ret:
                break
                
            if frame_count % sample_interval == 0:
                score = self._calculate_frame_difference(frame, prev_frame)
                if score > self.FRAME_DIFFERENCE_THRESHOLD:
                    frame_candidates.append((frame_count, frame, score))
                prev_frame = frame.copy()
                
            frame_count += 1
            
        cap.release()
        
        # Select the most significant frames
        selected_candidates = sorted(frame_candidates, key=lambda x: x[2], reverse=True)[:target_frames]
        
        # If max_frames is specified, sample evenly across the candidates
        if max_frames is not None and max_frames < len(selected_candidates):
            step = len(selected_candidates) / max_frames
            selected_frames = [selected_candidates[int(i * step)] for i in range(max_frames)]
        else:
            selected_frames = selected_candidates

        self.frames = []
        for idx, (frame_num, frame, score) in enumerate(selected_frames):
            frame_path = self.output_dir / f"frame_{idx}.jpg"
            cv2.imwrite(str(frame_path), frame)
            timestamp = frame_num / fps
            self.frames.append(Frame(idx, frame_path, timestamp, score))
        
        logger.info(f"Extracted {len(self.frames)} frames from video (target was {target_frames})")
        return self.frames


--- æ–‡ä»¶è·¯å¾„: video_analyzer\prompt.py ---

from pathlib import Path
import logging
from typing import List, Dict
import pkg_resources

logger = logging.getLogger(__name__)

class PromptLoader:
    def __init__(self, prompt_dir: str, prompts: List[Dict[str, str]]):
        # Handle user-provided prompt directory
        self.prompt_dir = Path(prompt_dir).expanduser() if prompt_dir else None
        self.prompts = prompts

    def _find_prompt_file(self, prompt_path: str) -> Path:
        """Find prompt file in package resources, package directory, or user directory."""
        # First try package resources (works for both install modes)
        try:
            package_path = pkg_resources.resource_filename('video_analyzer', f'prompts/{prompt_path}')
            if Path(package_path).exists():
                return Path(package_path)
        except Exception as e:
            logger.debug(f"Could not find package prompt via pkg_resources: {e}")

        # Try package directory (for development mode)
        pkg_root = Path(__file__).parent
        pkg_path = pkg_root / 'prompts' / prompt_path
        if pkg_path.exists():
            return pkg_path

        # Finally try user-specified directory if provided
        if self.prompt_dir:
            user_path = Path(self.prompt_dir).expanduser()
            # Try absolute path
            if user_path.is_absolute():
                full_path = user_path / prompt_path
                if full_path.exists():
                    return full_path
            else:
                # Try relative to current directory
                cwd_path = Path.cwd() / self.prompt_dir / prompt_path
                if cwd_path.exists():
                    return cwd_path

        raise FileNotFoundError(
            f"Prompt file not found in package resources, package directory, or user directory ({self.prompt_dir})"
        )

    def get_by_index(self, index: int) -> str:
        """Load prompt from file by index.
        
        Args:
            index: Index of the prompt in the prompts list
            
        Returns:
            The prompt text content
            
        Raises:
            IndexError: If index is out of range
            FileNotFoundError: If prompt file doesn't exist
        """
        try:
            if index < 0 or index >= len(self.prompts):
                raise IndexError(f"Prompt index {index} out of range (0-{len(self.prompts)-1})")
            
            prompt = self.prompts[index]
            prompt_path = self._find_prompt_file(prompt["path"])
                
            logger.debug(f"Loading prompt '{prompt['name']}' from {prompt_path}")
            with open(prompt_path) as f:
                return f.read().strip()
        except Exception as e:
            logger.error(f"Error loading prompt at index {index}: {e}")
            raise

    def get_by_name(self, name: str) -> str:
        """Load prompt from file by name.
        
        Args:
            name: Name of the prompt to load
            
        Returns:
            The prompt text content
            
        Raises:
            ValueError: If prompt name not found
            FileNotFoundError: If prompt file doesn't exist
        """
        try:
            prompt = next((p for p in self.prompts if p["name"] == name), None)
            if prompt is None:
                raise ValueError(f"Prompt with name '{name}' not found")
            
            prompt_path = self._find_prompt_file(prompt["path"])
                
            logger.debug(f"Loading prompt '{name}' from {prompt_path}")
            with open(prompt_path) as f:
                return f.read().strip()
        except Exception as e:
            logger.error(f"Error loading prompt '{name}': {e}")
            raise


--- æ–‡ä»¶è·¯å¾„: video_analyzer\clients\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: video_analyzer\clients\generic_openai_api.py ---

import requests
import json
import time
import re
from typing import Optional, Dict, Any, Tuple
from .llm_client import LLMClient
import logging

logger = logging.getLogger(__name__)

# Constants
DEFAULT_MAX_RETRIES = 3
RATE_LIMIT_WAIT_TIME = 25  # seconds
DEFAULT_WAIT_TIME = 25  # seconds

class GenericOpenAIAPIClient(LLMClient):
    def __init__(self, api_key: str, api_url: str, max_retries: int = DEFAULT_MAX_RETRIES):
        self.api_key = api_key
        self.base_url = api_url.rstrip('/')  # Remove trailing slash if present
        self.generate_url = f"{self.base_url}/chat/completions"
        self.max_retries = max_retries

    def generate(self,
        prompt: str,
        image_path: Optional[str] = None,
        stream: bool = False,
        model: str = "llama3.2-vision",
        temperature: float = 0.2,
        num_predict: int = 256) -> Dict[Any, Any]:
        """Generate response from OpenAI-compatible API."""
        # Prepare request content
        if image_path:
            base64_image = self.encode_image(image_path)
            content = [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
                }
            ]
        else:
            content = prompt

        # Prepare request data
        data = {
            "model": model,
            "messages": [{"role": "user", "content": content}],
            "stream": stream,
            "temperature": temperature,
            "max_tokens": num_predict
        }

        # Prepare headers
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://github.com/byjlw/video-analyzer",
            "X-Title": "Video Analyzer",
            "Content-Type": "application/json"
        }

        # Try request with retries
        for attempt in range(self.max_retries):
            try:
                response = requests.post(self.generate_url, headers=headers, json=data)
                response.raise_for_status()
                
                # Parse successful response
                try:
                    json_response = response.json()
                    if 'error' in json_response:
                        raise Exception(f"API error: {json_response['error']}")
                    
                    if stream:
                        return self._handle_streaming_response(response)
                    
                    if 'choices' not in json_response or not json_response['choices']:
                        raise Exception("No choices in response")
                        
                    message = json_response['choices'][0].get('message', {})
                    if not message or 'content' not in message:
                        raise Exception("No content in response message")
                        
                    return {"response": message['content']}
                    
                except json.JSONDecodeError:
                    raise Exception(f"Invalid JSON response: {response.text}")
                    
            except Exception as e:
                if attempt == self.max_retries - 1:  # Last attempt
                    raise Exception(f"An error occurred: {str(e)}")
                
                # Get wait time based on error
                wait_time = RATE_LIMIT_WAIT_TIME
                if isinstance(e, requests.exceptions.HTTPError) and e.response.status_code == 429:
                    # Try to get wait time from Retry-After header
                    if 'Retry-After' in e.response.headers:
                        try:
                            wait_time = int(e.response.headers['Retry-After'])
                            logger.info(f"Using Retry-After header value: {wait_time} seconds")
                        except (ValueError, TypeError):
                            logger.warning("Invalid Retry-After header value, using default wait time")
                else:
                    wait_time = DEFAULT_WAIT_TIME
                
                logger.warning(f"Request failed (attempt {attempt + 1}/{self.max_retries}): {str(e)}")
                logger.warning(f"Waiting {wait_time} seconds before retry")
                time.sleep(wait_time)

    def _handle_streaming_response(self, response: requests.Response) -> Dict[Any, Any]:
        """Handle streaming response from API.
        
        Args:
            response: Streaming response from API
            
        Returns:
            Dict containing accumulated response
        """
        accumulated_response = ""
        for line in response.iter_lines():
            if line:
                try:
                    json_response = json.loads(line.decode('utf-8'))
                    if 'choices' in json_response and len(json_response['choices']) > 0:
                        delta = json_response['choices'][0].get('delta', {})
                        if 'content' in delta:
                            accumulated_response += delta['content']
                except json.JSONDecodeError:
                    continue

        return {"response": accumulated_response}


--- æ–‡ä»¶è·¯å¾„: video_analyzer\clients\llm_client.py ---

from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
import base64

class LLMClient(ABC):
    def encode_image(self, image_path: str) -> str:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    @abstractmethod
    def generate(self,
        prompt: str,
        image_path: Optional[str] = None,
        stream: bool = False,
        model: str = "llama3.2-vision",
        temperature: float = 0.2,
        num_predict: int = 256) -> Dict[Any, Any]:
        pass


--- æ–‡ä»¶è·¯å¾„: video_analyzer\clients\ollama.py ---

import requests
import json
from typing import Optional, Dict, Any, Iterator
from .llm_client import LLMClient

class OllamaClient(LLMClient):
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url.rstrip('/')
        self.generate_url = f"{self.base_url}/api/generate"

    def generate(self,
        prompt: str,
        image_path: Optional[str] = None,
        stream: bool = False,
        model: str = "llama3.2-vision",
        temperature: float = 0.2,
        num_predict: int = 256,
        context_length: Optional[int] = None) -> Iterator[str]:
        try:
            data = {
                "model": model,
                "prompt": prompt,
                "stream": True,  # Always stream from the API
                "options": {
                    "temperature": temperature,
                    "num_predict": num_predict
                }
            }
            
            if context_length is not None:
                data["options"]["num_ctx"] = context_length

            if image_path:
                data["images"] = [self.encode_image(image_path)]
                    
            response = requests.post(self.generate_url, json=data, stream=True)
            response.raise_for_status()
            
            # This function is now always a generator
            def response_generator():
                for line in response.iter_lines():
                    if line:
                        try:
                            json_response = json.loads(line.decode('utf-8'))
                            if 'response' in json_response:
                                yield json_response['response']
                            if json_response.get('done'):
                                break
                        except json.JSONDecodeError:
                            continue
            
            # If the user doesn't want to stream, we consume the generator here
            if not stream:
                return "".join(list(response_generator()))
            else:
                return response_generator()
                
        except requests.exceptions.RequestException as e:
            raise Exception(f"API request failed: {str(e)}")
        except Exception as e:
            raise Exception(f"An error occurred: {str(e)}")

--- æ–‡ä»¶è·¯å¾„: video_analyzer\config\default_config.json ---

{
    "clients": {
        "default": "ollama",
        "temperature": 0.0,
        "ollama": {
            "url": "http://localhost:11434",
            "model": "llama3.2-vision"
        },
        "openai_api": {
            "api_key": "",
            "model": "meta-llama/llama-3.2-11b-vision-instruct",
            "api_url": "https://openrouter.ai/api/v1"
        }
    },
    "prompt_dir": "prompts",
    "prompts": [
        {
            "name": "Frame Analysis",
            "path": "frame_analysis/frame_analysis.txt"
        },
        {
            "name": "Video Reconstruction",
            "path": "frame_analysis/describe.txt"
        }
    ],
    "output_dir": "output",
    "frames": {
        "per_minute": 60,
        "analysis_threshold": 10.0,
        "min_difference": 5.0,
        "max_count": 30,
        "start_stage": 1,
        "max_frames": 2147483647
    },
    "response_length": {
        "frame": 300,
        "reconstruction": 1000,
        "narrative": 500
    },
    "audio": {
        "whisper_model": "medium",
        "sample_rate": 16000,
        "channels": 1,
        "quality_threshold": 0.2,
        "chunk_length": 30,
        "language_confidence_threshold": 0.8,
        "language": "en",
        "device": "cpu"
    },
    "keep_frames": false,
    "prompt": ""
}


--- æ–‡ä»¶è·¯å¾„: video_analyzer\prompts\frame_analysis\describe.txt ---

Video Summary Instructions
Available Materials
Frame 1 of the video (viewable)

Available Materials
Complete set of chronological frame descriptions from all previous viewers

{FRAME_NOTES}

Video Transcript

{TRANSCRIPT}



Your Task
You are synthesizing multiple frame descriptions into a cohesive video summary. You have access to the first frame and detailed notes about all subsequent frames.
Step 1: Review Process

First Frame Analysis

Study your available frame in detail
Note opening composition, characters, and setting
Identify the initial tone and context


Notes Review

Read through all frame descriptions chronologically
Mark key transitions and major developments
Identify narrative patterns and themes
Note any inconsistencies or gaps in descriptions



Step 2: Synthesis Guidelines
Create your summary following this structure:

Opening Description

Begin with what you can directly verify from your frame
Establish the initial setting, characters, and situation


Narrative Development

Build the story chronologically
Connect scenes and transitions naturally
Maintain consistent character descriptions
Track significant object movements and changes
Include relevant audio elements mentioned in notes


Technical Elements

Note camera movements described
Include editing transitions
Reference significant visual effects
Mention notable lighting or composition changes



Writing Style Guidelines

Write in present tense
Use clear, active voice
Maintain objective descriptions
Avoid speculation beyond provided notes
Include specific details that build credibility
Connect scenes with smooth transitions
Maintain consistent tone throughout

Quality Check
Before submitting, verify:

The summary flows naturally
No contradictions exist between sections
All major elements from notes are included
The narrative is coherent for someone who hasn't seen the video
Technical terms are used correctly
The opening matches your viewed frame
Transitions between described frames feel natural

{prompt}

Format Your Summary As:

```
IDEO SUMMARY
Duration: [if provided in notes]

[Opening paragraph - based on your viewed frame]

[Main body - chronological progression from notes]

[Closing observations - final state/resolution]

Note: This summary is based on direct observation of the first frame combined with detailed notes from subsequent frames.


--- æ–‡ä»¶è·¯å¾„: video_analyzer\prompts\frame_analysis\frame_analysis.txt ---

Frame Description Instructions
Previous Notes Section
[Previous frame descriptions will appear here in chronological order]

{PREVIOUS_FRAMES}

Your Tasks
You are viewing Frame [X] of this video sequence. Your goal is to document what you observe in a way that contributes to a coherent narrative of the entire video.
Step 1: Quick Scan

Watch for key changes from the previous descriptions
Note any new elements or developments
Identify if this is a transition moment or continuation

Step 2: Document Your Frame
Follow this structure for your notes:

Setting/Scene (if changed from previous)

Only describe if there's a notable change from previous frames
Include any new environmental details


Action/Movement

What is happening in this specific moment?
Focus on motion and changes
Note the direction of movement
Describe gestures or expressions


New Information

Document any new objects, people, or text that appears
Note any changes in audio described in previous frames
Record any new dialogue or text shown


Continuity Points

Connect your observations to previous notes
Highlight how this frame advances the narrative
Note if something mentioned in previous frames is no longer visible



Writing Guidelines

Use present tense
Be specific and concise
Avoid interpretation - stick to what you can see
Use clear transitional phrases to connect to previous descriptions
Include timestamp if available

{prompt}

Format Your Notes As:

```
Frame [X] If there are no existing frames you are Frame 0, otherwise you're the next frame
[Your observations following the structure above]
Key continuation points:
- [List 2-3 elements that the next viewer should particularly watch for]

```


--- æ–‡ä»¶è·¯å¾„: video_analyzer.egg-info\PKG-INFO ---

Metadata-Version: 2.4
Name: video-analyzer
Version: 0.1.1
Summary: A tool for analyzing videos using Vision models
Author: Jesse White
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: opencv-python>=4.8.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: torch>=2.0.0
Requires-Dist: openai-whisper>=20231117
Requires-Dist: requests>=2.31.0
Requires-Dist: Pillow>=10.0.0
Requires-Dist: pydub>=0.25.1
Requires-Dist: faster-whisper>=0.6.0
Dynamic: author
Dynamic: description
Dynamic: description-content-type
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Video Analysis using vision models like Llama3.2 Vision and OpenAI's Whisper Models

A video analysis tool that combines vision models like Llama's 11B vision model and Whisper to create a description by taking key frames, feeding them to the vision model to get details. It uses the details from each frame and the transcript, if available, to describe what's happening in the video. 

## Table of Contents
- [Features](#features)
- [Requirements](#requirements)
  - [System Requirements](#system-requirements)
  - [Installation](#installation)
  - [Ollama Setup](#ollama-setup)
  - [OpenAI-compatible API Setup](#openai-compatible-api-setup-optional)
- [Usage](#usage)
  - [Quick Start](#quick-start)
  - [Sample Output](#sample-output)
  - [Complete Usage Guide](docs/USAGES.md)
- [Design](#design)
  - [Detailed Design Documentation](docs/DESIGN.md)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Output](#output)
- [Uninstallation](#uninstallation)
- [License](#license)
- [Contributing](#contributing)

## Features
- ğŸ’» Can run completely locally - no cloud services or API keys needed
- â˜ï¸  Or, leverage any OpenAI API compatible LLM service (openrouter, openai, etc) for speed and scale
- ğŸ¬ Intelligent key frame extraction from videos
- ğŸ”Š High-quality audio transcription using OpenAI's Whisper
- ğŸ‘ï¸ Frame analysis using Ollama and Llama3.2 11B Vision Model
- ğŸ“ Natural language descriptions of video content
- ğŸ”„ Automatic handling of poor quality audio
- ğŸ“Š Detailed JSON output of analysis results
- âš™ï¸ Highly configurable through command line arguments or config file

## Design
The system operates in three stages:

1. Frame Extraction & Audio Processing
   - Uses OpenCV to extract key frames
   - Processes audio using Whisper for transcription
   - Handles poor quality audio with confidence checks

2. Frame Analysis
   - Analyzes each frame using vision LLM
   - Each analysis includes context from previous frames
   - Maintains chronological progression
   - Uses frame_analysis.txt prompt template

3. Video Reconstruction
   - Combines frame analyses chronologically
   - Integrates audio transcript
   - Uses first frame to set the scene
   - Creates comprehensive video description

![Design](docs/design.png)

## Requirements

### System Requirements
- Python 3.11 or higher
- FFmpeg (required for audio processing)
- When running LLMs locally (not necessary when using openrouter)
  - At least 16GB RAM (32GB recommended)
  - GPU at least 12GB of VRAM or Apple M Series with at least 32GB

### Installation

1. Clone the repository:
```bash
git clone https://github.com/byjlw/video-analyzer.git
cd video-analyzer
```

2. Create and activate a virtual environment:
```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install the package:
```bash
pip install .  # For regular installation
# OR
pip install -e .  # For development installation
```

4. Install FFmpeg:
- Ubuntu/Debian:
  ```bash
  sudo apt-get update && sudo apt-get install -y ffmpeg
  ```
- macOS:
  ```bash
  brew install ffmpeg
  ```
- Windows:
  ```bash
  choco install ffmpeg
  ```

### Ollama Setup

1. Install Ollama following the instructions at [ollama.ai](https://ollama.ai)

2. Pull the default vision model:
```bash
ollama pull llama3.2-vision
```

3. Start the Ollama service:
```bash
ollama serve
```

### OpenAI-compatible API Setup (Optional)

If you want to use OpenAI-compatible APIs (like OpenRouter or OpenAI) instead of Ollama:

1. Get an API key from your provider:
   - [OpenRouter](https://openrouter.ai)
   - [OpenAI](https://platform.openai.com)

2. Configure via command line:
   ```bash
   # For OpenRouter
   video-analyzer video.mp4 --client openai_api --api-key your-key --api-url https://openrouter.ai/api/v1 --model gpt-4o

   # For OpenAI
   video-analyzer video.mp4 --client openai_api --api-key your-key --api-url https://api.openai.com/v1 --model gpt-4o
   ```

   Or add to config/config.json:
   ```json
   {
     "clients": {
       "default": "openai_api",
       "openai_api": {
         "api_key": "your-api-key",
         "api_url": "https://openrouter.ai/api/v1"  # or https://api.openai.com/v1
       }
     }
   }
   ```

Note: With OpenRouter, you can use llama 3.2 11b vision for free by adding :free to the model name

## Design
For detailed information about the project's design and implementation, including how to make changes, see [docs/DESIGN.md](docs/DESIGN.md).

## Usage

For detailed usage instructions and all available options, see [docs/USAGES.md](docs/USAGES.md).

### Quick Start

```bash
# Local analysis with Ollama (default)
video-analyzer video.mp4

# Cloud analysis with OpenRouter
video-analyzer video.mp4 \
    --client openai_api \
    --api-key your-key \
    --api-url https://openrouter.ai/api/v1 \
    --model meta-llama/llama-3.2-11b-vision-instruct:free

# Analysis with custom prompt
video-analyzer video.mp4 \
    --prompt "What activities are happening in this video?" \
    --whisper-model large
```

## Output

The tool generates a JSON file (`output\analysis.json`) containing:
- Metadata about the analysis
- Audio transcript (if available)
- Frame-by-frame analysis
- Final video description

### Sample Output
```
The video begins with a person with long blonde hair, wearing a pink t-shirt and yellow shorts, standing in front of a black plastic tub or container on wheels. The ground appears to be covered in wood chips.\n\nAs the video progresses, the person remains facing away from the camera, looking down at something inside the tub. ........
```
full sample output in `docs/sample_analysis.json`
## Configuration

The tool uses a cascading configuration system with command line arguments taking highest priority, followed by user config (config/config.json), and finally the default config. See [docs/USAGES.md](docs/USAGES.md) for detailed configuration options.


## Uninstallation

To uninstall the package:
```bash
pip uninstall video-analyzer
```

## License

Apache License

## Contributing

We welcome contributions! Please see [docs/CONTRIBUTING.md](docs/CONTRIBUTING.md) for detailed guidelines on how to:
- Review the project design
- Propose changes through GitHub Discussions
- Submit pull requests


--- æ–‡ä»¶è·¯å¾„: video_analyzer.egg-info\SOURCES.txt ---

LICENSE
MANIFEST.in
readme.md
requirements.txt
setup.py
video_analyzer/__init__.py
video_analyzer/analyzer.py
video_analyzer/audio_processor.py
video_analyzer/cli.py
video_analyzer/config.py
video_analyzer/frame.py
video_analyzer/prompt.py
video_analyzer.egg-info/PKG-INFO
video_analyzer.egg-info/SOURCES.txt
video_analyzer.egg-info/dependency_links.txt
video_analyzer.egg-info/entry_points.txt
video_analyzer.egg-info/requires.txt
video_analyzer.egg-info/top_level.txt
video_analyzer/clients/__init__.py
video_analyzer/clients/generic_openai_api.py
video_analyzer/clients/llm_client.py
video_analyzer/clients/ollama.py
video_analyzer/config/default_config.json
video_analyzer/prompts/frame_analysis/describe.txt
video_analyzer/prompts/frame_analysis/frame_analysis.txt

--- æ–‡ä»¶è·¯å¾„: video_analyzer.egg-info\dependency_links.txt ---




--- æ–‡ä»¶è·¯å¾„: video_analyzer.egg-info\entry_points.txt ---

[console_scripts]
video-analyzer = video_analyzer.cli:main


--- æ–‡ä»¶è·¯å¾„: video_analyzer.egg-info\requires.txt ---

opencv-python>=4.8.0
numpy>=1.24.0
torch>=2.0.0
openai-whisper>=20231117
requests>=2.31.0
Pillow>=10.0.0
pydub>=0.25.1
faster-whisper>=0.6.0


--- æ–‡ä»¶è·¯å¾„: video_analyzer.egg-info\top_level.txt ---

video_analyzer



